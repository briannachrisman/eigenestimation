{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import gc\n",
    "\n",
    "# Add the test directory to sys.path\n",
    "parent_dir = os.path.expanduser('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Standard library imports\n",
    "import importlib\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "\n",
    "\n",
    "# Reload modules using importlib\n",
    "importlib.reload(importlib.import_module('eigenestimation.eigenhora'))\n",
    "importlib.reload(importlib.import_module('eigenestimation.loss'))\n",
    "importlib.reload(importlib.import_module('eigenestimation.train'))\n",
    "importlib.reload(importlib.import_module('evaluation.activating_examples'))\n",
    "importlib.reload(importlib.import_module('toy_models.transformer_wrapper'))\n",
    "importlib.reload(importlib.import_module('eigenestimation.utils'))\n",
    "importlib.reload(importlib.import_module('evaluation.transformers'))\n",
    "\n",
    "\n",
    "\n",
    "from eigenestimation.eigenhora import EigenHora\n",
    "from eigenestimation import loss\n",
    "from eigenestimation.train import Train\n",
    "from evaluation.activating_examples import DrawNeuralNetwork\n",
    "from evaluation.transformers import PrintTopActivatingTexts, PrintTopActivatingTextsAllFeatures\n",
    "from toy_models import transformer_wrapper\n",
    "from eigenestimation.utils import TransformDataLoader, DeleteParams\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-33M into HookedTransformer\n",
      "[('transformer.embed.W_E', 38597376), ('transformer.pos_embed.W_pos', 1572864), ('transformer.blocks.0.attn.W_Q', 589824), ('transformer.blocks.0.attn.W_O', 589824), ('transformer.blocks.0.attn.b_Q', 768), ('transformer.blocks.0.attn.b_O', 768), ('transformer.blocks.0.attn.W_K', 589824), ('transformer.blocks.0.attn.W_V', 589824), ('transformer.blocks.0.attn.b_K', 768), ('transformer.blocks.0.attn.b_V', 768), ('transformer.blocks.0.mlp.W_in', 2359296), ('transformer.blocks.0.mlp.b_in', 3072), ('transformer.blocks.0.mlp.W_out', 2359296), ('transformer.blocks.0.mlp.b_out', 768), ('transformer.blocks.1.attn.W_Q', 589824), ('transformer.blocks.1.attn.W_O', 589824), ('transformer.blocks.1.attn.b_Q', 768), ('transformer.blocks.1.attn.b_O', 768), ('transformer.blocks.1.attn.W_K', 589824), ('transformer.blocks.1.attn.W_V', 589824), ('transformer.blocks.1.attn.b_K', 768), ('transformer.blocks.1.attn.b_V', 768), ('transformer.blocks.1.mlp.W_in', 2359296), ('transformer.blocks.1.mlp.b_in', 3072), ('transformer.blocks.1.mlp.W_out', 2359296), ('transformer.blocks.1.mlp.b_out', 768), ('transformer.blocks.2.attn.W_Q', 589824), ('transformer.blocks.2.attn.W_O', 589824), ('transformer.blocks.2.attn.b_Q', 768), ('transformer.blocks.2.attn.b_O', 768), ('transformer.blocks.2.attn.W_K', 589824), ('transformer.blocks.2.attn.W_V', 589824), ('transformer.blocks.2.attn.b_K', 768), ('transformer.blocks.2.attn.b_V', 768), ('transformer.blocks.2.mlp.W_in', 2359296), ('transformer.blocks.2.mlp.b_in', 3072), ('transformer.blocks.2.mlp.W_out', 2359296), ('transformer.blocks.2.mlp.b_out', 768), ('transformer.blocks.3.attn.W_Q', 589824), ('transformer.blocks.3.attn.W_O', 589824), ('transformer.blocks.3.attn.b_Q', 768), ('transformer.blocks.3.attn.b_O', 768), ('transformer.blocks.3.attn.W_K', 589824), ('transformer.blocks.3.attn.W_V', 589824), ('transformer.blocks.3.attn.b_K', 768), ('transformer.blocks.3.attn.b_V', 768), ('transformer.blocks.3.mlp.W_in', 2359296), ('transformer.blocks.3.mlp.b_in', 3072), ('transformer.blocks.3.mlp.W_out', 2359296), ('transformer.blocks.3.mlp.b_out', 768), ('transformer.unembed.W_U', 38597376), ('transformer.unembed.b_U', 50257)]\n",
      "589824\n",
      "transformer.blocks.3.attn.W_K torch.Size([16, 768, 48]) 589824\n",
      "torch.Size([5854, 8])\n"
     ]
    }
   ],
   "source": [
    "# @title Import pretrained gpt2 (2 layers)\n",
    "# Disable fused kernels (FlashAttention and memory-efficient attention)\n",
    "# We have to disable this to compute second-order gradients on transformer models.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
    "import transformer_lens\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "# Ensure the math kernel is enabled (it is True by default)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "# Load in a 2-L GPT2.\n",
    "#gpt2 = GPT2Model.from_pretrained('gpt2', config=config)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#transformer_model = TransformerWrapper(gpt2, tokenizer)\n",
    "\n",
    "\n",
    "#gpt2  = transformer_lens.HookedTransformer.from_pretrained('gpt2-small')\n",
    "#tokenizer = gpt2.tokenizer\n",
    "\n",
    "tinystories_1m  = transformer_lens.HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\")#\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "transformer_model0 = transformer_wrapper.TransformerWrapper(tinystories_1m, tokenizer)\n",
    "\n",
    "\n",
    "tinystories_33m  = transformer_lens.HookedTransformer.from_pretrained(\"roneneldan/TinyStories-33M\")#\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "transformer_model = transformer_wrapper.TransformerWrapper(tinystories_33m, tokenizer)\n",
    "\n",
    "\n",
    "print( [(name, param.numel()) for name, param in transformer_model.named_parameters()])\n",
    "#transformer_model0  = transformer_lens.HookedTransformer.from_pretrained(\"roneneldan/TinyStories-2Layers-33M\")#\n",
    "#tokenizer0 = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "##tokenizer.pad_token = tokenizer.eos_token\n",
    "#transformer_model0 = TransformerWrapper(transformer_model0, tokenizer0).requires_grad_(False)\n",
    "\n",
    "\n",
    "# Make the eigenestimation a little smaller but only looking at a subset of the parameters.\n",
    "# Pick a random subset of tensors to include in paramters, and turn the rest into frozen buffers.\n",
    "params_to_delete = [name for name, param in transformer_model.named_parameters()]\n",
    "params_to_delete = [p for p in params_to_delete if #('blocks.4.attn.W' not in p)]# and ('blocks.6.mlp.W' not in p)]#!='transformer.h.1.ln_2.weight']\n",
    "   'transformer.blocks.3.attn.W_K' not in p]#!='transformer.h.1.ln_2.weight']\n",
    "\n",
    "# Delete 3/4 of the parameters.\n",
    "#for p in (params_to_delete[::20]):\n",
    "#  params_to_delete.remove(p)\n",
    "\n",
    "DeleteParams(transformer_model, params_to_delete)\n",
    "\n",
    "print(sum([p.numel() for p in transformer_model.parameters()]))\n",
    "for n,p in transformer_model.named_parameters(): print(n, p.shape, p.numel())\n",
    "\n",
    "# Load in data.\n",
    "dataset = load_dataset('roneneldan/TinyStories', split=\"validation[:1%]\")\n",
    "X_transformer = tokenize_and_concatenate(dataset, transformer_model.tokenizer, max_length = 8, add_bos_token=False)['tokens']\n",
    "print(X_transformer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenestimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 36.538,  Reconstruction Loss: 36.538,  Sparsity Loss: 0.000\n",
      "Epoch 1 : 36.957,  Reconstruction Loss: 36.957,  Sparsity Loss: 0.000\n",
      "Epoch 2 : 37.029,  Reconstruction Loss: 37.029,  Sparsity Loss: 0.000\n",
      "Epoch 3 : 36.684,  Reconstruction Loss: 36.684,  Sparsity Loss: 0.000\n",
      "Epoch 4 : 36.863,  Reconstruction Loss: 36.863,  Sparsity Loss: 0.000\n",
      "Epoch 5 : 37.081,  Reconstruction Loss: 37.081,  Sparsity Loss: 0.000\n",
      "Epoch 6 : 36.595,  Reconstruction Loss: 36.595,  Sparsity Loss: 0.000\n",
      "Epoch 7 : 36.586,  Reconstruction Loss: 36.586,  Sparsity Loss: 0.000\n",
      "Epoch 8 : 36.655,  Reconstruction Loss: 36.655,  Sparsity Loss: 0.000\n",
      "Epoch 9 : 36.779,  Reconstruction Loss: 36.779,  Sparsity Loss: 0.000\n",
      "Epoch 10 : 36.750,  Reconstruction Loss: 36.750,  Sparsity Loss: 0.000\n",
      "Epoch 11 : 36.463,  Reconstruction Loss: 36.463,  Sparsity Loss: 0.080\n",
      "Epoch 12 : 36.269,  Reconstruction Loss: 36.269,  Sparsity Loss: 0.308\n",
      "Epoch 13 : 36.171,  Reconstruction Loss: 36.170,  Sparsity Loss: 0.333\n",
      "Epoch 14 : 36.143,  Reconstruction Loss: 36.142,  Sparsity Loss: 0.342\n",
      "Epoch 15 : 36.311,  Reconstruction Loss: 36.311,  Sparsity Loss: 0.344\n",
      "Epoch 16 : 36.543,  Reconstruction Loss: 36.543,  Sparsity Loss: 0.345\n",
      "Epoch 17 : 36.231,  Reconstruction Loss: 36.230,  Sparsity Loss: 0.504\n",
      "Epoch 18 : 36.079,  Reconstruction Loss: 36.079,  Sparsity Loss: 0.596\n",
      "Epoch 19 : 35.962,  Reconstruction Loss: 35.962,  Sparsity Loss: 0.611\n",
      "Epoch 20 : 35.813,  Reconstruction Loss: 35.813,  Sparsity Loss: 0.705\n",
      "Epoch 21 : 35.679,  Reconstruction Loss: 35.678,  Sparsity Loss: 0.791\n",
      "Epoch 22 : 35.913,  Reconstruction Loss: 35.912,  Sparsity Loss: 0.820\n",
      "Epoch 23 : 35.696,  Reconstruction Loss: 35.696,  Sparsity Loss: 0.818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m eigenmodel \u001b[38;5;241m=\u001b[39m EigenHora(transformer_model, transformer_model0, loss\u001b[38;5;241m.\u001b[39mKLDivergenceLoss(), hora_features, hora_rank, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m TransformDataLoader(X_transformer[::\u001b[38;5;241m10\u001b[39m,:\u001b[38;5;241m4\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, transform_fn\u001b[38;5;241m=\u001b[39meigenmodel\u001b[38;5;241m.\u001b[39mcompute_jacobian)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL0_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/notebooks/../eigenestimation/train.py:27\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(eigenmodel, jacobian_dataloader, lr, n_epochs, L0_penalty, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m total_losses: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     26\u001b[0m n_batches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjacobian\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjacobian_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meigenmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacobian\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/notebooks/../eigenestimation/utils.py:13\u001b[0m, in \u001b[0;36mTransformDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/notebooks/../eigenestimation/eigenhora.py:36\u001b[0m, in \u001b[0;36mEigenHora.compute_jacobian\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_jacobian\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:604\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    603\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    606\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:399\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/eigenestimation/notebooks/../eigenestimation/eigenhora.py:30\u001b[0m, in \u001b[0;36mEigenHora.compute_loss\u001b[0;34m(self, x, param_dict)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, param_dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 30\u001b[0m     outputs: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     32\u001b[0m         truth: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel0(outputs)\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/_functorch/functional_call.py:148\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/nn/utils/stateless.py:298\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    294\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    296\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    297\u001b[0m ):\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workspace/eigenestimation/notebooks/../toy_models/transformer_wrapper.py:17\u001b[0m, in \u001b[0;36mTransformerWrapper.forward\u001b[0;34m(self, tokenized_X)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenized_X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Generate model outputs\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     model_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Rearrange the output to a flat batch of logits\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_logits:\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:546\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    539\u001b[0m ):\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m         (\n\u001b[1;32m    542\u001b[0m             residual,\n\u001b[1;32m    543\u001b[0m             tokens,\n\u001b[1;32m    544\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    545\u001b[0m             attention_mask,\n\u001b[0;32m--> 546\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m~/workspace/eigenestimation/env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:298\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, attention_mask, past_kv_cache)\u001b[0m\n\u001b[1;32m    296\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m--> 298\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_for_block_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    301\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    304\u001b[0m ):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# This means we need to have an explicit attention mask.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;66;03m# If the padding side is left or we are using caching, we need to compute the attention\u001b[39;00m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;66;03m# mask for the adjustment of absolute positional embeddings and attention masking so\u001b[39;00m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# that pad tokens are not attended.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def transformer_model0(y):\n",
    "    return torch.ones_like(y).softmax(dim=-1)\n",
    "shora_features = 100\n",
    "hora_rank = 1\n",
    "eigenmodel = EigenHora(transformer_model, transformer_model0, loss.KLDivergenceLoss(), hora_features, hora_rank, device=device).to(device)\n",
    "dataloader = TransformDataLoader(X_transformer[::10,:4], batch_size=8, transform_fn=eigenmodel.compute_jacobian)\n",
    "Train(eigenmodel, dataloader, lr=.01, n_epochs=100, L0_penalty=.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------feature 0-------\n",
      "Tom tried* t*o -> 7.712070271281846e-08\n",
      " and tried*,* but -> 5.0858769640171886e-08\n",
      "Tom* tried* to -> 3.895867450864898e-08\n",
      " lots of* be*akers -> 2.8452637579334805e-08\n",
      "They* tried* to roll -> 2.6440616807121842e-08\n",
      "------feature 1-------\n",
      " They knew that* playing* -> 1.3160922872346159e-11\n",
      " soap* on* his face -> 1.0703455483040969e-11\n",
      "Tom tried* t*o -> 1.0185556391373307e-11\n",
      " soap on* his* face -> 9.219998028942022e-12\n",
      " soap on his* face* -> 7.127769728609845e-12\n",
      "------feature 2-------\n",
      " has* long* ears and -> 3.179251653606663e-11\n",
      " still loved her*,* -> 2.8877197508214714e-11\n",
      " big tank that* could* -> 2.8161643156332516e-11\n",
      " has long ears* and* -> 2.126722409290238e-11\n",
      " I love* you*, -> 1.6956847384563822e-11\n",
      "------feature 3-------\n",
      " \"I promise* to* -> 1.4351831014574907e-22\n",
      " he liked to* draw* -> 1.2611533175720248e-22\n",
      " mov*in*g again -> 1.1582816903456253e-22\n",
      " takes it out* of* -> 1.038383983538532e-22\n",
      " put the* picture* in -> 1.0322939142413534e-22\n",
      "------feature 4-------\n",
      " They knew that* playing* -> 2.8684794123323698e-11\n",
      " truck with* flashing* l -> 2.5710256840172363e-11\n",
      " and* juicy*. He -> 1.107197233202184e-11\n",
      " were* stuck*. Lily -> 1.0675414545413542e-11\n",
      " came out of* his* -> 7.50059649806678e-12\n",
      "------feature 5-------\n",
      "'s not just* about* -> 5.823900028900653e-09\n",
      "One day,* T* -> 5.148775183982934e-09\n",
      " her if* she* shared -> 4.502797246175305e-09\n",
      " find a way* to* -> 4.44168124502653e-09\n",
      " day,* every* day -> 4.3597547794149705e-09\n",
      "------feature 6-------\n",
      "One day,* T* -> 4.891792808499329e-13\n",
      " to get some* water* -> 3.7792208598674826e-13\n",
      "Tom tried* t*o -> 3.566918895664406e-13\n",
      " big tank that* could* -> 3.4701627964379977e-13\n",
      " a clumsy monkey* named* -> 2.768309670039826e-13\n",
      "------feature 7-------\n",
      " He slipped* and* fell -> 1.1380090619850236e-10\n",
      " moving* again* -> 1.1180068676397426e-10\n",
      " to wait for* Christmas* -> 1.1068181787754483e-10\n",
      " left and* right*. -> 6.918805806055417e-11\n",
      " and asked*,* \" -> 6.251187761874277e-11\n",
      "------feature 8-------\n",
      " a* trip* to the -> 6.261842055350949e-21\n",
      " I said* yes*. -> 5.7949846295796445e-21\n",
      " had touched the* flower* -> 5.402069799649122e-21\n",
      " drawings* up* to dry -> 4.660745930255767e-21\n",
      " fast* as* he could -> 4.32872177160809e-21\n",
      "------feature 9-------\n",
      " He slipped and* fell* -> 1.7368273364111973e-14\n",
      ".* Grand*ma will -> 1.3952111560804233e-14\n",
      " day, every* day* -> 1.2509921924590012e-14\n",
      " He slipped* and* fell -> 1.0461378670886118e-14\n",
      " Sarah. He* w* -> 9.30632558049458e-15\n",
      "------feature 10-------\n",
      " day, every* day* -> 3.860385521989827e-14\n",
      " had touched the* flower* -> 2.7287628875786486e-14\n",
      " Lily's little* brother* -> 1.6513003868478572e-14\n",
      ", saw how* sad* -> 1.5332788817355898e-14\n",
      " looked down at* Tim* -> 1.4652673876753425e-14\n",
      "------feature 11-------\n",
      " from the* re*eds -> 4.363507954963097e-08\n",
      " It is a* shell* -> 2.2201698257617863e-08\n",
      " saw a man* and* -> 1.581404340811332e-08\n",
      " proud of herself* for* -> 1.5010654053071448e-08\n",
      " at home for* one* -> 1.4264130321350876e-08\n",
      "------feature 12-------\n",
      " to* ride* fast and -> 2.3075085679030716e-12\n",
      " to* ride* it.\" -> 2.3075085679030716e-12\n",
      " could get* hurt* easily -> 1.7562562732234555e-12\n",
      " proud of herself* for* -> 1.6956107741841753e-12\n",
      " Go away*!\"* Ben -> 1.5468004481433706e-12\n",
      "------feature 13-------\n",
      " a small da*isy* -> 3.5753336183834913e-19\n",
      "One day,* T* -> 2.869703577929951e-19\n",
      " They knew that* playing* -> 2.8411632618574574e-19\n",
      " He ran and* ran* -> 2.6031291158982526e-19\n",
      " and* address*. If -> 2.2063057606381886e-19\n",
      "------feature 14-------\n",
      " and* Twe*ety had -> 6.766623927845217e-10\n",
      "Tom tried t*o* -> 6.754423687027611e-10\n",
      " two birds fighting* outside* -> 5.21554799348678e-10\n",
      " Nemo* away*. -> 4.719211688097857e-10\n",
      " together all* the* time -> 4.6495521321965327e-10\n",
      "------feature 15-------\n",
      " important to* always* be -> 6.345414194086629e-10\n",
      " important* to* always be -> 5.438611783148417e-10\n",
      " important to always* be* -> 4.6376497087052826e-10\n",
      "Tom* tried* to -> 4.479974169413481e-10\n",
      " from the* re*eds -> 3.814554228043221e-10\n",
      "------feature 16-------\n",
      " I love you*,* -> 1.0319224190675924e-19\n",
      " from the* re*eds -> 7.854264093124262e-20\n",
      ". But the* door* -> 6.030538283832827e-20\n",
      " were* stuck*. Lily -> 5.825875705711309e-20\n",
      " her if she* shared* -> 5.385871196030504e-20\n",
      "------feature 17-------\n",
      " still loved* her*, -> 3.5342512916258784e-08\n",
      " day, every* day* -> 1.3609204430053978e-08\n",
      " still loved her*,* -> 1.079948930282626e-08\n",
      " a* trip* to the -> 9.663615685440163e-09\n",
      " asked him* if* he -> 9.414324431133991e-09\n",
      "------feature 18-------\n",
      " They knew that* playing* -> 4.724541197921226e-16\n",
      "y with* arguments*. -> 3.4952070767641896e-16\n",
      " a trip to* the* -> 3.1612129859367544e-16\n",
      " to wait for* Christmas* -> 2.7049099845037584e-16\n",
      " Sarah. He* w* -> 2.0299261047979555e-16\n",
      "------feature 19-------\n",
      " a trip* to* the -> 8.768054016172755e-08\n",
      " a clumsy monkey* named* -> 7.812796098960462e-08\n",
      "They left* their* toys -> 6.774051541924564e-08\n",
      " farmer heard* the* cow -> 6.727142931595154e-08\n",
      ". It is* pretty* -> 6.4921309217425e-08\n",
      "------feature 20-------\n",
      " mom to read* the* -> 1.9756818801397458e-05\n",
      " to* ride* fast and -> 1.4214419024938252e-05\n",
      " to* ride* it.\" -> 1.4214419024938252e-05\n",
      " a trip* to* the -> 6.2158478613127954e-06\n",
      " were* stuck*. Lily -> 6.114066763984738e-06\n",
      "------feature 21-------\n",
      " gears that* made* it -> 2.4328464429170253e-16\n",
      " a little bird* that* -> 2.311373447951286e-16\n",
      "'s not* just* about -> 1.8517676797684884e-16\n",
      " flew past Anna* and* -> 1.7570052070499235e-16\n",
      " his* friend* Lily painting -> 1.6905656632029703e-16\n",
      "------feature 22-------\n",
      ". The* end*. -> 6.049323264534223e-10\n",
      ". The* end*. -> 6.049323264534223e-10\n",
      " he had a* cavity* -> 5.115092238661134e-10\n",
      " things. The* end* -> 4.1886738522123323e-10\n",
      "One day,* T* -> 3.8522507406213435e-10\n",
      "------feature 23-------\n",
      " two birds* fighting* outside -> 3.033194957424712e-07\n",
      " with his cushion* throne* -> 2.877011979762756e-07\n",
      " important to always* be* -> 2.0915501863782993e-07\n",
      " takes it* out* of -> 1.856525955190591e-07\n",
      " Timmy to* a* -> 1.6325532214978011e-07\n",
      "------feature 24-------\n",
      "Tom tried t*o* -> 2.487195172040657e-11\n",
      " to get some* water* -> 1.6320139684111723e-11\n",
      ". They put* on* -> 1.6296413871130788e-11\n",
      " Nemo* away*. -> 1.5382627463478293e-11\n",
      " mom to read* the* -> 1.3515772702421547e-11\n",
      "------feature 25-------\n",
      " I wear* some* too -> 1.760058943034437e-08\n",
      " naughty squirrel came* along* -> 1.7006025032628713e-08\n",
      " and* called* for help -> 1.50359760198171e-08\n",
      " and* called* out for -> 1.50359760198171e-08\n",
      " were* stuck*. Lily -> 1.2507566538033643e-08\n",
      "------feature 26-------\n",
      " day, every* day* -> 5.104606737305062e-10\n",
      " important* to* always be -> 4.0389383504368936e-10\n",
      " paper. That* is* -> 3.7950023679123035e-10\n",
      " the treasure with* you* -> 2.388880759873757e-10\n",
      " important to* always* be -> 2.3575486007842983e-10\n",
      "------feature 27-------\n",
      " spider was about* to* -> 1.9258083128192993e-08\n",
      "Tom tried t*o* -> 1.887527822930224e-08\n",
      " a clumsy monkey* named* -> 1.0829939611767259e-08\n",
      " has long ears* and* -> 1.0548877327209993e-08\n",
      " together all* the* time -> 1.017291051397251e-08\n",
      "------feature 28-------\n",
      " to wait for* Christmas* -> 7.217834302506288e-19\n",
      " said, \"*Ell* -> 3.5561929175029556e-19\n",
      " be your* friend* forever -> 3.371423774619396e-19\n",
      " truck with flashing* l* -> 3.3679945939924807e-19\n",
      ". But the* door* -> 3.205731226118899e-19\n",
      "------feature 29-------\n",
      " to ride fast* and* -> 2.3487782385700484e-08\n",
      "ase to the* sink* -> 1.7707016297663358e-08\n",
      " It is a* shell* -> 1.6069835240273278e-08\n",
      " be* your* friend forever -> 1.5545339238087763e-08\n",
      " favorite* rock* in the -> 1.4759645061701576e-08\n",
      "------feature 30-------\n",
      " were* stuck*. Lily -> 3.14894344899798e-16\n",
      " day, every* day* -> 2.9534543328209957e-16\n",
      " help others when* they* -> 2.6039243342519365e-16\n",
      " Nemo* away*. -> 2.432494924243915e-16\n",
      " heavy frame all* by* -> 2.4049128845033546e-16\n",
      "------feature 31-------\n",
      " had touched the* flower* -> 1.0874260624404997e-05\n",
      " own* game*. We -> 7.043134701234521e-06\n",
      "What is that* cloud* -> 6.222758202056866e-06\n",
      "'s* play*!\"newline -> 4.307206381781725e-06\n",
      " a fancy* te*ap -> 3.605714937293669e-06\n",
      "------feature 32-------\n",
      " I love you*,* -> 1.6853807238703666e-08\n",
      " I love* you*, -> 1.6181527229264248e-08\n",
      " and* Twe*ety had -> 1.523805970293779e-08\n",
      " twirled around* in* -> 1.2956696160415504e-08\n",
      " day, every* day* -> 1.209603084362243e-08\n",
      "------feature 33-------\n",
      " his toy cars* and* -> 2.7527043755526392e-08\n",
      " became stronger* and* healthier -> 2.2904391272504654e-08\n",
      " see which one* is* -> 1.9768286563248694e-08\n",
      " saw a man* and* -> 1.2256785808517634e-08\n",
      " think it* is* a -> 1.2067961741024646e-08\n",
      "------feature 34-------\n",
      " a small da*isy* -> 8.014103514142334e-08\n",
      " two birds* fighting* outside -> 7.77809958663056e-08\n",
      ". They put* on* -> 7.359216169788851e-08\n",
      " important to always* be* -> 7.238866572834013e-08\n",
      " two birds fighting* outside* -> 4.64654981158219e-08\n",
      "------feature 35-------\n",
      " and* address*. If -> 9.124399014126539e-10\n",
      " He slipped* and* fell -> 4.410653509090423e-10\n",
      " day,* every* day -> 4.231636152596252e-10\n",
      " go up* and* down -> 3.706127627012279e-10\n",
      "newlineBut* on* the -> 3.568349227212053e-10\n",
      "------feature 36-------\n",
      " has* long* ears and -> 3.8890182849256327e-13\n",
      " Ben* frown*. They -> 1.8869446478421426e-13\n",
      " creating lots of* wet* -> 1.4819220882974354e-13\n",
      " at* each* other. -> 1.350551750512255e-13\n",
      " we see* it*? -> 1.1340505357698705e-13\n",
      "------feature 37-------\n",
      " two birds* fighting* outside -> 4.722248491491245e-15\n",
      "'s* just* a little -> 4.5091006971276996e-15\n",
      " I will* protect* the -> 3.400044460387386e-15\n",
      " was happy to* help* -> 3.265802443741788e-15\n",
      " liked to* walk* in -> 3.0216750546071267e-15\n",
      "------feature 38-------\n",
      " a* trip* to the -> 4.809269853467413e-07\n",
      " still loved* her*, -> 3.3726985293469625e-07\n",
      " I wear some* too* -> 2.8332311785561615e-07\n",
      " I* wear* some too -> 2.4413006372014934e-07\n",
      " squirrel* stopped* and froze -> 2.372181171494958e-07\n",
      "------feature 39-------\n",
      " day,* every* day -> 4.3639988541246955e-13\n",
      " rolled away from* him* -> 2.9572828560975317e-13\n",
      " know* what* to say -> 1.702850778136436e-13\n",
      " \"I* promise* to -> 1.6894179197951553e-13\n",
      " a big* hug* and -> 1.6117655983732132e-13\n",
      "------feature 40-------\n",
      " day, every* day* -> 6.429059062895703e-09\n",
      " day,* every* day -> 3.4448131014386263e-09\n",
      " I wear* some* too -> 2.6336948177885233e-09\n",
      " did not hear* her* -> 2.5201796205465143e-09\n",
      " a* trip* to the -> 2.2675701316643426e-09\n",
      "------feature 41-------\n",
      " stretch her arms* up* -> 2.2559691892354294e-09\n",
      "What is that* cloud* -> 8.616759528123907e-10\n",
      " They knew that* playing* -> 7.718238825837886e-10\n",
      " a big hug* and* -> 6.023526122334033e-10\n",
      " at* each* other. -> 5.293237514969462e-10\n",
      "------feature 42-------\n",
      " a small da*isy* -> 5.304025448016336e-12\n",
      "Tom tried* t*o -> 2.4175182255364858e-12\n",
      "Tom tried t*o* -> 1.225260995094124e-12\n",
      " on her* finger* t -> 1.1557289413682836e-12\n",
      " It is a* shell* -> 9.528212637291422e-13\n",
      "------feature 43-------\n",
      " finally won* a* game -> 2.93588486854901e-11\n",
      " clothes* to* wear. -> 2.371057516992181e-11\n",
      " dolls* that* looked just -> 1.7222475182099295e-11\n",
      " spider was* about* to -> 1.520960093981394e-11\n",
      " Lily's little* brother* -> 1.3396281479949312e-11\n",
      "------feature 44-------\n",
      " a girl* named* Lily -> 2.7023033810635866e-10\n",
      " he liked to* draw* -> 1.6777026989878152e-10\n",
      " stretch her arms* up* -> 1.6203506592038508e-10\n",
      " at* each* other. -> 1.5751953907905403e-10\n",
      " it's* not* safe -> 1.4482201549093077e-10\n",
      "------feature 45-------\n",
      " Come and* eat*.\" -> 7.238754475679579e-20\n",
      "One day,* T* -> 5.931884717323661e-20\n",
      " help* others* when they -> 5.69171282646889e-20\n",
      " Lily's little* brother* -> 4.806414697948342e-20\n",
      ".* Grand*ma will -> 3.622120504793139e-20\n",
      "------feature 46-------\n",
      " a trip* to* the -> 9.54416350396059e-07\n",
      " a* trip* to the -> 7.421614895974926e-07\n",
      " to get some* water* -> 3.97532545548529e-07\n",
      " to go* home*. -> 3.2733751709201897e-07\n",
      " liked to walk* in* -> 3.1841173608881945e-07\n",
      "------feature 47-------\n",
      " and called for* help* -> 2.2713873448765298e-08\n",
      " a trip* to* the -> 2.0152759461211645e-08\n",
      " Bob had an* idea* -> 1.2737753962710485e-08\n",
      "One day,* T* -> 1.0561763019723003e-08\n",
      " Go* away*!\" Ben -> 9.979848059060714e-09\n",
      "------feature 48-------\n",
      " finally* won* a game -> 3.0200089895515703e-07\n",
      " her other* dolls* to -> 1.7183432987621927e-07\n",
      " I love* you*, -> 1.7165746157843387e-07\n",
      ".* Grand*ma will -> 1.7101179139444866e-07\n",
      ", or* scared*. -> 1.6842538741457247e-07\n",
      "------feature 49-------\n",
      " and address.* If* -> 2.441831403743322e-09\n",
      ".* Can*'t chew -> 2.223899508990712e-09\n",
      "newlineAt first*,* -> 1.8824648506665653e-09\n",
      " were* stuck*. Lily -> 1.785030567802437e-09\n",
      ". Can't* chew* -> 1.0447243070643708e-09\n",
      "------feature 50-------\n",
      " naughty squirrel came* along* -> 2.736964517871532e-12\n",
      " squirrel* stopped* and froze -> 2.1920017884147214e-12\n",
      "Tom tried t*o* -> 1.7822675843839897e-12\n",
      " creative. I* love* -> 1.7217997860807799e-12\n",
      " the* next* day, -> 1.6128257583625238e-12\n",
      "------feature 51-------\n",
      " Ben* frown*. They -> 2.2499349050519868e-09\n",
      " a fancy te*ap* -> 2.172378721354562e-09\n",
      " and* juicy*. He -> 2.0241792686448434e-09\n",
      " a trip to* the* -> 2.00963157226397e-09\n",
      " The other* sw*ans -> 1.8953427716184024e-09\n",
      "------feature 52-------\n",
      " to* wait* for Christmas -> 5.719717234564659e-13\n",
      " to get some* water* -> 3.6813800163675026e-13\n",
      ".\" They* lay* down -> 3.1086832869182957e-13\n",
      " They knew that* playing* -> 2.845409454929615e-13\n",
      " to wait* for* Christmas -> 2.7635142438309224e-13\n",
      "------feature 53-------\n",
      "Tom tried* t*o -> 1.2404337113025576e-08\n",
      " to ride fast* and* -> 4.3251517922726634e-09\n",
      " and tried*,* but -> 2.8175535238261773e-09\n",
      " fun. They* learned* -> 2.1334669586536847e-09\n",
      " The whale learned* that* -> 1.801210736118719e-09\n",
      "------feature 54-------\n",
      "Tom tried* t*o -> 3.162332617989705e-10\n",
      " on her finger* t* -> 1.1854739412342497e-10\n",
      " to* ride* fast and -> 6.962837945101441e-11\n",
      " to* ride* it.\" -> 6.962837945101441e-11\n",
      "The judge noticed* Lily* -> 5.604785732749029e-11\n",
      "------feature 55-------\n",
      "Tom tried* t*o -> 2.829761147626897e-10\n",
      "newlineBut on* the* -> 2.1741786149220843e-10\n",
      ". It is* pretty* -> 1.421674444834764e-10\n",
      " asked him if* he* -> 1.0649019860364817e-10\n",
      " farmer heard* the* cow -> 1.0394712174344178e-10\n",
      "------feature 56-------\n",
      " and called for* help* -> 2.6753144144464613e-10\n",
      " are you* sad*, -> 1.2132715665469362e-10\n",
      " naughty squirrel came* along* -> 9.708208542624774e-11\n",
      " still loved* her*, -> 8.947521890068089e-11\n",
      " on stage* too*. -> 8.606593909776805e-11\n",
      "------feature 57-------\n",
      " truck with flashing* l* -> 2.4966607270471286e-06\n",
      " lots of* be*akers -> 1.9502060695231194e-06\n",
      "What is that* cloud* -> 1.5156547306105494e-06\n",
      " had torn her* favorite* -> 1.3253097677079495e-06\n",
      "newlineBut on* the* -> 1.2299082072786405e-06\n",
      "------feature 58-------\n",
      "One day,* T* -> 4.1037692871075036e-22\n",
      " has long ears* and* -> 3.3472355231292143e-22\n",
      " and explore* the* world -> 3.2414357704403793e-22\n",
      " see which one* is* -> 2.7400721297924168e-22\n",
      " and* explore* the world -> 2.673277194354564e-22\n",
      "------feature 59-------\n",
      " mom to read* the* -> 2.0216221585656058e-08\n",
      " and* Twe*ety had -> 1.8144213242976548e-08\n",
      " the* next* day, -> 1.6168527849913517e-08\n",
      " on her finger* t* -> 1.2985986508340375e-08\n",
      " I wear* some* too -> 1.2274014693502977e-08\n",
      "------feature 60-------\n",
      "The judge noticed* Lily* -> 2.1141088879517156e-09\n",
      " and* milk*. Lily -> 1.902781043838786e-09\n",
      " became stronger* and* healthier -> 1.7798873486185585e-09\n",
      " room with many* books* -> 1.6598390439881427e-09\n",
      " on* stage* too. -> 1.4465206810143627e-09\n",
      "------feature 61-------\n",
      "Tom tried* t*o -> 4.2628099095054495e-07\n",
      " and tried*,* but -> 1.7768866200640332e-07\n",
      " She was* trying* to -> 1.2230000834279053e-07\n",
      " summer because* he* could -> 1.2118202619149088e-07\n",
      ", your dress* is* -> 1.1120940968112336e-07\n",
      "------feature 62-------\n",
      ".\" They* lay* down -> 1.0869882045982681e-24\n",
      " are you sad*,* -> 6.713192005326903e-25\n",
      "ily and* Tom* looked -> 6.059305694027272e-25\n",
      " with our* toys*!\" -> 6.057105758177837e-25\n",
      " to explore* right* away -> 5.9244144235390055e-25\n",
      "------feature 63-------\n",
      " became stronger* and* healthier -> 9.413593832934865e-15\n",
      " a big hug* and* -> 6.3496927035994535e-15\n",
      " I* love* you, -> 6.186296236425836e-15\n",
      " had torn* her* favorite -> 6.0965890945645015e-15\n",
      " a big* hug* and -> 4.622130467675208e-15\n",
      "------feature 64-------\n",
      " a small da*isy* -> 1.1652896953173427e-15\n",
      " soap on* his* face -> 7.900614053428577e-16\n",
      " and* address*. If -> 5.832384059964598e-16\n",
      " big tank that* could* -> 5.180651499358722e-16\n",
      " a* dust*pan to -> 4.899504323506074e-16\n",
      "------feature 65-------\n",
      "They left their* toys* -> 1.3413193624377262e-13\n",
      " naughty squirrel came* along* -> 1.0441449002076761e-13\n",
      " truck with flashing* l* -> 1.0223920099200132e-13\n",
      "Tom tried* t*o -> 8.933200452152304e-14\n",
      " and the knife* and* -> 6.739990239101185e-14\n",
      "------feature 66-------\n",
      " go up* and* down -> 9.81740466698966e-10\n",
      " day,* every* day -> 3.9493414094593504e-10\n",
      " named Lily*.* Lily -> 3.8721909012551237e-10\n",
      "They left their* toys* -> 3.3539476751442976e-10\n",
      " her friends* came* to -> 3.036502160824739e-10\n",
      "------feature 67-------\n",
      " She takes Lily* and* -> 1.3243187039849557e-23\n",
      " B*amb*i to -> 1.1307031288763168e-23\n",
      " truck with flashing* l* -> 1.0539519326220152e-23\n",
      " two birds fighting* outside* -> 7.332278309438388e-24\n",
      " fun. They* learned* -> 6.981266761051247e-24\n",
      "------feature 68-------\n",
      " He* misses* Ben a -> 7.371002084255451e-07\n",
      " a clumsy monkey* named* -> 6.873588063172065e-07\n",
      " He misses* Ben* a -> 6.171057975734584e-07\n",
      " soap on* his* face -> 6.038619630999165e-07\n",
      " Bambi* to* -> 5.31947421222867e-07\n",
      "------feature 69-------\n",
      " to* wait* for Christmas -> 2.3445958508006015e-09\n",
      " day, every* day* -> 2.312826152817138e-09\n",
      " the* next* day, -> 1.8188921480088993e-09\n",
      " friend,* Ben*.\" -> 1.1879696115713045e-09\n",
      " go up and* down* -> 1.1569114555243232e-09\n",
      "------feature 70-------\n",
      " were* stuck*. Lily -> 4.901611116991944e-09\n",
      ". Can't* chew* -> 2.406336241378426e-09\n",
      " his toy* car* wasn -> 2.344270999543596e-09\n",
      ". Can*'t* chew -> 2.2254880160943458e-09\n",
      " and Twe*ety* had -> 1.976833141625889e-09\n",
      "------feature 71-------\n",
      " the* daughter* played on -> 2.9193551540374756\n",
      " the cat* who* was -> 2.5246455669403076\n",
      ".* *newlinenewline -> 2.2273473739624023\n",
      " The other sw*ans* -> 2.128319025039673\n",
      " his* mom*. They -> 1.883445143699646\n",
      "------feature 72-------\n",
      " to ride fast* and* -> 1.4309389784195048e-10\n",
      " finally won* a* game -> 1.232616231305883e-10\n",
      ", your* dress* is -> 1.0974380432182684e-10\n",
      " see which one* is* -> 7.863433515442608e-11\n",
      " He misses* Ben* a -> 7.114799027707619e-11\n",
      "------feature 73-------\n",
      ", your* dress* is -> 3.647646735771559e-07\n",
      " to do* inside*. -> 3.3309513014501135e-07\n",
      " back inside and* had* -> 3.250604834192927e-07\n",
      " and called for* help* -> 3.218421511519409e-07\n",
      " at* home* for one -> 2.6712723411037587e-07\n",
      "------feature 74-------\n",
      " a small da*isy* -> 4.578428745269775\n",
      " and* milk*. Lily -> 4.48018217086792\n",
      " his toy* car* wasn -> 3.9933719635009766\n",
      " and* juicy*. He -> 3.5419061183929443\n",
      " be your* friend* forever -> 3.4962141513824463\n",
      "------feature 75-------\n",
      " important to always* be* -> 6.862349230529752e-12\n",
      " the* branches*.newline -> 6.3624340171375415e-12\n",
      " important to* always* be -> 4.769641712837336e-12\n",
      " the* next* day, -> 4.7392302755799864e-12\n",
      " the tall* hedge*. -> 3.683091158446228e-12\n",
      "------feature 76-------\n",
      " on her finger* t* -> 1.1929137677668677e-08\n",
      " has long ears* and* -> 5.5365347861879854e-09\n",
      "y with* arguments*. -> 3.0435736153577864e-09\n",
      " not* do* things that -> 2.949778865612984e-09\n",
      "'s always* a* solution -> 2.9255435851638367e-09\n",
      "------feature 77-------\n",
      " saw a man* and* -> 1.6429801519279863e-08\n",
      " day,* every* day -> 1.2760181355986333e-08\n",
      " \"I promise* to* -> 1.2675290150809815e-08\n",
      " the man made* sure* -> 1.2529653758974746e-08\n",
      " at home for* one* -> 1.220067336049624e-08\n",
      "------feature 78-------\n",
      " and* juicy*. He -> 7.8268656730651855\n",
      " a small da*isy* -> 6.90095329284668\n",
      " the* treasure* with you -> 5.605183124542236\n",
      " the* treasure*.newline -> 5.605183124542236\n",
      " the* water*. She -> 3.7860803604125977\n",
      "------feature 79-------\n",
      "One day,* T* -> 2.523409193599946e-06\n",
      " clothes* to* wear. -> 2.3734091882943176e-06\n",
      " squirrel* stopped* and froze -> 1.772127802723844e-06\n",
      " for a* walk* in -> 1.0207508012172184e-06\n",
      " many ducks* and* sw -> 1.0022803280662629e-06\n",
      "------feature 80-------\n",
      " The* balls* flew out -> 4.100955686148211e-13\n",
      "They got ready* and* -> 2.2030931224291395e-13\n",
      " the* branches*.newline -> 2.1839299845557297e-13\n",
      " the other sw*ans* -> 1.9732980982740955e-13\n",
      " the basket* with* many -> 1.8763541610213041e-13\n",
      "------feature 81-------\n",
      " Lily's little* brother* -> 1.8948551495229227e-12\n",
      " still loved* her*, -> 1.5168728903156015e-12\n",
      " day, every* day* -> 1.4583265151024705e-12\n",
      " and called for* help* -> 1.4125132270434437e-12\n",
      " the children how* to* -> 1.2796772105513887e-12\n",
      "------feature 82-------\n",
      " truck with* flashing* l -> 1.1836534419273903e-08\n",
      " still loved her*,* -> 3.4831124651191203e-09\n",
      "Tom tried t*o* -> 3.2891829260250915e-09\n",
      " not do things* that* -> 3.017187166776125e-09\n",
      " He slipped* and* fell -> 2.7570139504717872e-09\n",
      "------feature 83-------\n",
      " truck with flashing* l* -> 5.239081314367981e-15\n",
      " go up* and* down -> 3.922555792142514e-15\n",
      " wreck* near* the park -> 2.4612187643973738e-15\n",
      ". She put* on* -> 1.909559758377804e-15\n",
      " wreck near the* park* -> 1.8856622060786817e-15\n",
      "------feature 84-------\n",
      " and called for* help* -> 11.451726913452148\n",
      "What is that* cloud* -> 5.60857629776001\n",
      " stretch her arms* up* -> 4.612113952636719\n",
      " a trip* to* the -> 4.154975414276123\n",
      " became stronger* and* healthier -> 4.127471446990967\n",
      "------feature 85-------\n",
      ", your* dress* is -> 1.384769461765245e-06\n",
      " truck with* flashing* l -> 4.876465595771151e-07\n",
      " truck with flashing* l* -> 3.935190306947334e-07\n",
      " runs after it*,* -> 3.7085547432980093e-07\n",
      " liked to walk* in* -> 2.919096857567638e-07\n",
      "------feature 86-------\n",
      " important to always* be* -> 2.752280465756485e-07\n",
      "One day,* T* -> 2.5090011490647157e-07\n",
      " saw the big* dog* -> 2.2709544111876312e-07\n",
      ". But the* door* -> 1.7776572747152386e-07\n",
      " favorite* rock* in the -> 1.58037892106222e-07\n",
      "------feature 87-------\n",
      " B*amb*i to -> 2.4192433437477626e-20\n",
      " fast as* he* could -> 2.285287744334622e-20\n",
      " fast* as* he could -> 1.7198270775933723e-20\n",
      " Lily's little* brother* -> 1.4165072171122795e-20\n",
      "ens to* climb* the -> 1.1545038165378601e-20\n",
      "------feature 88-------\n",
      " stretch her arms* up* -> 8.213433821957494e-10\n",
      " stretch her* arms* up -> 1.925896803145477e-10\n",
      " hand* out*. She -> 1.301413005139196e-10\n",
      " see which one* is* -> 9.795503297382879e-11\n",
      " Lily's little* brother* -> 9.396988742693679e-11\n",
      "------feature 89-------\n",
      " to play* football* in -> 4.056856628409378e-09\n",
      " Nemo* away*. -> 2.889970707187217e-09\n",
      " f*auc*et for -> 2.0207104878267046e-09\n",
      " big tank that* could* -> 1.5524735941241374e-09\n",
      " day, every* day* -> 1.4231258393948565e-09\n",
      "------feature 90-------\n",
      "Tom tried t*o* -> 1.267806714645714e-12\n",
      " drawings up to* dry* -> 8.099307899703256e-13\n",
      " Lily's little* brother* -> 7.830092368758812e-13\n",
      " and tried*,* but -> 6.398739502665174e-13\n",
      " she should* have* been -> 4.945232049378967e-13\n",
      "------feature 91-------\n",
      " truck with* flashing* l -> 1.6219621132296247e-11\n",
      " he had a* cavity* -> 1.2726406733998274e-11\n",
      " not* do* things that -> 1.042141546669928e-11\n",
      " help others* when* they -> 9.766485363493782e-12\n",
      " a dustpan* to* -> 8.610446557144602e-12\n",
      "------feature 92-------\n",
      " go up* and* down -> 6.192223955814313e-10\n",
      " liked to* walk* in -> 5.664507751745873e-10\n",
      "One day,* T* -> 5.57185630967183e-10\n",
      " He slipped* and* fell -> 4.90535612129861e-10\n",
      " They knew that* playing* -> 4.579084056377525e-10\n",
      "------feature 93-------\n",
      " and* called* for help -> 1.6547064437648255e-19\n",
      " and* called* out for -> 1.6547064437648255e-19\n",
      " flew past Anna* and* -> 1.4162973523435869e-19\n",
      " and called for* help* -> 1.350117345719604e-19\n",
      " the bulb.* The* -> 1.288272282493282e-19\n",
      "------feature 94-------\n",
      " and* called* for help -> 3.018432437329466e-08\n",
      " and* called* out for -> 3.018432437329466e-08\n",
      "newlineBut on* the* -> 2.3537300108955606e-08\n",
      " help others when* they* -> 1.656425041574039e-08\n",
      " oats to be* strong* -> 1.4786619928486289e-08\n",
      "------feature 95-------\n",
      " many ducks* and* sw -> 2.72921974633792e-20\n",
      " finally* won* a game -> 2.3216613961845075e-20\n",
      " she should* have* been -> 2.130892221179612e-20\n",
      "Tom tried* t*o -> 1.7121468994762735e-20\n",
      " not do things* that* -> 1.4693737517772213e-20\n",
      "------feature 96-------\n",
      " to ride fast* and* -> 1.4077273817747482e-06\n",
      " has long ears* and* -> 6.44632109469967e-07\n",
      " be your* friend* forever -> 4.975254341843538e-07\n",
      " toys and* forget* about -> 4.572948455461301e-07\n",
      " has long* ears* and -> 4.4843440605291107e-07\n",
      "------feature 97-------\n",
      ". But the* door* -> 8.511193416715912e-10\n",
      " go up* and* down -> 3.0335786660451447e-10\n",
      " He slipped* and* fell -> 2.0475764139771258e-10\n",
      " I love* you*, -> 2.003996135702124e-10\n",
      " see which* one* is -> 1.851075542846914e-10\n",
      "------feature 98-------\n",
      " with his cushion* throne* -> 6.259790532402013e-17\n",
      " big tank that* could* -> 2.768331642603873e-17\n",
      " favorite* rock* in the -> 2.726648688048346e-17\n",
      " on* stage* too. -> 2.452775473804694e-17\n",
      " and tried*,* but -> 2.2430673214212703e-17\n",
      "------feature 99-------\n",
      " favorite* rock* in the -> 1.2915283287284751e-09\n",
      " and* juicy*. He -> 1.0083726076359767e-09\n",
      " and tried*,* but -> 8.001984630467973e-10\n",
      " was a* farmer* who -> 5.1414789092874e-10\n",
      " went to the* circus* -> 5.021694171603031e-10\n"
     ]
    }
   ],
   "source": [
    "top_texts = PrintTopActivatingTextsAllFeatures(eigenmodel, dataloader, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eigenestimation)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
