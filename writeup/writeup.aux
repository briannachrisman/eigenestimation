\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bereska2024mechanistic}
\citation{gao2024scaling,cunningham2023sparse,bricken2023towards}
\citation{engels2024not,engels2024decomposing}
\citation{merullo2024talking,lindsey2024sparse}
\citation{pascanu2013difficulty,goodfellow2014generative,ho2020denoising,mnih2015human}
\citation{watanabe2007almost,watanabe2000algebraic,watanabe2005algebraic,bushnaq2024using,davies2023unifying}
\citation{wang2024loss,hoogland2024developmental}
\citation{wei2022deep}
\citation{nanda2023attribution,kramar2024atp,syed2023attribution}
\citation{turner2023steering,subramani2022extracting}
\citation{matena2023npeff}
\citation{braun2025interpretability}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1_jacobian_diagram}{{1}{2}{Decomposing a loss landscape into a set of parameter directions, where a smaller subset of directions can approximately reconstruct any per-sample-pair gradient of divergence.\relax }{figure.caption.1}{}}
\newlabel{fig:1_jacobian_diagram@cref}{{[figure][1][]1}{[1][2][]2}}
\citation{bussmann2024batchtopk}
\citation{}
\newlabel{sec:setup}{{2.1}{3}{}{subsection.2.1}{}}
\newlabel{sec:setup@cref}{{[subsection][1][2]2.1}{[1][3][]3}}
\newlabel{sec:criteria}{{2.2}{3}{}{subsection.2.2}{}}
\newlabel{sec:criteria@cref}{{[subsection][2][2]2.2}{[1][3][]3}}
\newlabel{eq:divergence}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:divergence@cref}{{[equation][2][]2}{[1][3][]3}}
\newlabel{sec:decomposition}{{2.3}{3}{}{subsection.2.3}{}}
\newlabel{sec:decomposition@cref}{{[subsection][3][2]2.3}{[1][3][]3}}
\newlabel{sec:training}{{2.4}{3}{}{subsection.2.4}{}}
\newlabel{sec:training@cref}{{[subsection][4][2]2.4}{[1][3][]3}}
\citation{elhage2022toy}
\newlabel{alg:training}{{\caption@xref {alg:training}{ on input line 248}}{4}{}{equation.2.5}{}}
\newlabel{alg:training@cref}{{[subsection][4][2]2.4}{[1][4][]4}}
\newlabel{sec:intervention}{{2.5}{4}{}{subsection.2.5}{}}
\newlabel{sec:intervention@cref}{{[subsection][5][2]2.5}{[1][4][]4}}
\newlabel{tab:toy_models}{{1}{5}{Our toy models and their various properties.\relax }{table.caption.2}{}}
\newlabel{tab:toy_models@cref}{{[table][1][]1}{[1][4][]5}}
\newlabel{fig:3_tms_subnetworks_first5}{{2}{6}{Selected 5 subnetworks that L3D decomposes the TMS-in-parallel model into\relax }{figure.caption.3}{}}
\newlabel{fig:3_tms_subnetworks_first5@cref}{{[figure][2][]2}{[1][5][]6}}
\newlabel{fig:4_tms_intervention}{{3}{6}{The effect of intervening on the TMS-in-parallel model in the direction of $\text {Network}_13$. We generate 1000 inputs from the TMS input distribution (x-axis), intervene on each subnetwork with magnitude $\delta $ (color scale) and measure the change in outputs (y-axis) for each sample.\relax }{figure.caption.4}{}}
\newlabel{fig:4_tms_intervention@cref}{{[figure][3][]3}{[1][5][]6}}
\citation{cunningham2023sparse}
\newlabel{fig:5_circuit_superposition_decomposition}{{4}{7}{The subnetworks L3D decomposes the TMCS model into.\relax }{figure.caption.5}{}}
\newlabel{fig:5_circuit_superposition_decomposition@cref}{{[figure][4][]4}{[1][6][]7}}
\newlabel{fig:6_circuit_superposition_coefficients}{{5}{7}{The coefficients derived from the subnetworks compared to actual coefficients used to train TMCS.\relax }{figure.caption.6}{}}
\newlabel{fig:6_circuit_superposition_coefficients@cref}{{[figure][5][]5}{[1][6][]7}}
\newlabel{fig:7_high_rank_decomposition}{{6}{8}{Parameter representations learned by L3D for the high rank circuit decomposition task.\relax }{figure.caption.7}{}}
\newlabel{fig:7_high_rank_decomposition@cref}{{[figure][6][]6}{[1][7][]8}}
\newlabel{fig:8_squared_subnetworks}{{7}{9}{Subnetworks learned by L3D for the $X \mapsto X^2$ model/\relax }{figure.caption.8}{}}
\newlabel{fig:8_squared_subnetworks@cref}{{[figure][7][]7}{[1][8][]9}}
\newlabel{fig:9_squared_intervention}{{8}{9}{The effect of intervening on each subnetwork in the $X \mapsto X^2$ model. We generate 1000 inputs from the TMSC input distribution, intervene on each subnetwork with magnitude $\delta $ and measure the change in outputs for each sample.\relax }{figure.caption.9}{}}
\newlabel{fig:9_squared_intervention@cref}{{[figure][8][]8}{[1][8][]9}}
\citation{}
\citation{kawaguchi2016deep,choromanska2015loss,dauphin2014identifying,soudry2017exponentially}
\citation{keskar2016large,sagun2017empirical}
\bibdata{writeup.bib}
\bibcite{bereska2024mechanistic}{{1}{2024}{{Bereska \& Gavves}}{{Bereska and Gavves}}}
\bibcite{braun2025interpretability}{{2}{2025}{{Braun et~al.}}{{Braun, Bushnaq, Heimersheim, Mendel, and Sharkey}}}
\bibcite{bricken2023towards}{{3}{2023}{{Bricken et~al.}}{{Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Tamkin, and Carter}}}
\bibcite{bushnaq2024using}{{4}{2024}{{Bushnaq et~al.}}{{Bushnaq, Mendel, Heimersheim, Braun, Goldowsky-Dill, H{\"a}nni, Wu, and Hobbhahn}}}
\bibcite{bussmann2024batchtopk}{{5}{2024}{{Bussmann et~al.}}{{Bussmann, Leask, and Nanda}}}
\bibcite{choromanska2015loss}{{6}{2015}{{Choromanska et~al.}}{{Choromanska, Henaff, Mathieu, Arous, and LeCun}}}
\bibcite{cunningham2023sparse}{{7}{2023}{{Cunningham et~al.}}{{Cunningham, Ewart, Riggs, Huben, and Sharkey}}}
\bibcite{dauphin2014identifying}{{8}{2014}{{Dauphin et~al.}}{{Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and Bengio}}}
\bibcite{davies2023unifying}{{9}{2023}{{Davies et~al.}}{{Davies, Langosco, and Krueger}}}
\bibcite{elhage2022toy}{{10}{2022}{{Elhage et~al.}}{{Elhage, Hume, Olsson, Schiefer, Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, et~al.}}}
\bibcite{engels2024not}{{11}{2024{a}}{{Engels et~al.}}{{Engels, Michaud, Liao, Gurnee, and Tegmark}}}
\bibcite{engels2024decomposing}{{12}{2024{b}}{{Engels et~al.}}{{Engels, Riggs, and Tegmark}}}
\bibcite{gao2024scaling}{{13}{2024}{{Gao et~al.}}{{Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}}}
\bibcite{goodfellow2014generative}{{14}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{ho2020denoising}{{15}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{hoogland2024developmental}{{16}{2024}{{Hoogland et~al.}}{{Hoogland, Wang, Farrugia-Roberts, Carroll, Wei, and Murfet}}}
\bibcite{kawaguchi2016deep}{{17}{2016}{{Kawaguchi}}{{}}}
\bibcite{keskar2016large}{{18}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kramar2024atp}{{19}{2024}{{Kram{\'a}r et~al.}}{{Kram{\'a}r, Lieberum, Shah, and Nanda}}}
\bibcite{lindsey2024sparse}{{20}{2024}{{Lindsey et~al.}}{{Lindsey, Templeton, Marcus, Conerly, Batson, and Olah}}}
\bibcite{matena2023npeff}{{21}{2023}{{Matena \& Raffel}}{{Matena and Raffel}}}
\bibcite{merullo2024talking}{{22}{2024}{{Merullo et~al.}}{{Merullo, Eickhoff, and Pavlick}}}
\bibcite{mnih2015human}{{23}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.}}}
\bibcite{nanda2023attribution}{{24}{2023}{{Nanda}}{{}}}
\bibcite{pascanu2013difficulty}{{25}{2013}{{Pascanu}}{{}}}
\bibcite{sagun2017empirical}{{26}{2017}{{Sagun et~al.}}{{Sagun, Evci, Guney, Dauphin, and Bottou}}}
\bibcite{soudry2017exponentially}{{27}{2017}{{Soudry \& Hoffer}}{{Soudry and Hoffer}}}
\bibcite{subramani2022extracting}{{28}{2022}{{Subramani et~al.}}{{Subramani, Suresh, and Peters}}}
\bibcite{syed2023attribution}{{29}{2023}{{Syed et~al.}}{{Syed, Rager, and Conmy}}}
\bibcite{turner2023steering}{{30}{2023}{{Turner et~al.}}{{Turner, Thiergart, Leech, Udell, Vazquez, Mini, and MacDiarmid}}}
\bibcite{wang2024loss}{{31}{2024}{{Wang et~al.}}{{Wang, Farrugia-Roberts, Hoogland, Carroll, Wei, and Murfet}}}
\bibcite{watanabe2000algebraic}{{32}{2000}{{Watanabe}}{{}}}
\bibcite{watanabe2005algebraic}{{33}{2005}{{Watanabe}}{{}}}
\bibcite{watanabe2007almost}{{34}{2007}{{Watanabe}}{{}}}
\bibcite{wei2022deep}{{35}{2022}{{Wei et~al.}}{{Wei, Murfet, Gong, Li, Gell-Redman, and Quella}}}
\bibstyle{icml2025}
\newlabel{sec:low_rank}{{B.1}{15}{}{subsection.B.1}{}}
\newlabel{sec:low_rank@cref}{{[subappendix][1][2147483647,2]B.1}{[1][15][]15}}
\citation{}
\newlabel{sec:finetuning}{{B.2}{16}{}{subsection.B.2}{}}
\newlabel{sec:finetuning@cref}{{[subappendix][2][2147483647,2]B.2}{[1][16][]16}}
\newlabel{sec:impact}{{B.3}{16}{}{subsection.B.3}{}}
\newlabel{sec:impact@cref}{{[subappendix][3][2147483647,2]B.3}{[1][16][]16}}
\newlabel{sec:toymodel_hyperparams}{{B.4}{16}{}{subsection.B.4}{}}
\newlabel{sec:toymodel_hyperparams@cref}{{[subappendix][4][2147483647,2]B.4}{[1][16][]16}}
\newlabel{sec:L3D_hyperparams}{{B.5}{16}{}{subsection.B.5}{}}
\newlabel{sec:L3D_hyperparams@cref}{{[subappendix][5][2147483647,2]B.5}{[1][16][]16}}
\newlabel{fig:s1_high_rank_circuit_setup}{{S1}{17}{The full architecture of high rank circuit toy model (model C).\relax }{figure.caption.11}{}}
\newlabel{fig:s1_high_rank_circuit_setup@cref}{{[figure][1][2147483647]S1}{[1][16][]17}}
\newlabel{fig:s2_tms_encoder_directions}{{S2}{17}{Encoder directions in the hidden layer dimension of the TMS toy model.\relax }{figure.caption.12}{}}
\newlabel{fig:s2_tms_encoder_directions@cref}{{[figure][2][2147483647]S2}{[1][16][]17}}
\newlabel{fig:s3_tms_full_subnetworks}{{S3}{18}{All subnetworks for the TMS decomposition\relax }{figure.caption.13}{}}
\newlabel{fig:s3_tms_full_subnetworks@cref}{{[figure][3][2147483647]S3}{[1][16][]18}}
\newlabel{fig:s3_tms_interventions}{{S4}{19}{Effects of intervening on the first 5 subnetworks of the TMS-in-parallel model.\relax }{figure.caption.14}{}}
\newlabel{fig:s3_tms_interventions@cref}{{[figure][4][2147483647]S4}{[1][16][]19}}
\newlabel{fig:s5_high_rank_circuits_loss_vs_rank}{{S5}{20}{Loss vs Rank\relax }{figure.caption.15}{}}
\newlabel{fig:s5_high_rank_circuits_loss_vs_rank@cref}{{[figure][5][2147483647]S5}{[1][16][]20}}
\newlabel{fig:s6_high_rank_decompositions}{{S6}{20}{Decomposing the toy model of high rank circuits into different numbers of subnetworks\relax }{figure.caption.16}{}}
\newlabel{fig:s6_high_rank_decompositions@cref}{{[figure][6][2147483647]S6}{[1][16][]20}}
\newlabel{fig:s7_squared_intervention_more_deltas}{{S7}{21}{Effects of intervening on each of the subnetworks of the $X \mapsto X^2$ model.\relax }{figure.caption.17}{}}
\newlabel{fig:s7_squared_intervention_more_deltas@cref}{{[figure][7][2147483647]S7}{[1][16][]21}}
\newlabel{fig:s8_squared_intervention_middle_weights}{{S8}{22}{Effect of intervening using just the weights and biases of the middle hidden layers in the subnetworks of the $X \mapsto X^2$ model.\relax }{figure.caption.18}{}}
\newlabel{fig:s8_squared_intervention_middle_weights@cref}{{[figure][8][2147483647]S8}{[1][16][]22}}
\newlabel{fig:s9_squared_intervention_multi_features}{{S9}{23}{Effects of intervening with multiple subnetworks ($v_0$ on the x-axis, $v_1$ on the y-axis) at once.\relax }{figure.caption.19}{}}
\newlabel{fig:s9_squared_intervention_multi_features@cref}{{[figure][9][2147483647]S9}{[1][16][]23}}
\newlabel{fig:s10_squared_decompositions_features}{{S10}{24}{Decomposing the $X \mapsto X^2$ model into different numbers of subnetworks\relax }{figure.caption.20}{}}
\newlabel{fig:s10_squared_decompositions_features@cref}{{[figure][10][2147483647]S10}{[1][16][]24}}
\newlabel{fig:s11_squared_features_vs_loss}{{S11}{24}{Training loss vs. number of features for the $X \mapsto X^2$ model\relax }{figure.caption.21}{}}
\newlabel{fig:s11_squared_features_vs_loss@cref}{{[figure][11][2147483647]S11}{[1][16][]24}}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{24}
