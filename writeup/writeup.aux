\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bereska2024mechanistic}
\citation{gao2024scaling,cunningham2023sparse,bricken2023towards}
\citation{engels2024not,engels2024decomposing}
\citation{merullo2024talking,lindsey2024sparse}
\citation{pascanu2013difficulty,goodfellow2014generative,ho2020denoising,mnih2015human}
\citation{watanabe2007almost,watanabe2000algebraic,watanabe2005algebraic,bushnaq2024using,davies2023unifying}
\citation{wang2024loss,hoogland2024developmental}
\citation{wei2022deep}
\citation{nanda2023attribution,kramar2024atp,syed2023attribution}
\citation{turner2023steering,subramani2022extracting}
\citation{matena2023npeff}
\citation{braun2025interpretability}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1_jacobian_diagram}{{1}{2}{Decomposing a loss landscape into a set of parameter directions , where (1) the set of directions can approximately reconstruct any per-sample-pair gradient of divergence and (2) for any given sample, the curvature in most of the subnetworks directions is approximately zero.\relax }{figure.caption.1}{}}
\newlabel{fig:1_jacobian_diagram@cref}{{[figure][1][]1}{[1][2][]2}}
\newlabel{sec:setup}{{2.1}{2}{}{subsection.2.1}{}}
\newlabel{sec:setup@cref}{{[subsection][1][2]2.1}{[1][2][]2}}
\citation{bussmann2024batchtopk}
\citation{}
\newlabel{sec:criteria}{{2.2}{3}{}{subsection.2.2}{}}
\newlabel{sec:criteria@cref}{{[subsection][2][2]2.2}{[1][3][]3}}
\newlabel{eq:divergence}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:divergence@cref}{{[equation][2][]2}{[1][3][]3}}
\newlabel{sec:decomposition}{{2.3}{3}{}{subsection.2.3}{}}
\newlabel{sec:decomposition@cref}{{[subsection][3][2]2.3}{[1][3][]3}}
\newlabel{sec:training}{{2.4}{3}{}{subsection.2.4}{}}
\newlabel{sec:training@cref}{{[subsection][4][2]2.4}{[1][3][]3}}
\newlabel{sec:intervention}{{2.5}{3}{}{subsection.2.5}{}}
\newlabel{sec:intervention@cref}{{[subsection][5][2]2.5}{[1][3][]3}}
\citation{elhage2022toy}
\newlabel{alg:training}{{\caption@xref {alg:training}{ on input line 248}}{4}{}{equation.2.5}{}}
\newlabel{alg:training@cref}{{[subsection][4][2]2.4}{[1][3][]4}}
\newlabel{tab:toy_models}{{1}{5}{Toy models and their properties (transposed)\relax }{table.caption.2}{}}
\newlabel{tab:toy_models@cref}{{[table][1][]1}{[1][4][]5}}
\newlabel{fig:3_tms_subnetworks_first5}{{2}{5}{Selected 5 subnetworks that L3D decomposes the TMS-in-parallel model into\relax }{figure.caption.3}{}}
\newlabel{fig:3_tms_subnetworks_first5@cref}{{[figure][2][]2}{[1][4][]5}}
\citation{cunningham2023sparse}
\newlabel{fig:4_tms_intervention}{{3}{6}{The effect of intervening on the TMS-in-parallel model in the direction of $\text {Network}_1$.\relax }{figure.caption.4}{}}
\newlabel{fig:4_tms_intervention@cref}{{[figure][3][]3}{[1][5][]6}}
\newlabel{fig:5_circuit_superposition_decomposition}{{4}{7}{The learned subnetworks of TMCS\relax }{figure.caption.5}{}}
\newlabel{fig:5_circuit_superposition_decomposition@cref}{{[figure][4][]4}{[1][6][]7}}
\newlabel{fig:6_circuit_superposition_coefficients}{{5}{7}{The coefficients derived from the subnetworks compared to actual coefficients\relax }{figure.caption.6}{}}
\newlabel{fig:6_circuit_superposition_coefficients@cref}{{[figure][5][]5}{[1][6][]7}}
\newlabel{fig:7_high_rank_decomposition}{{6}{8}{Parameter representations learned by L3D for the high rank circuit decomposition task.\relax }{figure.caption.7}{}}
\newlabel{fig:7_high_rank_decomposition@cref}{{[figure][6][]6}{[1][6][]8}}
\newlabel{fig:8_squared_subnetworks}{{7}{8}{Subnetworks learned by L3D for the $X \mapsto X^2$ model, and effect of intervening on each subnetwork\relax }{figure.caption.8}{}}
\newlabel{fig:8_squared_subnetworks@cref}{{[figure][7][]7}{[1][7][]8}}
\newlabel{fig:9_squared_intervention}{{8}{9}{The effect of intervening on each subnetwork\relax }{figure.caption.9}{}}
\newlabel{fig:9_squared_intervention@cref}{{[figure][8][]8}{[1][8][]9}}
\bibdata{writeup.bib}
\bibcite{bereska2024mechanistic}{{1}{2024}{{Bereska \& Gavves}}{{Bereska and Gavves}}}
\bibcite{braun2025interpretability}{{2}{2025}{{Braun et~al.}}{{Braun, Bushnaq, Heimersheim, Mendel, and Sharkey}}}
\bibcite{bricken2023towards}{{3}{2023}{{Bricken et~al.}}{{Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Tamkin, and Carter}}}
\bibcite{bushnaq2024using}{{4}{2024}{{Bushnaq et~al.}}{{Bushnaq, Mendel, Heimersheim, Braun, Goldowsky-Dill, H{\"a}nni, Wu, and Hobbhahn}}}
\bibcite{bussmann2024batchtopk}{{5}{2024}{{Bussmann et~al.}}{{Bussmann, Leask, and Nanda}}}
\bibcite{cunningham2023sparse}{{6}{2023}{{Cunningham et~al.}}{{Cunningham, Ewart, Riggs, Huben, and Sharkey}}}
\bibcite{davies2023unifying}{{7}{2023}{{Davies et~al.}}{{Davies, Langosco, and Krueger}}}
\bibcite{elhage2022toy}{{8}{2022}{{Elhage et~al.}}{{Elhage, Hume, Olsson, Schiefer, Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, et~al.}}}
\bibcite{engels2024not}{{9}{2024{a}}{{Engels et~al.}}{{Engels, Michaud, Liao, Gurnee, and Tegmark}}}
\bibcite{engels2024decomposing}{{10}{2024{b}}{{Engels et~al.}}{{Engels, Riggs, and Tegmark}}}
\bibcite{gao2024scaling}{{11}{2024}{{Gao et~al.}}{{Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}}}
\bibcite{goodfellow2014generative}{{12}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{ho2020denoising}{{13}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{hoogland2024developmental}{{14}{2024}{{Hoogland et~al.}}{{Hoogland, Wang, Farrugia-Roberts, Carroll, Wei, and Murfet}}}
\bibcite{kramar2024atp}{{15}{2024}{{Kram{\'a}r et~al.}}{{Kram{\'a}r, Lieberum, Shah, and Nanda}}}
\bibcite{lindsey2024sparse}{{16}{2024}{{Lindsey et~al.}}{{Lindsey, Templeton, Marcus, Conerly, Batson, and Olah}}}
\bibcite{matena2023npeff}{{17}{2023}{{Matena \& Raffel}}{{Matena and Raffel}}}
\bibcite{merullo2024talking}{{18}{2024}{{Merullo et~al.}}{{Merullo, Eickhoff, and Pavlick}}}
\bibcite{mnih2015human}{{19}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.}}}
\bibcite{nanda2023attribution}{{20}{2023}{{Nanda}}{{}}}
\bibcite{pascanu2013difficulty}{{21}{2013}{{Pascanu}}{{}}}
\bibcite{subramani2022extracting}{{22}{2022}{{Subramani et~al.}}{{Subramani, Suresh, and Peters}}}
\bibcite{syed2023attribution}{{23}{2023}{{Syed et~al.}}{{Syed, Rager, and Conmy}}}
\bibcite{turner2023steering}{{24}{2023}{{Turner et~al.}}{{Turner, Thiergart, Leech, Udell, Vazquez, Mini, and MacDiarmid}}}
\bibcite{wang2024loss}{{25}{2024}{{Wang et~al.}}{{Wang, Farrugia-Roberts, Hoogland, Carroll, Wei, and Murfet}}}
\bibcite{watanabe2000algebraic}{{26}{2000}{{Watanabe}}{{}}}
\bibcite{watanabe2005algebraic}{{27}{2005}{{Watanabe}}{{}}}
\bibcite{watanabe2007almost}{{28}{2007}{{Watanabe}}{{}}}
\bibcite{wei2022deep}{{29}{2022}{{Wei et~al.}}{{Wei, Murfet, Gong, Li, Gell-Redman, and Quella}}}
\bibstyle{icml2025}
\newlabel{sec:low_rank}{{B.1}{13}{}{subsection.B.1}{}}
\newlabel{sec:low_rank@cref}{{[subappendix][1][2147483647,2]B.1}{[1][13][]13}}
\citation{}
\newlabel{sec:finetuning}{{B.2}{14}{}{subsection.B.2}{}}
\newlabel{sec:finetuning@cref}{{[subappendix][2][2147483647,2]B.2}{[1][14][]14}}
\newlabel{sec:impact}{{B.2.1}{14}{}{subsubsection.B.2.1}{}}
\newlabel{sec:impact@cref}{{[subsubappendix][1][2147483647,2,2]B.2.1}{[1][14][]14}}
\newlabel{sec:toymodel_hyperparams}{{B.3}{14}{}{subsection.B.3}{}}
\newlabel{sec:toymodel_hyperparams@cref}{{[subappendix][3][2147483647,2]B.3}{[1][14][]14}}
\newlabel{sec:L3D_hyperparams}{{B.4}{14}{}{subsection.B.4}{}}
\newlabel{sec:L3D_hyperparams@cref}{{[subappendix][4][2147483647,2]B.4}{[1][14][]14}}
\newlabel{fig:s1_high_rank_circuit_setup}{{S1}{15}{Full architecture of high rank circuit toy model.\relax }{figure.caption.11}{}}
\newlabel{fig:s1_high_rank_circuit_setup@cref}{{[figure][1][2147483647]S1}{[1][14][]15}}
\newlabel{fig:s2_tms_encoder_directions}{{S2}{15}{Encoder directions in TMS\relax }{figure.caption.12}{}}
\newlabel{fig:s2_tms_encoder_directions@cref}{{[figure][2][2147483647]S2}{[1][14][]15}}
\newlabel{fig:s3_tms_full_subnetworks}{{S3}{16}{All subnetworks for the TMS decomposition\relax }{figure.caption.13}{}}
\newlabel{fig:s3_tms_full_subnetworks@cref}{{[figure][3][2147483647]S3}{[1][14][]16}}
\newlabel{fig:s3_tms_interventions}{{S4}{17}{Effect of intervening on the first 5 subnetworks of the TMS-in-parallel model.\relax }{figure.caption.14}{}}
\newlabel{fig:s3_tms_interventions@cref}{{[figure][4][2147483647]S4}{[1][14][]17}}
\newlabel{fig:s5_high_rank_circuits_loss_vs_rank}{{S5}{18}{Loss vs Rank\relax }{figure.caption.15}{}}
\newlabel{fig:s5_high_rank_circuits_loss_vs_rank@cref}{{[figure][5][2147483647]S5}{[1][14][]18}}
\newlabel{fig:s6_high_rank_decompositions}{{S6}{18}{Decomposing the toy model of high rank circuits into different numbers of subnetworks\relax }{figure.caption.16}{}}
\newlabel{fig:s6_high_rank_decompositions@cref}{{[figure][6][2147483647]S6}{[1][14][]18}}
\newlabel{fig:s7_squared_intervention_more_deltas}{{S7}{19}{Effects of intervening on each of the 5 subnetworks of the $X \mapsto X^2$ model.\relax }{figure.caption.17}{}}
\newlabel{fig:s7_squared_intervention_more_deltas@cref}{{[figure][7][2147483647]S7}{[1][14][]19}}
\newlabel{fig:s8_squared_intervention_middle_weights}{{S8}{20}{Intervening on just the weights and biases of the middle hidden layers in the $X \mapsto X^2$ network.\relax }{figure.caption.18}{}}
\newlabel{fig:s8_squared_intervention_middle_weights@cref}{{[figure][8][2147483647]S8}{[1][14][]20}}
\newlabel{fig:s9_squared_intervention_multi_features}{{S9}{21}{Intervening on multiple subnetworks at once.\relax }{figure.caption.19}{}}
\newlabel{fig:s9_squared_intervention_multi_features@cref}{{[figure][9][2147483647]S9}{[1][14][]21}}
\newlabel{fig:s10_squared_decompositions_features}{{S10}{22}{Decomposing the $X \mapsto X^2$ model into different numbers of subnetworks\relax }{figure.caption.20}{}}
\newlabel{fig:s10_squared_decompositions_features@cref}{{[figure][10][2147483647]S10}{[1][14][]22}}
\newlabel{fig:s11_squared_features_vs_loss}{{S11}{22}{Training loss vs number of features for the $X \mapsto X^2$ model, and decompositions for a 3- and 5- subnetwork decomposition.\relax }{figure.caption.21}{}}
\newlabel{fig:s11_squared_features_vs_loss@cref}{{[figure][11][2147483647]S11}{[1][14][]22}}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{22}
