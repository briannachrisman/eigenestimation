\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bereska2024mechanistic}
\citation{gao2024scaling,cunningham2023sparse,bricken2023towards}
\citation{matena2023npeff}
\citation{braun2025interpretability}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1_jacobian_diagram}{{1}{1}{Decomposing a loss landscape into a set of parameter directions , where (1) the set of directions can approximately reconstruct any per-sample-pair gradient of divergence and (2) for any given sample, the curvature in most of the subnetworks directions is approximately zero.\relax }{figure.caption.1}{}}
\newlabel{fig:1_jacobian_diagram@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{bussmann2024batchtopk}
\citation{}
\newlabel{sec:setup}{{2.1}{2}{}{subsection.2.1}{}}
\newlabel{sec:setup@cref}{{[subsection][1][2]2.1}{[1][2][]2}}
\newlabel{sec:criteria}{{2.2}{2}{}{subsection.2.2}{}}
\newlabel{sec:criteria@cref}{{[subsection][2][2]2.2}{[1][2][]2}}
\newlabel{eq:divergence}{{2}{2}{}{equation.2.2}{}}
\newlabel{eq:divergence@cref}{{[equation][2][]2}{[1][2][]2}}
\newlabel{sec:decomposition}{{2.3}{2}{}{subsection.2.3}{}}
\newlabel{sec:decomposition@cref}{{[subsection][3][2]2.3}{[1][2][]2}}
\newlabel{sec:training}{{2.4}{2}{}{subsection.2.4}{}}
\newlabel{sec:training@cref}{{[subsection][4][2]2.4}{[1][2][]2}}
\citation{elhage2022toy}
\newlabel{alg:training}{{\caption@xref {alg:training}{ on input line 249}}{3}{}{equation.2.5}{}}
\newlabel{alg:training@cref}{{[subsection][4][2]2.4}{[1][3][]3}}
\newlabel{sec:intervention}{{2.5}{3}{}{subsection.2.5}{}}
\newlabel{sec:intervention@cref}{{[subsection][5][2]2.5}{[1][3][]3}}
\newlabel{tab:toy_models}{{1}{4}{Toy models and their properties (transposed)\relax }{table.caption.2}{}}
\newlabel{tab:toy_models@cref}{{[table][1][]1}{[1][3][]4}}
\citation{cunningham2023sparse}
\newlabel{fig:3_tms_subnetworks_first5}{{2}{5}{Selected 5 subnetworks that L3D decomposes the TMS-in-parallel model into\relax }{figure.caption.3}{}}
\newlabel{fig:3_tms_subnetworks_first5@cref}{{[figure][2][]2}{[1][4][]5}}
\newlabel{fig:4_tms_intervention}{{3}{5}{The effect of intervening on the TMS-in-parallel model in the direction of $\text {Network}_1$.\relax }{figure.caption.4}{}}
\newlabel{fig:4_tms_intervention@cref}{{[figure][3][]3}{[1][4][]5}}
\newlabel{fig:5_circuit_superposition_decomposition}{{4}{6}{The learned subnetworks of TMCS\relax }{figure.caption.5}{}}
\newlabel{fig:5_circuit_superposition_decomposition@cref}{{[figure][4][]4}{[1][5][]6}}
\newlabel{fig:6_circuit_superposition_coefficients}{{5}{6}{The coefficients derived from the subnetworks compared to actual coefficients\relax }{figure.caption.6}{}}
\newlabel{fig:6_circuit_superposition_coefficients@cref}{{[figure][5][]5}{[1][5][]6}}
\newlabel{fig:7_high_rank_decomposition}{{6}{7}{Parameter representations learned by L3D for the high rank circuit decomposition task.\relax }{figure.caption.7}{}}
\newlabel{fig:7_high_rank_decomposition@cref}{{[figure][6][]6}{[1][6][]7}}
\newlabel{fig:8_squared_subnetworks}{{7}{8}{Subnetworks learned by L3D for the $X \mapsto X^2$ model, and effect of intervening on each subnetwork\relax }{figure.caption.8}{}}
\newlabel{fig:8_squared_subnetworks@cref}{{[figure][7][]7}{[1][7][]8}}
\newlabel{fig:9_squared_intervention}{{8}{8}{The effect of intervening on each subnetwork\relax }{figure.caption.9}{}}
\newlabel{fig:9_squared_intervention@cref}{{[figure][8][]8}{[1][7][]8}}
\bibdata{writeup.bib}
\bibcite{bereska2024mechanistic}{{1}{2024}{{Bereska \& Gavves}}{{Bereska and Gavves}}}
\bibcite{braun2025interpretability}{{2}{2025}{{Braun et~al.}}{{Braun, Bushnaq, Heimersheim, Mendel, and Sharkey}}}
\bibcite{bricken2023towards}{{3}{2023}{{Bricken et~al.}}{{Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Tamkin, and Carter}}}
\bibcite{bussmann2024batchtopk}{{4}{2024}{{Bussmann et~al.}}{{Bussmann, Leask, and Nanda}}}
\bibcite{cunningham2023sparse}{{5}{2023}{{Cunningham et~al.}}{{Cunningham, Ewart, Riggs, Huben, and Sharkey}}}
\bibcite{elhage2022toy}{{6}{2022}{{Elhage et~al.}}{{Elhage, Hume, Olsson, Schiefer, Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, et~al.}}}
\bibcite{gao2024scaling}{{7}{2024}{{Gao et~al.}}{{Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}}}
\bibcite{matena2023npeff}{{8}{2023}{{Matena \& Raffel}}{{Matena and Raffel}}}
\bibstyle{icml2025}
\newlabel{sec:low_rank}{{B.1}{11}{}{subsection.B.1}{}}
\newlabel{sec:low_rank@cref}{{[subappendix][1][2147483647,2]B.1}{[1][11][]11}}
\citation{}
\newlabel{sec:finetuning}{{B.2}{12}{}{subsection.B.2}{}}
\newlabel{sec:finetuning@cref}{{[subappendix][2][2147483647,2]B.2}{[1][12][]12}}
\newlabel{sec:impact}{{B.2.1}{12}{}{subsubsection.B.2.1}{}}
\newlabel{sec:impact@cref}{{[subsubappendix][1][2147483647,2,2]B.2.1}{[1][12][]12}}
\newlabel{sec:toymodel_hyperparams}{{B.3}{12}{}{subsection.B.3}{}}
\newlabel{sec:toymodel_hyperparams@cref}{{[subappendix][3][2147483647,2]B.3}{[1][12][]12}}
\newlabel{sec:L3D_hyperparams}{{B.4}{12}{}{subsection.B.4}{}}
\newlabel{sec:L3D_hyperparams@cref}{{[subappendix][4][2147483647,2]B.4}{[1][12][]12}}
\newlabel{fig:s1_high_rank_circuit_setup}{{S1}{13}{Full architecture of high rank circuit toy model.\relax }{figure.caption.11}{}}
\newlabel{fig:s1_high_rank_circuit_setup@cref}{{[figure][1][2147483647]S1}{[1][12][]13}}
\newlabel{fig:s2_tms_encoder_directions}{{S2}{13}{Encoder directions in TMS\relax }{figure.caption.12}{}}
\newlabel{fig:s2_tms_encoder_directions@cref}{{[figure][2][2147483647]S2}{[1][12][]13}}
\newlabel{fig:s3_tms_full_subnetworks}{{S3}{14}{All subnetworks for the TMS decomposition\relax }{figure.caption.13}{}}
\newlabel{fig:s3_tms_full_subnetworks@cref}{{[figure][3][2147483647]S3}{[1][12][]14}}
\newlabel{fig:s3_tms_interventions}{{S4}{15}{Effect of intervening on the first 5 subnetworks of the TMS-in-parallel model.\relax }{figure.caption.14}{}}
\newlabel{fig:s3_tms_interventions@cref}{{[figure][4][2147483647]S4}{[1][12][]15}}
\newlabel{fig:s5_high_rank_circuits_loss_vs_rank}{{S5}{16}{Loss vs Rank\relax }{figure.caption.15}{}}
\newlabel{fig:s5_high_rank_circuits_loss_vs_rank@cref}{{[figure][5][2147483647]S5}{[1][12][]16}}
\newlabel{fig:s6_high_rank_decompositions}{{S6}{16}{Decomposing the toy model of high rank circuits into different numbers of subnetworks\relax }{figure.caption.16}{}}
\newlabel{fig:s6_high_rank_decompositions@cref}{{[figure][6][2147483647]S6}{[1][12][]16}}
\newlabel{fig:s7_squared_intervention_more_deltas}{{S7}{17}{Effects of intervening on each of the 5 subnetworks of the $X \mapsto X^2$ model.\relax }{figure.caption.17}{}}
\newlabel{fig:s7_squared_intervention_more_deltas@cref}{{[figure][7][2147483647]S7}{[1][12][]17}}
\newlabel{fig:s8_squared_intervention_middle_weights}{{S8}{18}{Intervening on just the weights and biases of the middle hidden layers in the $X \mapsto X^2$ network.\relax }{figure.caption.18}{}}
\newlabel{fig:s8_squared_intervention_middle_weights@cref}{{[figure][8][2147483647]S8}{[1][12][]18}}
\newlabel{fig:s9_squared_intervention_multi_features}{{S9}{19}{Intervening on multiple subnetworks at once.\relax }{figure.caption.19}{}}
\newlabel{fig:s9_squared_intervention_multi_features@cref}{{[figure][9][2147483647]S9}{[1][12][]19}}
\newlabel{fig:s10_squared_decompositions_features}{{S10}{20}{Decomposing the $X \mapsto X^2$ model into different numbers of subnetworks\relax }{figure.caption.20}{}}
\newlabel{fig:s10_squared_decompositions_features@cref}{{[figure][10][2147483647]S10}{[1][12][]20}}
\newlabel{fig:s11_squared_features_vs_loss}{{S11}{20}{Training loss vs number of features for the $X \mapsto X^2$ model, and decompositions for a 3- and 5- subnetwork decomposition.\relax }{figure.caption.21}{}}
\newlabel{fig:s11_squared_features_vs_loss@cref}{{[figure][11][2147483647]S11}{[1][12][]20}}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{20}
