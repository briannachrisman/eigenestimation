{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import importlib\n",
        "import gc\n",
        "import copy\n",
        "\n",
        "# Third-party imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "\n",
        "# Local imports\n",
        "import toy_models.xornet\n",
        "import toy_models.tms\n",
        "import toy_models.train\n",
        "import toy_models.transformer_wrapper\n",
        "import eigenestimation_algorithm.train\n",
        "import eigenestimation_algorithm.eigenestimation\n",
        "import eigenestimation_algorithm.evaluation\n",
        "\n",
        "# Reload modules for interactive sessions\n",
        "importlib.reload(toy_models.xornet)\n",
        "importlib.reload(toy_models.tms)\n",
        "importlib.reload(toy_models.train)\n",
        "importlib.reload(toy_models.transformer_wrapper)\n",
        "importlib.reload(eigenestimation_algorithm.train)\n",
        "importlib.reload(eigenestimation_algorithm.eigenestimation)\n",
        "importlib.reload(eigenestimation_algorithm.evaluation)\n",
        "\n",
        "# Specific imports from local modules\n",
        "from toy_models.xornet import XORNet, GenerateXORData\n",
        "from toy_models.tms import Autoencoder, GenerateTMSData\n",
        "from toy_models.train import TrainModel\n",
        "from toy_models.transformer_wrapper import TransformerWrapper, DeleteParams, KLDivergenceLoss\n",
        "from eigenestimation_algorithm.eigenestimation import EigenEstimation\n",
        "from eigenestimation_algorithm.train import TrainEigenEstimation\n",
        "from eigenestimation_algorithm.evaluation import (\n",
        "    PrintFeatureVals,\n",
        "    ActivatingExamples,\n",
        "    PrintFeatureValsTransformer,\n",
        "    PrintActivatingExamplesTransformer,\n",
        ")\n",
        "\n",
        "# Device configuration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(eigenestimation_algorithm.evaluation)\n",
        "\n",
        "from eigenestimation_algorithm.evaluation import (\n",
        "    PrintFeatureVals,\n",
        "    ActivatingExamples,\n",
        "    PrintFeatureValsTransformer,\n",
        "    PrintActivatingExamplesTransformer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toy Models\n",
        "\n",
        "Setup toy models, generate/pull data, and train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XORNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'eigenestimation_algorithm.evaluation' from '/home/ubuntu/brianna-interpretability/eigenestimation/eigenestimation_algorithm/evaluation.py'>"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_xornet, Y_xornet, dataloader_xornet = GenerateXORData(n_repeats=100, batch_size=24)\n",
        "model_xornet = XORNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 300, Loss: 0.0003058672300539911\n"
          ]
        }
      ],
      "source": [
        "X_xornet, Y_xornet, dataloader_xornet = GenerateXORData(n_repeats=100, batch_size=24)\n",
        "model_xornet = XORNet().to(device)\n",
        "\n",
        "\n",
        "_, _, _ =TrainModel(\n",
        "    model=model_xornet,\n",
        "    criterion=nn.MSELoss(),\n",
        "    learning_rate=.01,\n",
        "    dataloader=dataloader_xornet,\n",
        "    n_epochs=1000,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "9Wj0Z4mdTQAO",
        "outputId": "6f33c69e-b0be-476c-aa99-de20e16a233c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 300, Loss: 0.03502044454216957\n",
            "Epoch 400, Loss: 0.0001329688384430483\n",
            "Epoch 500, Loss: 8.670073839311954e-06\n",
            "Epoch 600, Loss: 0.0001047763362294063\n",
            "Epoch 700, Loss: 3.223416570108384e-05\n",
            "Epoch 800, Loss: 0.03689640387892723\n",
            "Epoch 900, Loss: 0.027663107961416245\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Train TMS\n",
        "n_features = 5\n",
        "hidden_dim = 2\n",
        "n_datapoints = 1024\n",
        "sparsity = .075\n",
        "\n",
        "batch_size = 12\n",
        "learning_rate = .1\n",
        "n_epochs = 1000\n",
        "\n",
        "X_tms, Y_tms, dataloader_TMS = GenerateTMSData(\n",
        "    num_features=n_features, num_datapoints=n_datapoints, sparsity=sparsity, batch_size=batch_size)\n",
        "tms_model = Autoencoder(n_features, hidden_dim).to(device)\n",
        "_, _, _ = TrainModel(tms_model, nn.MSELoss(), learning_rate, dataloader_TMS, n_epochs=n_epochs, device=device)\n",
        "\n",
        "\n",
        "# Plot TMS representations.\n",
        "en = copy.deepcopy(tms_model.W).detach().cpu().numpy()\n",
        "\n",
        "for i in range(en.shape[1]):\n",
        "  plt.plot([0, en[0,i]], [0,en[1,i]], 'b-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGTF2lp-813s"
      },
      "source": [
        "## 2-layer transformer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-JyFbkBhuOlR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2 into HookedTransformer\n",
            "2359296\n",
            "transformer.blocks.3.attn.W_Q 589824\n",
            "transformer.blocks.3.attn.W_O 589824\n",
            "transformer.blocks.3.attn.W_K 589824\n",
            "transformer.blocks.3.attn.W_V 589824\n"
          ]
        }
      ],
      "source": [
        "# @title Import pretrained gpt2 (2 layers)\n",
        "# Disable fused kernels (FlashAttention and memory-efficient attention)\n",
        "# We have to disable this to compute second-order gradients on transformer models.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
        "import transformer_lens\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "\n",
        "# Ensure the math kernel is enabled (it is True by default)\n",
        "torch.backends.cuda.enable_math_sdp(True)\n",
        "\n",
        "# Load in a 2-L GPT2.\n",
        "#config = GPT2Config.from_pretrained('gpt2', n_layer=2)\n",
        "#gpt2 = GPT2Model.from_pretrained('gpt2', config=config)\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",)\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "#transformer_model = TransformerWrapper(gpt2, tokenizer)\n",
        "\n",
        "\n",
        "gpt2  = transformer_lens.HookedTransformer.from_pretrained('gpt2')\n",
        "#gpt2  = transformer_lens.HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\")#GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "transformer_model = TransformerWrapper(gpt2, tokenizer)\n",
        "\n",
        "# Make the eigenestimation a little smaller but only looking at a subset of the parameters.\n",
        "# Pick a random subset of tensors to include in paramters, and turn the rest into frozen buffers.\n",
        "params_to_delete = [name for name, param in transformer_model.named_parameters()]\n",
        "params_to_delete = [p for p in params_to_delete if 'blocks.3.attn.W_' not in p]#!='transformer.h.1.ln_2.weight']\n",
        "\n",
        "# Delete 3/4 of the parameters.\n",
        "#for p in (params_to_delete[::20]):\n",
        "#  params_to_delete.remove(p)\n",
        "\n",
        "DeleteParams(transformer_model, params_to_delete)\n",
        "\n",
        "print(sum([p.numel() for p in transformer_model.parameters()]))\n",
        "for n,p in transformer_model.named_parameters(): print(n, p.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=10):   0%|          | 0/2199 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2631 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2279 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=10):  10%|█         | 220/2199 [00:00<00:02, 671.73 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2154 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2135 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2073 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=10):  60%|██████    | 1320/2199 [00:00<00:00, 3645.82 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2808 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2208 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2874 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=10): 100%|██████████| 2199/2199 [00:00<00:00, 3269.03 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load in data.\n",
        "tinystories_dataset = dataset = load_dataset('roneneldan/TinyStories', split=\"validation[:10%]\")\n",
        "X_transformer= tokenize_and_concatenate(tinystories_dataset, tokenizer, max_length = 32, add_bos_token=True)['tokens']\n",
        "transformer_dataloader = DataLoader(X_transformer, batch_size=24, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([14100, 32])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_transformer.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F3k4QTXsbgM"
      },
      "source": [
        "# Eigenestimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz2siQHyQWZ0"
      },
      "source": [
        "# Tests on Toy Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNRhg3iQ2yk"
      },
      "source": [
        "## Xornet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7gdVpNvP5oU",
        "outputId": "fdbd20b8-606b-4cab-e291-154210b81316"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_xornet' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      9\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 10\u001b[0m eigenmodel_xornet \u001b[38;5;241m=\u001b[39m EigenEstimation(\u001b[43mmodel_xornet\u001b[49m, nn\u001b[38;5;241m.\u001b[39mMSELoss, n_u_vectors)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m dataloader_xornet_eigen \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     13\u001b[0m     einops\u001b[38;5;241m.\u001b[39mrepeat(X_xornet, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms f -> (s r) f\u001b[39m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m=\u001b[39mrepeats), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m TrainEigenEstimation(eigenmodel_xornet, dataloader_xornet_eigen, learning_rate, n_epochs, lambda_penalty, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_xornet' is not defined"
          ]
        }
      ],
      "source": [
        "n_u_vectors = 3\n",
        "batch_size = 32\n",
        "lambda_penalty = 1\n",
        "repeats = 24\n",
        "n_epochs = 100\n",
        "learning_rate = .01\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "eigenmodel_xornet = EigenEstimation(model_xornet, nn.MSELoss, n_u_vectors).to(device)\n",
        "\n",
        "dataloader_xornet_eigen = DataLoader(\n",
        "    einops.repeat(X_xornet, 's f -> (s r) f', r=repeats), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "TrainEigenEstimation(eigenmodel_xornet, dataloader_xornet_eigen, learning_rate, n_epochs, lambda_penalty, device=device)\n",
        "\n",
        "# Clear cuda cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUGWSKrfRJOv",
        "outputId": "79eaa80b-8a64-4671-c74c-301123572273"
      },
      "outputs": [],
      "source": [
        "# Look at features\n",
        "PrintFeatureVals(X_xornet, eigenmodel_xornet)\n",
        "\n",
        "for f_idx in range(eigenmodel_xornet.n_u_vectors):\n",
        "  sample, val = ActivatingExamples(X_xornet, eigenmodel_xornet, f_idx, 3)\n",
        "  print('feature', f_idx)\n",
        "  for s, v in zip(sample, val):\n",
        "    print(s, '->', v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCx0rhdoTGJx"
      },
      "source": [
        "## TMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLj93rI3TIJ9",
        "outputId": "b45eb47a-3e70-4927-8785-a1722ff46080"
      },
      "outputs": [],
      "source": [
        "#@title Train Eigenmodel\n",
        "n_u_vectors = 5\n",
        "batch_size = 24\n",
        "lambda_penalty = 10\n",
        "n_epochs = 100\n",
        "learning_rate = .01\n",
        "\n",
        "\n",
        "dataloader = DataLoader(X_tms, batch_size=batch_size, shuffle=True,\n",
        "                               generator=torch.Generator(device='cuda'))\n",
        "eigenmodel_tms = EigenEstimation(tms_model, nn.MSELoss, n_u_vectors)\n",
        "TrainEigenEstimation(eigenmodel_tms, dataloader, learning_rate, n_epochs, lambda_penalty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLJPlPcZTIKK",
        "outputId": "f75e7cf5-9dd5-4829-f6cd-cfecaf2daa08"
      },
      "outputs": [],
      "source": [
        "#@title Look at features\n",
        "X = X_tms[:10]\n",
        "PrintFeatureVals(X_tms[:10], eigenmodel_tms)\n",
        "\n",
        "for f_idx in range(eigenmodel_tms.n_u_vectors):\n",
        "  sample, val = ActivatingExamples(X_tms[:100,], eigenmodel_tms, f_idx, 3)\n",
        "  print('feature', f_idx)\n",
        "  for s, v in zip(sample, val):\n",
        "    print(s.round(3), '->', v.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBq1uAotxJB_"
      },
      "source": [
        "## 2L Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "142"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_transformer[::100,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6973GptxH2T",
        "outputId": "bcae6bb6-fe6d-4226-8b7f-c93bb10b472f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 : 446557888.000,  High Hessian Loss: -222594.219,  Basis Loss: 446780512.000\n",
            "Epoch 1 : -0.000,  High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 2 : -0.000,  High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 3 : -0.000,  High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 4 : -0.000,  High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 5 : -0.000,  High Hessian Loss: -0.000,  Basis Loss: 0.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x766d59c87010>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 : -0.000,  High Hessian Loss: -0.000,  Basis Loss: 0.000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m x_transformer_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(X_transformer[::\u001b[38;5;241m100\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m eigenmodel_transformer \u001b[38;5;241m=\u001b[39m EigenEstimation(transformer_model\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m     16\u001b[0m                           KLDivergenceLoss, n_u_vectors, u_chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mTrainEigenEstimation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43meigenmodel_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_transformer_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mu_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/train.py:50\u001b[0m, in \u001b[0;36mTrainEigenEstimation\u001b[0;34m(eigenmodel, x_dataloader, lr, n_epochs, lambda_penalty, u_batch_size, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m dH_du \u001b[38;5;241m=\u001b[39m \u001b[43meigenmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m high_H_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m(dH_du\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m*\u001b[39mu\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39meigenmodel\u001b[38;5;241m.\u001b[39mu\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m#/x.numel()\u001b[39;00m\n\u001b[1;32m     55\u001b[0m high_H_loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Step backward\u001b[39;00m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:88\u001b[0m, in \u001b[0;36mEigenEstimation.forward\u001b[0;34m(self, x, parameters)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, parameters) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Compute the double gradient along u\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     dH_du: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap_double_grad_along_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dH_du\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:78\u001b[0m, in \u001b[0;36mEigenEstimation.vmap_double_grad_along_u\u001b[0;34m(self, x, us)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvmap_double_grad_along_u\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, us):\n\u001b[0;32m---> 78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_grad_along_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_chunk_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mus\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:320\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     chunks_flat_args \u001b[38;5;241m=\u001b[39m _get_chunked_inputs(\n\u001b[1;32m    318\u001b[0m         flat_args, flat_in_dims, batch_size, chunk_size\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chunked_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    332\u001b[0m     func,\n\u001b[1;32m    333\u001b[0m     batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    340\u001b[0m )\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:430\u001b[0m, in \u001b[0;36m_chunked_vmap\u001b[0;34m(func, flat_in_dims, chunks_flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_rng_state(rs)\n\u001b[1;32m    429\u001b[0m     chunks_output\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 430\u001b[0m         \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    442\u001b[0m flat_output_chunks, arg_spec \u001b[38;5;241m=\u001b[39m _flatten_chunks_output(chunks_output)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# chunked output tensors are held by both `flat_output_chunks` and `chunks_output`.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# eagerly remove the reference from `chunks_output`.\u001b[39;00m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:75\u001b[0m, in \u001b[0;36mEigenEstimation.double_grad_along_u\u001b[0;34m(self, x, u)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jvp(\n\u001b[1;32m     71\u001b[0m       partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, x), (w0,), (u,)\n\u001b[1;32m     72\u001b[0m       )[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Compute the second derivative along u.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1083\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(func, primals, tangents, strict, has_aux)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;129m@exposed_in\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjvp\u001b[39m(\n\u001b[1;32m   1026\u001b[0m     func: Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     has_aux: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1032\u001b[0m ):\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m    Standing for the Jacobian-vector product, returns a tuple containing\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    the output of `func(*primals)` and the \"Jacobian of ``func`` evaluated at\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \n\u001b[1;32m   1081\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1139\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m   1138\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1139\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:70\u001b[0m, in \u001b[0;36mEigenEstimation.double_grad_along_u.<locals>.inner_jvp\u001b[0;34m(w0)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_jvp\u001b[39m(w0):\n\u001b[0;32m---> 70\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1083\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(func, primals, tangents, strict, has_aux)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;129m@exposed_in\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjvp\u001b[39m(\n\u001b[1;32m   1026\u001b[0m     func: Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     has_aux: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1032\u001b[0m ):\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m    Standing for the Jacobian-vector product, returns a tuple containing\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    the output of `func(*primals)` and the \"Jacobian of ``func`` evaluated at\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \n\u001b[1;32m   1081\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1139\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m   1138\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1139\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:52\u001b[0m, in \u001b[0;36mEigenEstimation.compute_loss\u001b[0;34m(self, x, parameters)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, parameters: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     49\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Perform a stateless functional call to the model with given parameters\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     param_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_to_parameters(parameters)\n\u001b[0;32m---> 52\u001b[0m     outputs: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Detach outputs to prevent gradients flowing back\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/functional_call.py:148\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/utils/stateless.py:298\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    294\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    296\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    297\u001b[0m ):\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/toy_models/transformer_wrapper.py:15\u001b[0m, in \u001b[0;36mTransformerWrapper.forward\u001b[0;34m(self, tokenized_X)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenized_X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Generate model outputs\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Rearrange the output to a flat batch of logits\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     probs \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:573\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    570\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 573\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:262\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    260\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    261\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(v\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 262\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_z_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_attn_result:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mload_in_4bit:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# call bitsandbytes method to dequantize and multiply\u001b[39;00m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:429\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_z_scores\u001b[0;34m(self, v, pattern)\u001b[0m\n\u001b[1;32m    421\u001b[0m v_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    422\u001b[0m     v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch key_pos head_index d_head -> batch head_index key_pos d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    424\u001b[0m pattern_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    425\u001b[0m     pattern,\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch head_index query_pos key_pos -> batch head_index query_pos key_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    427\u001b[0m )\n\u001b[1;32m    428\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_z(\n\u001b[0;32m--> 429\u001b[0m     \u001b[43meinops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpattern_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch head_index query_pos d_head -> batch query_pos head_index d_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/einops/einops.py:536\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    532\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n\u001b[0;32m--> 536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrearrange\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Train Eigenmodel\n",
        "#model = transformer_model\n",
        "n_u_vectors = 10\n",
        "batch_size = 8\n",
        "lambda_penalty = 1\n",
        "n_epochs = 10\n",
        "learning_rate = .001\n",
        "u_batch_size = 10\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "x_transformer_dataloader = DataLoader(X_transformer[::100], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "eigenmodel_transformer = EigenEstimation(transformer_model.to(device), \n",
        "                          KLDivergenceLoss, n_u_vectors, u_chunk_size=20).to(device)\n",
        "\n",
        "TrainEigenEstimation(\n",
        "    eigenmodel_transformer,\n",
        "    x_transformer_dataloader,\n",
        "    lr=learning_rate,\n",
        "    n_epochs= n_epochs,\n",
        "    lambda_penalty=lambda_penalty,\n",
        "    u_batch_size = u_batch_size,\n",
        "    device = device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "142"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_transformer[::100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.blocks.3.attn.W_Q 589824\n",
            "transformer.blocks.3.attn.W_O 589824\n",
            "transformer.blocks.3.attn.W_K 589824\n",
            "transformer.blocks.3.attn.W_V 589824\n",
            "\n",
            "\n",
            " can play with the other toys. Go away.\"newlinenewlineSam is sad. He does not want to play with the other toys.** He** wants to play ->  He (Value: 0.074)\n",
            " can play with the other toys. Go away.\"newlinenewlineSam is sad. He does not want to play with the other toys**.** He wants to play -> . (Value: 0.050)\n",
            " him to play** it** again! newlinenewlineSam smiled and said, \"Let's number the songs and play them again!\" newlinenewlineEveryone cheered and ->  it (Value: 0.049)\n",
            ",\" Lily replied with a smile. \"I love my suit.\" newlinenewlineThe bird then said, \"I have a bitter taste** in** my mouth. ->  in (Value: 0.036)\n",
            "Once** upon** a time, there was a little girl named Lily. One day, she got very sick and had to go to the hospital. Her mommy ->  upon (Value: 0.024)\n",
            "\n",
            "\n",
            " see it up close. When they reached the model, the little girl was too scared to go near itâ€”**it** looked so big!newline -> it (Value: 0.029)\n",
            " loved learning the new songs and he practiced them every day.newlinenewlineThe little bird kept singing the new songs and** he** soon became the best singer in the ->  he (Value: 0.020)\n",
            "'s have some acorns together.\"newlinenewlineSara thought about it. She realized that Tom was right. Sharing** was** fun, and it made her happy ->  was (Value: 0.015)\n",
            " fun bouncing and throwing the ball.newlinenewlineThey even made up a** game** called â€˜Deliveryâ€™. They took it in turns delivering ->  game (Value: 0.014)\n",
            " the rocks there** were** colourful birds, singing and enjoying the peaceful surroundings. The sailor was very happy and he knew this was a special place.newlinenewlineHe ->  were (Value: 0.014)\n",
            "\n",
            "\n",
            "Once** upon** a time, there were two sisters. Their names were Jane and Marie. Jane was three years old and Marie was five years old. newline ->  upon (Value: 0.037)\n",
            "Once** upon** a time, there was a small white bird. The bird wanted to go out and explore the world. newlinenewlineToday was a special day ->  upon (Value: 0.037)\n",
            "Once** upon** a time, there was a little girl named Lily. One day, she got very sick and had to go to the hospital. Her mommy ->  upon (Value: 0.037)\n",
            "Once** upon** a time, there was a little girl named Lily. Lily loved to play with her toys and her cat, Mittens. One day, Lily ->  upon (Value: 0.037)\n",
            "Once** upon** a time there was a boy named Jack. He was very intelligent, and had lots of friends.newlinenewlineOne day, Jack was walking through ->  upon (Value: 0.037)\n",
            "\n",
            "\n",
            " fun bouncing and throwing the ball.newlinenewlineThey even made up a** game** called â€˜Deliveryâ€™. They took it in turns delivering ->  game (Value: 0.045)\n",
            " to teach the other animals to dance. She said, â€œLetâ€™s dance!â**€** to her friends.newlinenewline -> € (Value: 0.039)\n",
            " daddy. One day,** Lily** went to the park to play with her friends. She was having so much fun until she fell down and hurt her knee. ->  Lily (Value: 0.028)\n",
            " put the swan in the box** and** said he would take it to a safe place.newlinenewlineAnna was happy that the helper would fix the swan ->  and (Value: 0.028)\n",
            "owing outside. newlinenewlineLily told her friend's mom about the cat and they went outside to find** it**. They found the cat and gave it ->  it (Value: 0.027)\n",
            "\n",
            "\n",
            " yielded to the childrenâ€™s** will**, bringing them hours of fun and joy.Once there was a boy named Tom. Tom was a very fair ->  will (Value: 0.023)\n",
            " the rocks there** were** colourful birds, singing and enjoying the peaceful surroundings. The sailor was very happy and he knew this was a special place.newlinenewlineHe ->  were (Value: 0.022)\n",
            " firefighter managed to save the train!newlinenewlineThe passengers clapped and cheered, they were so happy that the brave firefighter** had** saved the train. He was ->  had (Value: 0.019)\n",
            " him to play** it** again! newlinenewlineSam smiled and said, \"Let's number the songs and play them again!\" newlinenewlineEveryone cheered and ->  it (Value: 0.018)\n",
            " the rocks there were** colourful** birds, singing and enjoying the peaceful surroundings. The sailor was very happy and he knew this was a special place.newlinenewlineHe ->  colourful (Value: 0.016)\n",
            "\n",
            "\n",
            " to teach the other animals to dance. She said, â€œLetâ€™s dance!â**€** to her friends.newlinenewline -> € (Value: 0.035)\n",
            " Lily cries. Spot bar**ks**. He wants to help Lily.newlinenewlineLily's mom hears Lily and Spot. She runs to the big tree. -> ks (Value: 0.031)\n",
            " yielded to the childrenâ€™s** will**, bringing them hours of fun and joy.Once there was a boy named Tom. Tom was a very fair ->  will (Value: 0.027)\n",
            " didn't know what to do. newlinenewlineThe kind lady said that** time** would help make him feel better. She took John's hand and mentioned to ->  time (Value: 0.026)\n",
            " the rocks there** were** colourful birds, singing and enjoying the peaceful surroundings. The sailor was very happy and he knew this was a special place.newlinenewlineHe ->  were (Value: 0.025)\n",
            "\n",
            "\n",
            ",\" Lily replied with a smile. \"I love my suit.\" newlinenewlineThe bird then said, \"I have a bitter taste** in** my mouth. ->  in (Value: 0.038)\n",
            " she was! The man was so shocked, he said hello to the woman. She said hello back** and** asked him why he was looking for her. The ->  and (Value: 0.020)\n",
            "my and daddy hugged the little girl and said**,** â€œGood job!â€ newlinenewlineThe little girl smiled and was proud. -> , (Value: 0.017)\n",
            " can play with the other toys. Go away.\"newlinenewlineSam is sad. He does not want to play with the other toys.** He** wants to play ->  He (Value: 0.017)\n",
            " didn't know what to do. newlinenewlineThe kind lady said that** time** would help make him feel better. She took John's hand and mentioned to ->  time (Value: 0.016)\n",
            "\n",
            "\n",
            " - it's too dangerous.â€newlinenewlineLucy then asked, â**€**œMummy, can I have a present?â€ -> € (Value: 0.016)\n",
            " said she had lost her favourite** a**pron, the one which grandpa had lent her.newlinenewlineSally was so sad for her mum, but then ->  a (Value: 0.014)\n",
            ", \"I saw it near a foolish octopus who likes to play tricks on me.\" Finn** and** Carl went to search for the shell. They found the ->  and (Value: 0.014)\n",
            ", off** you** all go!â€ newlinenewlineJohn and his friends thanked Ms. Jones for understanding and waved goodbye as she took off on the hang ->  you (Value: 0.012)\n",
            " okay. It was fun to just keep throwing the rocks. Plus it felt nice to feel the splash on her face when she threw them. **newline**newline -> newline (Value: 0.012)\n",
            "\n",
            "\n",
            " can play with the other toys. Go away.\"newlinenewlineSam is sad. He does not want to play with the other toys.** He** wants to play ->  He (Value: 0.050)\n",
            "Once** upon** a time, there was a small white bird. The bird wanted to go out and explore the world. newlinenewlineToday was a special day ->  upon (Value: 0.047)\n",
            "Once** upon** a time, there was a little girl named Lily. One day, she got very sick and had to go to the hospital. Her mommy ->  upon (Value: 0.047)\n",
            "Once** upon** a time, there was a little girl named Lily. Lily loved to play with her toys and her cat, Mittens. One day, Lily ->  upon (Value: 0.047)\n",
            "Once** upon** a time there was a boy named Jack. He was very intelligent, and had lots of friends.newlinenewlineOne day, Jack was walking through ->  upon (Value: 0.047)\n",
            "\n",
            "\n",
            "Once** upon** a time, there were two sisters. Their names were Jane and Marie. Jane was three years old and Marie was five years old. newline ->  upon (Value: 0.045)\n",
            "Once** upon** a time, there was a little girl named Lily. One day, she got very sick and had to go to the hospital. Her mommy ->  upon (Value: 0.045)\n",
            "Once** upon** a time, there was a little girl named Lily. Lily loved to play with her toys and her cat, Mittens. One day, Lily ->  upon (Value: 0.045)\n",
            "Once** upon** a time, there was a small white bird. The bird wanted to go out and explore the world. newlinenewlineToday was a special day ->  upon (Value: 0.045)\n",
            "Once** upon** a time there was a boy named Jack. He was very intelligent, and had lots of friends.newlinenewlineOne day, Jack was walking through ->  upon (Value: 0.045)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "for n, p in eigenmodel_transformer.model.named_parameters(): print(n, p.numel())\n",
        "print('\\n')\n",
        "for i in list(range(min(20,eigenmodel_transformer.n_u_vectors))):\n",
        "  PrintActivatingExamplesTransformer(eigenmodel_transformer, X_transformer[1::10,1:], i, 5, 48, device)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.0000e+00,  2.4947e-02,  1.3816e-01, -1.9070e-01,  3.1342e-01,\n",
              "          1.2991e-01, -3.4025e-01, -8.2488e-02, -3.5166e-01, -1.4883e-01],\n",
              "        [ 2.4947e-02,  1.0000e+00, -6.0290e-02, -1.6399e-01, -2.7528e-02,\n",
              "          4.4898e-02, -6.0586e-02,  2.4235e-01,  2.9361e-02,  1.0601e-01],\n",
              "        [ 1.3816e-01, -6.0290e-02,  1.0000e+00, -3.1174e-01,  3.5248e-01,\n",
              "          2.8283e-02,  4.7661e-02, -1.4453e-01, -4.5667e-02, -2.4800e-01],\n",
              "        [-1.9070e-01, -1.6399e-01, -3.1174e-01,  1.0000e+00, -8.1278e-02,\n",
              "         -1.5037e-01,  1.2420e-01,  1.2400e-01,  2.6263e-01, -1.4811e-01],\n",
              "        [ 3.1342e-01, -2.7528e-02,  3.5248e-01, -8.1278e-02,  1.0000e+00,\n",
              "          7.1888e-02,  1.4477e-02, -1.1070e-01, -2.1617e-01, -4.1232e-01],\n",
              "        [ 1.2991e-01,  4.4898e-02,  2.8283e-02, -1.5037e-01,  7.1888e-02,\n",
              "          1.0000e+00, -8.4762e-02,  1.7639e-02,  3.7509e-02,  4.3950e-04],\n",
              "        [-3.4025e-01, -6.0586e-02,  4.7661e-02,  1.2420e-01,  1.4477e-02,\n",
              "         -8.4762e-02,  1.0000e+00,  2.4747e-02,  2.0616e-01, -2.2009e-02],\n",
              "        [-8.2488e-02,  2.4235e-01, -1.4453e-01,  1.2400e-01, -1.1070e-01,\n",
              "          1.7639e-02,  2.4747e-02,  1.0000e+00,  1.3113e-02,  1.5104e-01],\n",
              "        [-3.5166e-01,  2.9361e-02, -4.5667e-02,  2.6263e-01, -2.1617e-01,\n",
              "          3.7509e-02,  2.0616e-01,  1.3113e-02,  1.0000e+00,  9.5201e-02],\n",
              "        [-1.4883e-01,  1.0601e-01, -2.4800e-01, -1.4811e-01, -4.1232e-01,\n",
              "          4.3950e-04, -2.2009e-02,  1.5104e-01,  9.5201e-02,  1.0000e+00]],\n",
              "       device='cuda:0', grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eigenmodel_transformer.u @ eigenmodel_transformer.u.transpose(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.4123, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "tensor(-0.4123, device='cuda:0', grad_fn=<MinBackward1>)\n",
            "tensor(0.1200, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    abs(torch.tril(eigenmodel_transformer.u @ eigenmodel_transformer.u.transpose(0,1), diagonal=-1)\n",
        "    ).max())\n",
        "print((\n",
        "    torch.tril(eigenmodel_transformer.u @ eigenmodel_transformer.u.transpose(0,1), diagonal=-1)).min()\n",
        ")\n",
        "print(abs(\n",
        "    2*torch.tril(eigenmodel_transformer.u @ eigenmodel_transformer.u.transpose(0,1), diagonal=-1)).mean()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SCRATCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "hvp_revrev.<locals>.<lambda>() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \u001b[38;5;66;03m# Parameters to differentiate\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tangent_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m))   \u001b[38;5;66;03m# Tangents for each parameter\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhvp_revrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangent_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Vectorize over both the parameters (tangents) and input samples\u001b[39;00m\n\u001b[1;32m     30\u001b[0m batched_hvp \u001b[38;5;241m=\u001b[39m vmap(\u001b[38;5;28;01mlambda\u001b[39;00m t: hvp_revrev(f, x_batch, t, params))(tangent_batch)\n",
            "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mhvp_revrev\u001b[0;34m(f, primals, params, *tangents)\u001b[0m\n\u001b[1;32m     12\u001b[0m grad_f \u001b[38;5;241m=\u001b[39m grad(\u001b[38;5;28;01mlambda\u001b[39;00m p: f(primals, p))\u001b[38;5;66;03m#(params)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Perform VJP to get HVP\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m _, vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39mtangents)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vjp_fn(\u001b[38;5;241m*\u001b[39mtangents)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:338\u001b[0m, in \u001b[0;36mvjp\u001b[0;34m(func, has_aux, *primals)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;129m@exposed_in\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(func: Callable, \u001b[38;5;241m*\u001b[39mprimals, has_aux: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Standing for the vector-Jacobian product, returns a tuple containing the\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    results of ``func`` applied to ``primals`` and a function that, when\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        should not depend on the result of a context manager outside of ``f``.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:399\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
            "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mhvp_revrev.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     12\u001b[0m grad_f \u001b[38;5;241m=\u001b[39m grad(\u001b[38;5;28;01mlambda\u001b[39;00m p: f(primals, p))\u001b[38;5;66;03m#(params)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Perform VJP to get HVP\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m _, vjp_fn \u001b[38;5;241m=\u001b[39m vjp(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mgrad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m, params)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39mtangents)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vjp_fn(\u001b[38;5;241m*\u001b[39mtangents)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:399\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1449\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[0;32m-> 1449\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1451\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1407\u001b[0m, in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m diff_args \u001b[38;5;241m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1405\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_args)\n\u001b[0;32m-> 1407\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
            "\u001b[0;31mTypeError\u001b[0m: hvp_revrev.<locals>.<lambda>() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.func import vmap, vjp, grad\n",
        "\n",
        "# Define a function with parameters and inputs\n",
        "def f(x, params):\n",
        "    # Example function with parameters (a simple linear combination)\n",
        "    return (x * params).sin().sum()\n",
        "\n",
        "# HVP function that takes parameters\n",
        "def hvp_revrev(f, primals, params, *tangents):\n",
        "    # Compute gradient of f with respect to params only\n",
        "    grad_f = grad(lambda p: f(primals, p))#(params)\n",
        "    # Perform VJP to get HVP\n",
        "    _, vjp_fn = vjp(lambda p: grad_f(primals, p), params)\n",
        "    print(*tangents)\n",
        "    return vjp_fn(*tangents)\n",
        "\n",
        "# Batch sizes for inputs and parameters\n",
        "batch_size = 2048\n",
        "num_tangents = 10\n",
        "\n",
        "# Sample inputs and parameters\n",
        "x_batch = torch.randn((10, 2), requires_grad=False)  # Input, no gradient needed\n",
        "params = torch.randn(2, requires_grad=True)    # Parameters to differentiate\n",
        "tangent_batch = torch.randn((3, 2))   # Tangents for each parameter\n",
        "\n",
        "print(hvp_revrev(f, x_batch[1,:], params, tangent_batch[1,:]))\n",
        "\n",
        "# Vectorize over both the parameters (tangents) and input samples\n",
        "batched_hvp = vmap(lambda t: hvp_revrev(f, x_batch, t, params))(tangent_batch)\n",
        "\n",
        "print(batched_hvp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9411869049072266\n",
            "1.9412012100219727\n",
            "torch.Size([48, 24])\n",
            "5.047354221343994\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[-5.3644e-07, -1.5974e-05,  0.0000e+00,  ..., -2.2590e-05,\n",
              "           2.3651e-04,  7.4776e-07],\n",
              "         [ 0.0000e+00,  8.0542e-07, -6.5770e-05,  ..., -3.8147e-06,\n",
              "           5.7220e-06,  2.6703e-05],\n",
              "         [ 2.3842e-07,  3.0518e-05,  0.0000e+00,  ..., -7.4191e-05,\n",
              "           1.5259e-05,  9.1553e-05],\n",
              "         ...,\n",
              "         [ 5.0664e-07, -6.6757e-06,  1.9073e-05,  ...,  6.6757e-06,\n",
              "           3.5095e-04, -1.6223e-04],\n",
              "         [ 2.3842e-07, -1.2643e-04,  6.1035e-05,  ...,  2.2888e-05,\n",
              "          -3.9673e-04,  3.0836e-05],\n",
              "         [-8.3447e-07, -3.5381e-04, -1.5640e-04,  ..., -3.6422e-04,\n",
              "           0.0000e+00,  9.5367e-05]],\n",
              "\n",
              "        [[-5.9605e-08, -1.4496e-04, -7.6294e-06,  ...,  3.8862e-05,\n",
              "           1.1826e-04,  1.9958e-06],\n",
              "         [-8.1956e-07,  1.3017e-06,  1.0261e-05,  ...,  8.5831e-06,\n",
              "           1.2207e-04,  5.7220e-06],\n",
              "         [ 1.2293e-07,  1.5259e-05,  1.5259e-05,  ..., -1.3313e-05,\n",
              "          -1.0490e-05,  1.5259e-05],\n",
              "         ...,\n",
              "         [ 1.7136e-06, -4.1008e-05,  0.0000e+00,  ...,  7.6294e-05,\n",
              "          -1.5259e-05, -2.0325e-05],\n",
              "         [ 2.3842e-07,  2.6511e-05, -6.1035e-05,  ...,  3.8147e-06,\n",
              "          -5.3406e-05,  4.0352e-05],\n",
              "         [-1.7881e-07, -8.4496e-04, -1.1826e-04,  ...,  1.6384e-04,\n",
              "           6.1035e-05,  4.5776e-05]],\n",
              "\n",
              "        [[-2.9802e-08,  1.0490e-05, -1.1444e-05,  ..., -1.5423e-06,\n",
              "           6.4850e-05,  1.5505e-07],\n",
              "         [ 3.7253e-08,  7.3685e-08,  9.4277e-06,  ..., -1.9073e-06,\n",
              "           3.0279e-05,  9.5367e-07],\n",
              "         [ 1.4901e-08,  5.7220e-06, -9.5367e-07,  ..., -4.3968e-06,\n",
              "          -1.5259e-05,  4.5776e-05],\n",
              "         ...,\n",
              "         [ 1.0058e-07,  5.4538e-05, -3.8147e-06,  ...,  2.8610e-06,\n",
              "          -5.0545e-05, -2.5183e-05],\n",
              "         [-7.4506e-09, -3.1775e-05,  1.5259e-05,  ..., -5.7220e-06,\n",
              "          -6.1035e-05,  2.1571e-05],\n",
              "         [ 2.9802e-08, -3.4332e-05, -2.0027e-05,  ..., -1.1317e-05,\n",
              "           1.3351e-05,  0.0000e+00]],\n",
              "\n",
              "        [[-3.4273e-07,  1.1444e-05,  1.6689e-06,  ..., -1.1683e-05,\n",
              "           8.7738e-05,  3.8486e-07],\n",
              "         [ 5.9605e-08,  2.2643e-07,  6.5296e-07,  ...,  2.2650e-06,\n",
              "          -7.6294e-06,  2.3079e-04],\n",
              "         [ 1.6205e-07,  1.9073e-06,  4.7684e-07,  ..., -1.2934e-05,\n",
              "          -5.7220e-06,  6.1035e-05],\n",
              "         ...,\n",
              "         [ 6.5565e-07,  5.4836e-06, -7.1526e-06,  ..., -7.6294e-06,\n",
              "           4.9591e-05, -2.5079e-05],\n",
              "         [ 3.2783e-07, -3.8798e-05,  1.5259e-05,  ..., -1.4305e-06,\n",
              "          -7.6294e-06,  1.4773e-05],\n",
              "         [-9.6858e-08, -3.2425e-05, -1.9073e-05,  ...,  1.2719e-05,\n",
              "           4.5776e-05,  3.0518e-05]],\n",
              "\n",
              "        [[ 3.1292e-07, -9.5367e-05, -1.5259e-05,  ..., -5.4389e-07,\n",
              "           6.1035e-05,  1.9401e-07],\n",
              "         [ 4.1723e-07,  1.1694e-07,  7.7361e-06,  ...,  1.4305e-06,\n",
              "           5.3406e-05,  8.9407e-08],\n",
              "         [-1.2666e-06, -4.5776e-05,  1.5259e-05,  ...,  1.3085e-06,\n",
              "           0.0000e+00, -1.5259e-05],\n",
              "         ...,\n",
              "         [ 2.9802e-07,  1.9407e-04, -1.3351e-05,  ..., -4.2915e-06,\n",
              "          -2.1935e-05,  1.2267e-05],\n",
              "         [ 9.2387e-07, -1.0108e-04, -3.0518e-05,  ..., -6.9141e-06,\n",
              "           4.1962e-05,  5.2257e-06],\n",
              "         [-1.7881e-07, -1.9655e-05, -2.2888e-05,  ..., -4.3218e-05,\n",
              "           4.5776e-05, -5.7220e-06]]], device='cuda:0')"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch.func import jvp, grad, vjp, jacrev, jacfwd, functional_call, vmap\n",
        "from torch import vmap\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.amp import autocast\n",
        "\n",
        "n_features = 24\n",
        "\n",
        "\n",
        "import torch.profiler\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    profile_memory=True,\n",
        "    record_shapes=True\n",
        ") as prof:\n",
        "\n",
        "    # Define the neural network\n",
        "    class SimpleNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimpleNN, self).__init__()\n",
        "            # Define two hidden layers with 12 nodes each\n",
        "            self.hidden1 = nn.Linear(n_features, 12)  # 1 input node to 12 hidden nodes\n",
        "            self.hidden2 = nn.Linear(12, 11) # 12 hidden nodes to 12 hidden nodes\n",
        "            self.output = nn.Linear(11, 24)   # 12 hidden nodes to 1 output node\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.hidden1(x))  # ReLU activation function for first layer\n",
        "            x = torch.relu(self.hidden2(x))  # ReLU activation function for second layer\n",
        "            x = self.output(x)               # Output layer without activation (regression)\n",
        "            return x\n",
        "\n",
        "    # Initialize the model\n",
        "    #model = transformer_model#\n",
        "\n",
        "\n",
        "    import torch\n",
        "\n",
        "    # Flatten model parameters into a single vector\n",
        "    def parameters_to_vector(named_parameters):\n",
        "        return torch.cat([param.view(-1) for name, param in named_parameters.items()])\n",
        "\n",
        "    # Restore parameters from a vector to the dictionary format\n",
        "    def vector_to_parameters(vector, named_parameters):\n",
        "        # Create an iterator to slice vector based on parameter shapes\n",
        "        pointer = 0\n",
        "        new_params = {}\n",
        "        for name, param in named_parameters.items():\n",
        "            numel = param.numel()  # Number of elements in this parameter\n",
        "            # Slice out `numel` elements from the vector\n",
        "            new_params[name] = vector[pointer:pointer + numel].view(param.shape)\n",
        "            pointer += numel\n",
        "        return new_params\n",
        "    \n",
        "\n",
        "\n",
        "    # Define a function with both inputs and parameters\n",
        "    def f(X, params):\n",
        "        # An example function using parameters (e.g., element-wise sine with params)\n",
        "        with torch.no_grad():\n",
        "            ans =  functional_call(model, vector_to_parameters(params, template), X)\n",
        "        #with autocast(device):\n",
        "        return ans.sum(dim=-1)\n",
        "\n",
        "\n",
        "    # Function to compute the Hessian-vector product with respect to parameters\n",
        "    def hvp_revrev(f, X, params, tangents):\n",
        "        ans = jvp(\n",
        "            lambda pp: jvp(\n",
        "                lambda p: f(X,p), (pp,), (tangents,)\n",
        "                )[1], \n",
        "                (params,), (tangents,))[1]\n",
        "        return ans.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "\n",
        "    print(torch.cuda.max_memory_allocated()/1024**3)\n",
        "    # Initialize input and parameters\n",
        "    batch_size = 64\n",
        "    n_vectors = 5\n",
        "    n_vectors_per_round=2\n",
        "    #X = torch.randn((batch_size, n_features), requires_grad=False).to(device)    # Fixed input tensor\n",
        "    #model = SimpleNN().to(device)\n",
        "\n",
        "\n",
        "    model = transformer_model.to(device)\n",
        "    X = X_transformer[:batch_size,:n_features].to(device)\n",
        "\n",
        "    template = {name: param.detach().clone() for name, param in model.named_parameters()}\n",
        "\n",
        "\n",
        "    param_dict = {name:v for name, v in model.named_parameters()}\n",
        "    param_vec = parameters_to_vector(param_dict)\n",
        "    param_dict = vector_to_parameters(param_vec, param_dict)\n",
        "\n",
        "    params = parameters_to_vector(param_dict)\n",
        "    tangent = torch.randn((n_vectors, len(params))).to(device)                    # Tangent vector for HVP\n",
        "    print(torch.cuda.max_memory_allocated()/1024**3)\n",
        "    #batched_hvp = vmap(lambda t: vmap(lambda x: jvp(lambda p: jacrev(f, argnums=1)(x,p), (params,), (t,)), chunk_size=3)(X), chunk_size=3)(tangent[:1])\n",
        "    #print(batched_hvp.shape)\n",
        "    #a = hvp_revrev(f, X[1], params, tangent[1])\n",
        "    unbatched_hvp = hvp_revrev(f, X, params, tangent[1])\n",
        "   # def compute_single_hvp(tangent):\n",
        "   #     return hvp_revrev(f, X, params, tangent)\n",
        "    #batched_hvp = vmap(lambda t: hvp_revrev(f, X, params, t), chunk_size=n_vectors_per_round, in_dims=0)(tangent)\n",
        "    print(a.shape)\n",
        "    print(torch.cuda.max_memory_allocated()/1024**3)\n",
        "batched_hvp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of active CUDA tensors: 155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_15370/3593842136.py:6: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor) and obj.is_cuda]\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "def list_active_tensors():\n",
        "    tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor) and obj.is_cuda]\n",
        "    print(f\"Number of active CUDA tensors: {len(tensors)}\")\n",
        "\n",
        "list_active_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.25814247131347656\n",
            "3.144221305847168\n",
            "torch.Size([48, 24])\n"
          ]
        }
      ],
      "source": [
        "import time \n",
        "t = time.time()\n",
        "tan = tangent[1]\n",
        "\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "X = X_transformer[:48,].to(device)\n",
        "a = jvp(\n",
        "    lambda pp: jvp(\n",
        "        lambda p: f(X,p), (pp,), (tan,)\n",
        "        )[1], \n",
        "    (params,), (tan,))[1]\n",
        "\n",
        "\n",
        "print(t-time.time())\n",
        "a.shape\n",
        "print(torch.cuda.max_memory_allocated()/1024**3)\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 24])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_hvp[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 24, 768])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_hvp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0.]]], device='cuda:0')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_hvp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-50.0201, -52.5890],\n",
              "        [-51.7902, -29.2547]], device='cuda:0', grad_fn=<SumBackward1>)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jacrev(f, argnums=1)(X[1],params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 3928])"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hvp_revrev(f, X[1,:], params, tangent[1]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/toy_models/transformer_wrapper.py:15\u001b[0m, in \u001b[0;36mTransformerWrapper.forward\u001b[0;34m(self, tokenized_X)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenized_X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Generate model outputs\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_X\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Rearrange the output to a flat batch of logits\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     probs \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1030\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ],
      "source": [
        "transformer_model(X[1,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9.61838150024414\n",
            "9.67314338684082\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "print(torch.cuda.max_memory_allocated()/1024**3)\n",
        "jacrev(f, argnums=1)(X[1],params)\n",
        "#X = X_transformer[:24,].to(device)\n",
        "#transformer_model.to(device)(X.to(device))\n",
        "print(torch.cuda.max_memory_allocated()/1024**3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden1.weight True\n",
            "hidden1.bias True\n",
            "hidden2.weight True\n",
            "hidden2.bias True\n",
            "output.weight True\n",
            "output.bias True\n"
          ]
        }
      ],
      "source": [
        "for n, p in model.named_parameters():\n",
        "    print(n,p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([48, 9880])\n",
            "torch.Size([9880])\n",
            "torch.Size([48, 64])\n"
          ]
        }
      ],
      "source": [
        "print(tangent.shape)\n",
        "print(params.shape)\n",
        "print(X.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.func import jvp\n",
        "x = torch.randn([])\n",
        "f = lambda x: x * torch.tensor([1., 2., 3])\n",
        "value, grad = jvp(f, (x,), (torch.tensor(1.),))\n",
        "assert torch.allclose(value, f(x))\n",
        "assert torch.allclose(grad, torch.tensor([1., 2, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "_vjp_with_argnums.<locals>.wrapper() takes from 1 to 3 positional arguments but 2048 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[97], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m tangent_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_tangents, batch_size)      \u001b[38;5;66;03m# Tangents for each parameter\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Vectorize HVP computation over tangent vectors using vmap\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m batched_hvp \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhvp_revrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtangent_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(batched_hvp)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
            "Cell \u001b[0;32mIn[97], line 25\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     22\u001b[0m tangent_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_tangents, batch_size)      \u001b[38;5;66;03m# Tangents for each parameter\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Vectorize HVP computation over tangent vectors using vmap\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m batched_hvp \u001b[38;5;241m=\u001b[39m vmap(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mhvp_revrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)(tangent_batch)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(batched_hvp)\n",
            "Cell \u001b[0;32mIn[97], line 15\u001b[0m, in \u001b[0;36mhvp_revrev\u001b[0;34m(f, X, params, tangents)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the vector-Jacobian product to get the HVP\u001b[39;00m\n\u001b[1;32m     14\u001b[0m _, vjp_fn \u001b[38;5;241m=\u001b[39m vjp(\u001b[38;5;28;01mlambda\u001b[39;00m p: f(X, p), params)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvjp_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: _vjp_with_argnums.<locals>.wrapper() takes from 1 to 3 positional arguments but 2048 were given"
          ]
        }
      ],
      "source": [
        "#from functorch import jvp, grad, vjp\n",
        "from torch.func import jvp, grad, vjp, vmap\n",
        "def hvp(f, primals, tangents):\n",
        "  return jvp(grad(f), primals, tangents)[1]\n",
        "\n",
        "def f(x):\n",
        "  return x.sin().sum()\n",
        "\n",
        "x = torch.randn(2048)\n",
        "tangent = torch.randn(2048)\n",
        "\n",
        "result = hvp(f, (x,), (tangent,))\n",
        "\n",
        "def hvp_revrev(f, primals, tangents):\n",
        "    \n",
        "  _, vjp_fn = vjp(grad(f), *primals)\n",
        "  return vjp_fn(*tangents)\n",
        "\n",
        "result_hvp_revrev = hvp_revrev(f, (x,), (tangent,))\n",
        "print(result_hvp_revrev.shape[0])\n",
        "assert torch.allclose(result, result_hvp_revrev[0])\n",
        "result_hvp_revrev\n",
        "\n",
        "\n",
        "# Create batch of inputs and tangents\n",
        "batch_size = 2048\n",
        "x_batch = torch.randn((10,batch_size), requires_grad=True)\n",
        "\n",
        "tangent_batch = torch.randn((2,batch_size))\n",
        "\n",
        "\n",
        "#batched_hvp = vmap(lambda x: vmap(lambda t: hvp_revrev(f, (x,), (t,)))(tangent_batch))(x_batch)\n",
        "\n",
        "#print(batched_hvp[0].shape)\n",
        "#print(len(batched_hvp))\n",
        "#batched_hvp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function torch.func.vmap(func: Callable, in_dims: Union[int, Tuple] = 0, out_dims: Union[int, Tuple[int, ...]] = 0, randomness: str = 'error', *, chunk_size=None) -> Callable>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "You are attempting to call Tensor.requires_grad_() (or perhaps using torch.autograd.functional.* APIs) inside of a function being transformed by a functorch transform. This is unsupported, please attempt to use the functorch transforms (e.g. grad, vjp, jacrev, jacfwd, hessian) or call requires_grad_() outside of a function being transformed instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m U \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([param\u001b[38;5;241m.\u001b[39mview(k_vectors, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m eigenmodel_transformer\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mvalues()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Compute HVPs\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m HVPs \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhvp_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# HVPs is of shape (k_vectors, n_params)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHessian-Matrix Product H @ U:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
            "Cell \u001b[0;32mIn[49], line 28\u001b[0m, in \u001b[0;36mhvp_func\u001b[0;34m(u_i)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhvp_func\u001b[39m(u_i):\n\u001b[0;32m---> 28\u001b[0m     hvp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_i\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hvp\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/functional.py:1137\u001b[0m, in \u001b[0;36mhvp\u001b[0;34m(func, inputs, v, create_graph, strict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m   1136\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhvp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1137\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_grad_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m         _, v \u001b[38;5;241m=\u001b[39m _as_tuple(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhvp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/functional.py:88\u001b[0m, in \u001b[0;36m_grad_preprocess\u001b[0;34m(inputs, create_graph, need_graph)\u001b[0m\n\u001b[1;32m     86\u001b[0m             res\u001b[38;5;241m.\u001b[39mappend(inp\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m         res\u001b[38;5;241m.\u001b[39mappend(\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneed_graph\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(res)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You are attempting to call Tensor.requires_grad_() (or perhaps using torch.autograd.functional.* APIs) inside of a function being transformed by a functorch transform. This is unsupported, please attempt to use the functorch transforms (e.g. grad, vjp, jacrev, jacfwd, hessian) or call requires_grad_() outside of a function being transformed instead."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
        "from torch.func import vmap\n",
        "\n",
        "# Assume model, x, targets, and loss_fn are defined\n",
        "# model = ...\n",
        "# x = ...\n",
        "# targets = ...\n",
        "# loss_fn = ...\n",
        "\n",
        "def get_flat_params(model):\n",
        "    return parameters_to_vector(model.parameters())\n",
        "\n",
        "def set_flat_params(model, flat_params):\n",
        "    vector_to_parameters(flat_params, model.parameters())\n",
        "\n",
        "# Get flat parameters w0\n",
        "w0 = get_flat_params(model).detach()\n",
        "w0.requires_grad_(True)\n",
        "\n",
        "def f(w):\n",
        "    set_flat_params(model, w)\n",
        "    outputs = model(x)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    return loss\n",
        "\n",
        "def hvp_func(u_i):\n",
        "    hvp = torch.autograd.functional.hvp(f, w0, u_i)[1]\n",
        "    return hvp\n",
        "\n",
        "# Prepare U tensor\n",
        "k_vectors = eigenmodel_transformer.n_u_vectors  # Number of u vectors\n",
        "n_params = w0.numel()               # Number of parameters\n",
        "# Assuming eigenmodel._parameters contains the u vectors\n",
        "U = torch.cat([param.view(k_vectors, -1) for param in eigenmodel_transformer._parameters.values()], dim=1)\n",
        "\n",
        "# Compute HVPs\n",
        "hvp_func(U)\n",
        "#HVPs = vmap(hvp_func)(U)\n",
        "\n",
        "# HVPs is of shape (k_vectors, n_params)\n",
        "#print(\"Hessian-Matrix Product H @ U:\")\n",
        "#print(HVPs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Import pretrained gpt2 (2 layers)\n",
        "# Disable fused kernels (FlashAttention and memory-efficient attention)\n",
        "# We have to disable this to compute second-order gradients on transformer models.\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "\n",
        "# Ensure the math kernel is enabled (it is True by default)\n",
        "torch.backends.cuda.enable_math_sdp(True)\n",
        "\n",
        "# Load in a 2-L GPT2.\n",
        "config = GPT2Config.from_pretrained('gpt2', n_layer=2)\n",
        "gpt2 = GPT2Model.from_pretrained('gpt2', config=config)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "transformer_model = TransformerWrapper(gpt2, tokenizer)\n",
        "\n",
        "# Make the eigenestimation a little smaller but only looking at a subset of the parameters.\n",
        "# Pick a random subset of tensors to include in paramters, and turn the rest into frozen buffers.\n",
        "params_to_delete = [name for name, param in transformer_model.named_parameters()]\n",
        "params_to_delete = [p for p in params_to_delete if p!='transformer.h.1.ln_1.weight']\n",
        "\n",
        "# Delete 3/4 of the parameters.\n",
        "#for p in (params_to_delete[::20]):\n",
        "#  params_to_delete.remove(p)\n",
        "\n",
        "DeleteParams(transformer_model, params_to_delete)\n",
        "\n",
        "print(sum([p.numel() for p in transformer_model.parameters()]))\n",
        "for n,_ in transformer_model.named_parameters(): print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "    loss = eigenmodel_transformer.compute_loss()\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "params = {k: v.detach() for k, v in model.named_parameters()}\n",
        "buffers = {k: v.detach() for k, v in model.named_buffers()}\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, x, y)\n",
        "\n",
        "print(ft_per_sample_grads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerWrapper(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-1): 2 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 247,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch import autograd\n",
        "import time\n",
        "#model = nn.Linear(10, 20)\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "\n",
        "\n",
        "# Run your code\n",
        "\n",
        "#x = torch.randn(8, 10) # 10 elements long\n",
        "\n",
        "model = transformer_model\n",
        "t  = time.time()\n",
        "k = 2\n",
        "vecs = tuple([einops.repeat(torch.randn_like(p, requires_grad=True), '...->k ...', k=k) for p in model.parameters()])\n",
        "\n",
        "\n",
        "batch_size=10\n",
        "vec = tuple(k[0] for k in vecs)\n",
        "\n",
        "# X\n",
        "grad_batch_size = 20\n",
        "x = X_transformer[:grad_batch_size,:]\n",
        "out_model = model(x.to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.func import jvp, grad, vjp\n",
        "\n",
        "# Define a function with both inputs and parameters\n",
        "def f(X, params):\n",
        "    # An example function using parameters (e.g., element-wise sine with params)\n",
        "    return (X * params).sin().sum()\n",
        "\n",
        "# Function to compute the Hessian-vector product with respect to parameters\n",
        "def hvp_revrev(f, X, params, tangents):\n",
        "    # Compute the gradient of `f` with respect to `params`\n",
        "    grad_f = grad(lambda p: f(X, p))(params)\n",
        "    \n",
        "    # Compute the vector-Jacobian product (vjp) of this gradient with respect to `params`\n",
        "    _, vjp_fn = vjp(lambda p: grad(lambda p_: f(X, p_))(p), params)\n",
        "    \n",
        "    # Apply vjp_fn to compute the Hessian-vector product\n",
        "    return vjp_fn(tangents)\n",
        "\n",
        "# Initialize input and parameters\n",
        "batch_size = 2048\n",
        "X = torch.randn((batch_size, 100), requires_grad=False)    # Fixed input tensor\n",
        "params = torch.randn(100, requires_grad=True) # Parameters to differentiate\n",
        "tangent = torch.randn((batch_size, 100))                    # Tangent vector for HVP\n",
        "\n",
        "# Compute the Hessian-vector product with respect to params\n",
        "#result_hvp = hvp_revrev(f, X[1:,], params, tangent[1,:])\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "batched_hvp = vmap(lambda x: vmap(lambda t: hvp_revrev(f, x, params, t))(tangent))(X)\n",
        "\n",
        "print(batched_hvp[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "print(torch.cuda.reset_max_memory_allocated()/1024**3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "got 24 tensors and 2 gradients",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[46], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m t  \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([einops\u001b[38;5;241m.\u001b[39mrepeat(torch\u001b[38;5;241m.\u001b[39mrandn_like(p, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...->k ...\u001b[39m\u001b[38;5;124m'\u001b[39m, k\u001b[38;5;241m=\u001b[39mk) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()])\n\u001b[0;32m---> 18\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 160 x 20 x 10 - (batch x outputs) x (params)\u001b[39;00m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:492\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _engine_run_backward(\n\u001b[1;32m    483\u001b[0m             outputs,\n\u001b[1;32m    484\u001b[0m             gO,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m             accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    490\u001b[0m         )\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap_internals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_none_pass_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_vmap_internals.py:231\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    229\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    230\u001b[0m     )\n\u001b[0;32m--> 231\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    233\u001b[0m         _validate_outputs(batched_outputs, func)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:482\u001b[0m, in \u001b[0;36mgrad.<locals>.vjp\u001b[0;34m(gO)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: got 24 tensors and 2 gradients"
          ]
        }
      ],
      "source": [
        "from torch import autograd\n",
        "import time\n",
        "#model = nn.Linear(10, 20)\n",
        "\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "gc.collect()\n",
        "# Run your code\n",
        "\n",
        "#x = torch.randn(8, 10) # 10 elements long\n",
        "\n",
        "model = transformer_model\n",
        "#x = X_transformer[[1],:4]\n",
        "x.requires_grad=False\n",
        "\n",
        "t  = time.time()\n",
        "vecs = tuple([einops.repeat(torch.randn_like(p, requires_grad=True), '...->k ...', k=k) for p in model.parameters()])\n",
        "grads = autograd.grad(outputs=outputs, inputs=model.parameters(), create_graph=True, grad_outputs = [i for i in torch.eye(len(outputs))][:2], is_grads_batched=True) # 160 x 20 x 10 - (batch x outputs) x (params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 160 x 20 x 10 - (batch x outputs) x (params)\u001b[39;00m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:492\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _engine_run_backward(\n\u001b[1;32m    483\u001b[0m             outputs,\n\u001b[1;32m    484\u001b[0m             gO,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m             accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    490\u001b[0m         )\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap_internals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_none_pass_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_vmap_internals.py:231\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    229\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    230\u001b[0m     )\n\u001b[0;32m--> 231\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    233\u001b[0m         _validate_outputs(batched_outputs, func)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:482\u001b[0m, in \u001b[0;36mgrad.<locals>.vjp\u001b[0;34m(gO)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "grads = autograd.grad(outputs=outputs, inputs=model.parameters(), create_graph=True, grad_outputs = [i for i in torch.eye(len(outputs))], is_grads_batched=True) # 160 x 20 x 10 - (batch x outputs) x (params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.010288476943969727 timex!\n",
            "0.010380983352661133 timey!\n",
            "0.01228642463684082 time1!\n",
            "0.012312173843383789 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.015348434448242188 timex!\n",
            "0.015373945236206055 timey!\n",
            "0.016979694366455078 time1!\n",
            "0.017002582550048828 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.019487380981445312 timex!\n",
            "0.019510984420776367 timey!\n",
            "0.02113509178161621 time1!\n",
            "0.021158456802368164 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.0236208438873291 timex!\n",
            "0.023643970489501953 timey!\n",
            "0.02518177032470703 time1!\n",
            "0.025208234786987305 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.027808189392089844 timex!\n",
            "0.02783203125 timey!\n",
            "0.029371261596679688 time1!\n",
            "0.029393911361694336 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.03181171417236328 timex!\n",
            "0.031835317611694336 timey!\n",
            "0.03336381912231445 time1!\n",
            "0.033431291580200195 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.0358126163482666 timex!\n",
            "0.035836219787597656 timey!\n",
            "0.03736567497253418 time1!\n",
            "0.03738808631896973 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.03983187675476074 timex!\n",
            "0.039855241775512695 timey!\n",
            "0.04137468338012695 time1!\n",
            "0.0413968563079834 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.043810367584228516 timex!\n",
            "0.04383397102355957 timey!\n",
            "0.045363664627075195 time1!\n",
            "0.04542708396911621 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.04782748222351074 timex!\n",
            "0.0478520393371582 timey!\n",
            "0.049365997314453125 time1!\n",
            "0.04938793182373047 time2!!\n",
            "1 torch.Size([24, 768]) grads2\n",
            "1 torch.Size([24]) grads2_v\n",
            "0.31429481506347656 memory\n",
            "0.04986381530761719 time!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch import autograd\n",
        "import time\n",
        "#model = nn.Linear(10, 20)\n",
        "\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "gc.collect()\n",
        "# Run your code\n",
        "\n",
        "#x = torch.randn(8, 10) # 10 elements long\n",
        "\n",
        "model = transformer_model\n",
        "#x = X_transformer[[1],:4]\n",
        "x.requires_grad=False\n",
        "\n",
        "t  = time.time()\n",
        "vecs = tuple([einops.repeat(torch.randn_like(p, requires_grad=True), '...->k ...', k=k) for p in model.parameters()])\n",
        "grads = autograd.grad(outputs=outputs, inputs=model.parameters(), create_graph=True, grad_outputs = [i for i in torch.eye(len(outputs))], is_grads_batched=True) # 160 x 20 x 10 - (batch x outputs) x (params)\n",
        "\n",
        "x = X_transformer[1,:]\n",
        "vec = tuple(k[0] for k in vecs)\n",
        "for _ in range(10):\n",
        "\n",
        "    out_model = model(x.to(device))\n",
        "    loss = nn.MSELoss(reduction='none')(out_model, out_model.detach()).mean(dim=-1)\n",
        "\n",
        "    dims = ' '.join([f'd{i}' for i in range(len(loss.shape))])\n",
        "\n",
        "    out = loss.flatten()#einops.rearrange(loss, f'{dims} -> ({dims})') # Flatten.\n",
        "    outputs = [i for i in out]\n",
        "    print(time.time()-t, 'timex!')\n",
        "\n",
        "    print(time.time()-t, 'timey!')\n",
        "\n",
        "    #print(len(grads), grads[0].shape, 'grads')\n",
        "    #print(time.time()-t, 'time0!')\n",
        "\n",
        "    p=sum([einops.einsum(g, v, 'o ... , ... -> o') for g,v in zip(grads, vec)]) # (batch x outputs) x k.\n",
        "    #print(p.shape, 'p')\n",
        "\n",
        "    p_dims = ' '\n",
        "    p_reshape = [i for i in einops.rearrange(p, 'o->(o)')] # (batch k)\n",
        "    #print(len(p_reshape))\n",
        "    grads2 = autograd.grad(p_reshape, vec, create_graph=True, grad_outputs = [i for i in torch.eye(len(p_reshape))], is_grads_batched=True) # for each grad - (batch x outputs x k) x (params)\n",
        "    print(time.time()-t, 'time1!')\n",
        "    #grads2 = autograd.grad(p_reshape, vecs, create_graph=True, grad_outputs = [i for i in torch.eye(len(p_reshape))], is_grads_batched=True) # for each grad - (batch x outputs x k) x (params)\n",
        "    print(time.time()-t, 'time2!!')\n",
        "    print(len(grads2), grads2[0].shape, 'grads2')\n",
        "\n",
        "    #dims_grad2 =  [' '.join([f'd{i}' for idd in range(len(g.shape[1:]))]) for g in grads2]\n",
        "    #grads2_v = tuple((g**2).sum(dim=-1) for g in grads2)\n",
        "    grads2_v = tuple((g**2).sum(dim=-1) for g in grads2)\n",
        "    #print(time.time()-t, 'time3!!')\n",
        "\n",
        "    print(len(grads2_v), grads2_v[0].shape, 'grads2_v')\n",
        "\n",
        "    #grads2_v = sum([einops.einsum(g,v, 'batch outputs k ... , k ... -> batch outputs k') for g,v,d in zip(grads2_rearranged, vecs, dims)])\n",
        "\n",
        "\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    \n",
        "\n",
        "print(peak_memory, 'memory')    \n",
        "print(time.time()-t, 'time!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([480, 768])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grads[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 5            |        cudaMalloc retries: 5         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  19690 MiB |  19840 MiB |   1001 GiB |    982 GiB |\n",
            "|       from large pool |  19669 MiB |  19818 MiB |    936 GiB |    916 GiB |\n",
            "|       from small pool |     21 MiB |     81 MiB |     65 GiB |     65 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  19690 MiB |  19840 MiB |   1001 GiB |    982 GiB |\n",
            "|       from large pool |  19669 MiB |  19818 MiB |    936 GiB |    916 GiB |\n",
            "|       from small pool |     21 MiB |     81 MiB |     65 GiB |     65 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  19674 MiB |  19824 MiB |    999 GiB |    980 GiB |\n",
            "|       from large pool |  19653 MiB |  19803 MiB |    934 GiB |    915 GiB |\n",
            "|       from small pool |     21 MiB |     81 MiB |     65 GiB |     65 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  24722 MiB |  24722 MiB | 230422 MiB | 205700 MiB |\n",
            "|       from large pool |  24624 MiB |  24624 MiB | 224238 MiB | 199614 MiB |\n",
            "|       from small pool |     98 MiB |     98 MiB |   6184 MiB |   6086 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   4961 MiB |   5301 MiB | 481461 MiB | 476500 MiB |\n",
            "|       from large pool |   4954 MiB |   5291 MiB | 384885 MiB | 379930 MiB |\n",
            "|       from small pool |      6 MiB |     18 MiB |  96576 MiB |  96570 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     224    |     297    |  205042    |  204818    |\n",
            "|       from large pool |     103    |     106    |   39128    |   39025    |\n",
            "|       from small pool |     121    |     194    |  165914    |  165793    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     224    |     297    |  205042    |  204818    |\n",
            "|       from large pool |     103    |     106    |   39128    |   39025    |\n",
            "|       from small pool |     121    |     194    |  165914    |  165793    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |      79    |    3706    |    3627    |\n",
            "|       from large pool |      30    |      30    |     614    |     584    |\n",
            "|       from small pool |      49    |      49    |    3092    |    3043    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      99    |  107707    |  107644    |\n",
            "|       from large pool |      36    |      39    |    7218    |    7182    |\n",
            "|       from small pool |      27    |      62    |  100489    |  100462    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'jedi.inference.base_value.ValueSet'> S{}\n",
            "deleted\n",
            "<class 'unittest.mock._Call'> call.size()\n",
            "deleted\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3489/2896909092.py:6: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  if obj.is_cuda:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> torch.Size([1024, 1024])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([1024, 1024])\n",
            "deleted\n",
            "<class 'torch.storage.UntypedStorage'> 154389504\n",
            "<class 'torch.storage.UntypedStorage'> 3145728\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 7077888\n",
            "<class 'torch.storage.UntypedStorage'> 9216\n",
            "<class 'torch.storage.UntypedStorage'> 2359296\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 9437184\n",
            "<class 'torch.storage.UntypedStorage'> 12288\n",
            "<class 'torch.storage.UntypedStorage'> 9437184\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 7077888\n",
            "<class 'torch.storage.UntypedStorage'> 9216\n",
            "<class 'torch.storage.UntypedStorage'> 2359296\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 9437184\n",
            "<class 'torch.storage.UntypedStorage'> 12288\n",
            "<class 'torch.storage.UntypedStorage'> 9437184\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.storage.UntypedStorage'> 3072\n",
            "<class 'torch.Tensor'> torch.Size([1, 1, 1024, 1024])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([1, 1, 1024, 1024])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([15153, 24])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([50257, 768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 2304])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2304])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 2304])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2304])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([100, 4])\n",
            "deleted\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 5])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([240, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([10, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([240, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([480, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([20, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([480, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72, 2, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72, 2])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 3072])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([3, 24])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([2, 768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([768])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "deleted\n",
            "<class 'torch.Tensor'> torch.Size([72, 768])\n",
            "deleted\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prints currently alive Tensors and Variables\n",
        "import torch\n",
        "import gc\n",
        "for obj in gc.get_objects():\n",
        "    try:\n",
        "        if obj.is_cuda:\n",
        "                print(type(obj), obj.size())\n",
        "        \n",
        "                obj.detach().cpu()\n",
        "                del obj\n",
        "                print('deleted')\n",
        "                \n",
        "    except:\n",
        "        pass\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch' has no attribute 'is_cuda'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_cuda\u001b[49m(obj)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/__init__.py:2563\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2563\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'is_cuda'"
          ]
        }
      ],
      "source": [
        "torch.is_cuda(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8101683200"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.reset_peak_memory_stats()\n",
        "i = 0\n",
        "torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "    grads2 = autograd.grad(p_reshape, vecs, create_graph=True, grad_outputs = [i for i in torch.eye(len(p_reshape))], is_grads_batched=True) # for each grad - (batch x outputs x k) x (params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 1 dims. Maybe you wanted to use the vjp or jacrev APIs instead?",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size)\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (weights, examples, targets)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#grad_weight_per_example = vmap(grad(compute_grad, argnums=0), in_dims=(None, 0, 0))(*inputs)\u001b[39;00m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:399\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1449\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[0;32m-> 1449\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1451\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1422\u001b[0m, in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto return a Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1420\u001b[0m     )\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto return a scalar Tensor, got tensor with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Maybe you wanted to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse the vjp or jacrev APIs instead?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1427\u001b[0m     )\n\u001b[1;32m   1429\u001b[0m flat_diff_args, spec \u001b[38;5;241m=\u001b[39m tree_flatten(diff_args)\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 1 dims. Maybe you wanted to use the vjp or jacrev APIs instead?"
          ]
        }
      ],
      "source": [
        "from torch.func import grad, vmap\n",
        "batch_size, feature_size = 3, 5\n",
        "def model(weights, feature_vec):\n",
        "    # Very simple linear model with activation\n",
        "    assert feature_vec.dim() == 1\n",
        "    return feature_vec.dot(weights).relu()\n",
        "def compute_loss(weights, example, target):\n",
        "    y = model(weights, example)\n",
        "    return ((y - target) ** 2).mean()  # MSELoss\n",
        "\n",
        "def compute_grad(weights, example, target):\n",
        "    return jvp(compute_loss, argnums=0)(weights, example, target)\n",
        "\n",
        "\n",
        "weights = torch.randn(feature_size, requires_grad=True)\n",
        "examples = torch.randn(batch_size, feature_size)\n",
        "targets = torch.randn(batch_size)\n",
        "inputs = (weights, examples, targets)\n",
        "grad(compute_grad, argnums=0)(weights, examples[0], targets[0])\n",
        "#grad_weight_per_example = vmap(grad(compute_grad, argnums=0), in_dims=(None, 0, 0))(*inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 768])"
            ]
          },
          "execution_count": 345,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 24])"
            ]
          },
          "execution_count": 316,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([240, 768]) grads\n"
          ]
        }
      ],
      "source": [
        "print(len(grads), grads[0].shape, 'grads')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'(b o k d0 d1) -> b o k d0 d1'"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " f'(b o k {dims}) -> b o k {dims}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([7, 20, 10])"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vecs[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'p' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m10\u001b[39m, flat_params\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#p=einops.einsum('(out @ v)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241m.\u001b[39mbackward(create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(v\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m     17\u001b[0m linear\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
          ]
        }
      ],
      "source": [
        "from torch import autograd\n",
        "linear = nn.Linear(10, 20)\n",
        "\n",
        "x = torch.randn(1, 10)\n",
        "out = [i for i in (linear(x)**2)[0]]\n",
        "\n",
        "\n",
        "flat_params = torch.cat([p.view(-1) for p in linear.parameters()])# flatten.requires_grad=True\n",
        "v = torch.randn([10, flat_params.shape[0]], requires_grad=True)\n",
        "\n",
        "p=einops.einsum(out, v, )\n",
        "\n",
        "p.backward(create_graph=True)\n",
        "\n",
        "print(v.grad)\n",
        "\n",
        "linear.zero_grad()\n",
        "\n",
        "grads = autograd.grad(out, linear.parameters(), create_graph=True)\n",
        "grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "220"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flat_params.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor(1.0687, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.1070, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.8654, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0577, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0269, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0672, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0040, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.1864, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0787, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.8409, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0864, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0186, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0904, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0019, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0746, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.3169, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.1385, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0167, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.4491, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
              " tensor(0.0878, device='cuda:0', grad_fn=<UnbindBackward0>)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GradTrackingTensor(lvl=2, value=\n",
            "    BatchedTensor(lvl=1, bdim=0, value=\n",
            "        tensor([[-3.4078e-02, -2.3414e-01,  1.1105e+00,  2.0623e-01, -9.2142e-02,\n",
            "                  3.2440e-01,  2.4454e-03,  1.2116e-05,  5.1514e-02,  2.8423e-01,\n",
            "                 -3.2002e-01, -1.8888e+00, -3.6671e-11,  1.4318e+00, -1.5549e-01,\n",
            "                 -1.2171e+00,  8.1368e-03,  6.8094e+00,  7.2591e-03,  1.2072e+00,\n",
            "                  1.1051e+00,  4.1215e-02, -2.8333e-01, -1.7296e-09],\n",
            "                [ 2.5315e-02, -1.8952e-10, -9.9086e-08, -7.0067e-02,  5.0342e-04,\n",
            "                  7.4225e-04,  9.2458e-01,  5.8505e-03, -1.2408e+00, -3.8720e+00,\n",
            "                  1.2356e+01,  8.9047e-06, -7.4990e-01, -1.7808e+00,  5.2451e-02,\n",
            "                  2.3128e+00, -3.5312e-01,  1.1556e-01, -5.5430e-01,  7.9101e-01,\n",
            "                  1.8138e-01,  2.9986e-01,  1.3191e+00, -9.7872e-01],\n",
            "                [-9.1007e-02, -6.1624e-01,  2.2481e-02,  9.8138e-01, -2.8843e-02,\n",
            "                 -9.4954e-02,  1.1853e-01,  2.5127e+00,  2.3700e-01,  6.3030e-02,\n",
            "                  7.4460e-02, -5.0439e-01, -3.3080e+00,  1.6508e-01, -2.3315e-01,\n",
            "                 -6.3921e-03, -5.2783e-01,  7.3891e-02,  5.2336e-01, -8.9559e-01,\n",
            "                  3.4226e-02,  2.5009e-05,  9.9229e-01,  4.3431e-01]], device='cuda:0',\n",
            "               grad_fn=<CatBackward0>)\n",
            "    )\n",
            ")\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 1 dims. Maybe you wanted to use the vjp or jacrev APIs instead?",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m params \u001b[38;5;241m=\u001b[39m eigenmodel_transformer\u001b[38;5;241m.\u001b[39m_parameters\n\u001b[1;32m     67\u001b[0m w0 \u001b[38;5;241m=\u001b[39m eigenmodel_transformer\u001b[38;5;241m.\u001b[39mw0\n\u001b[0;32m---> 68\u001b[0m HVPs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhvp_inhouse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_transformer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(peak_memory)\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
            "Cell \u001b[0;32mIn[31], line 44\u001b[0m, in \u001b[0;36mhvp_inhouse\u001b[0;34m(params, X, w0)\u001b[0m\n\u001b[1;32m     25\u001b[0m u \u001b[38;5;241m=\u001b[39m {name:v[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#loss = eigenmodel_transformer.compute_loss(X, w0).sum()  # Sum over batch\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute gradient of loss w.r.t. parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Compute Hessian-vector product\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m hvp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyloss2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Flatten hvp and u vectors\u001b[39;00m\n\u001b[1;32m     47\u001b[0m hvp_flat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hvp\u001b[38;5;241m.\u001b[39mvalues()])\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/apis.py:399\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1449\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[0;32m-> 1449\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1451\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1422\u001b[0m, in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto return a Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1420\u001b[0m     )\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto return a scalar Tensor, got tensor with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Maybe you wanted to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse the vjp or jacrev APIs instead?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1427\u001b[0m     )\n\u001b[1;32m   1429\u001b[0m flat_diff_args, spec \u001b[38;5;241m=\u001b[39m tree_flatten(diff_args)\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 1 dims. Maybe you wanted to use the vjp or jacrev APIs instead?"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#from torch.autograd\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def myloss(X, w0):\n",
        "    return eigenmodel_transformer.compute_loss(X, w0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def myloss2(X, w0, u):\n",
        "\n",
        "    grads = torch.func.jacrev(myloss, argnums=1)(X, w0)##torch.autograd.grad(loss, w0.values(), create_graph=True)\n",
        "    grad_u = torch.cat([einops.einsum(g,u, 'batch p, p->batch') for g, u in zip(grads.values(), u.values())])\n",
        "    # Flatten gradients and u vectors\n",
        "#    grads_flat = torch.cat([g.view(-1) for g in grads.values()])\n",
        "#    u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "    \n",
        "    # Compute dot product\n",
        "#    grad_u = torch.dot(grads_flat, u_flat)\n",
        "    return grad_u\n",
        "\n",
        "def hvp_inhouse(params, X, w0):\n",
        "    u = {name:v[0] for name, v in params.items()}\n",
        "\n",
        "    #loss = eigenmodel_transformer.compute_loss(X, w0).sum()  # Sum over batch\n",
        "\n",
        "    # Compute gradient of loss w.r.t. parameters\n",
        "    #grads = torch.func.grad(myloss, argnums=1)(X, w0)##torch.autograd.grad(loss, w0.values(), create_graph=True)\n",
        "    \n",
        "    #print(grads)\n",
        "    # Flatten gradients and u vectors\n",
        "    #grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "    #u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "    \n",
        "    #print(grads_flat.shape)\n",
        "    #print(u_flat.shape)\n",
        "    # Compute dot product\n",
        "    #grad_u = torch.dot(grads_flat, u_flat)\n",
        "    \n",
        "\n",
        "    # Compute Hessian-vector product\n",
        "    hvp = torch.func.grad(myloss2, argnums=1, is_grads_batched=True)(X, w0, u)\n",
        "\n",
        "    # Flatten hvp and u vectors\n",
        "    hvp_flat = torch.cat([h.view(-1) for h in hvp.values()])\n",
        "    u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "\n",
        "    # Compute dot product\n",
        "    dH_du = torch.dot(hvp_flat, u_flat)\n",
        "    return dH_du\n",
        "\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "# Run your code\n",
        "peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "\n",
        "\n",
        "#test = hvp_inhouse(X_transformer[:3,], eigenmodel_transformer._parameters)\n",
        "\n",
        "\n",
        "\n",
        "# Compute HVPs for all vectors in U\n",
        "peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "params = eigenmodel_transformer._parameters\n",
        "w0 = eigenmodel_transformer.w0\n",
        "HVPs = torch.vmap(hvp_inhouse, in_dims=(None, 0, None))(params, X_transformer[:3,:], w0)\n",
        "print(peak_memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'orch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43morch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhvp_inhouse\u001b[39m(X, params, w0):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'orch' is not defined"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "def hvp_inhouse(X, params, w0):\n",
        "    u = {name:v[0] for name, v in params.items()}\n",
        "\n",
        "    loss = eigenmodel_transformer.compute_loss(X, w0).sum()  # Sum over batch\n",
        "\n",
        "    # Compute gradient of loss w.r.t. parameters\n",
        "    grads = torch.autograd.grad(loss, w0.values(), create_graph=True)\n",
        "    \n",
        "    print(grads)\n",
        "    # Flatten gradients and u vectors\n",
        "    grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "    u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "    \n",
        "    print(grads_flat.shape)\n",
        "    print(u_flat.shape)\n",
        "    # Compute dot product\n",
        "    grad_u = torch.dot(grads_flat, u_flat)\n",
        "    \n",
        "\n",
        "    # Compute Hessian-vector product\n",
        "    hvp = torch.autograd.grad(grad_u, w0.values(), retain_graph=True)\n",
        "\n",
        "    # Flatten hvp and u vectors\n",
        "    hvp_flat = torch.cat([h.view(-1) for h in hvp])\n",
        "    u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "\n",
        "    # Compute dot product\n",
        "    dH_du = torch.dot(hvp_flat, u_flat)\n",
        "    return dH_du\n",
        "\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "# Run your code\n",
        "\n",
        "\n",
        "#test = hvp_inhouse(X_transformer[:3,], eigenmodel_transformer._parameters)\n",
        "peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
        "print(peak_memory)\n",
        "\n",
        "from torch.func import vmap\n",
        "\n",
        "# Compute HVPs for all vectors in U\n",
        "params = eigenmodel_transformer._parameters\n",
        "w0 = eigenmodel_transformer.w0\n",
        "HVPs = vmap(hvp_inhouse, in_dims=(0, None, None))(X_transformer[:3,:], params, w0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 938.7214, 1162.7638,  806.2974], device='cuda:0',\n",
              "       grad_fn=<MvBackward0>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.func import functional_call, vmap, grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eigenmodel_transformer.compute_loss(X_transformer[:3,:], w0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def attempt(self, x, w0, u):\n",
        "    # Compute loss\n",
        "    loss = eigenestimation_algorithm.compute_loss(x, w0).sum()  # Sum over batch\n",
        "\n",
        "    # Compute gradient of loss w.r.t. parameters\n",
        "    grads = torch.autograd.grad(loss, w0.values(), create_graph=True)\n",
        "\n",
        "    # Flatten gradients and u vectors\n",
        "    grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "    u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "\n",
        "    # Compute dot product\n",
        "    grad_u = torch.dot(grads_flat, u_flat)\n",
        "    return grad_u\n",
        "\n",
        "\n",
        "    # Compute grad along u\n",
        "    grad_u = self.grad_along_u(x, self.w0, u)\n",
        "\n",
        "    # Compute Hessian-vector product\n",
        "    hvp = torch.autograd.grad(grad_u, self.w0.values(), retain_graph=True)\n",
        "\n",
        "    # Flatten hvp and u vectors\n",
        "    hvp_flat = torch.cat([h.view(-1) for h in hvp])\n",
        "    u_flat = torch.cat([u_i.view(-1) for u_i in u.values()])\n",
        "\n",
        "    # Compute dot product\n",
        "    dH_du = torch.dot(hvp_flat, u_flat)\n",
        "    return dH_du\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call(X, params):\n",
        "    params_dict = {k:v for k,v in zip(params.keys(), param_tuple)}\n",
        "    return functional_call(transformer_model, X[:2,:2], params_dict)\n",
        "##eigenmodel_transformer(X_transformer[:2,:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.func import jacrev, functional_call\n",
        "\n",
        "param_tuple = tuple([v[0] for v in eigenmodel_transformer._parameters.values()])\n",
        "v_tuple = tuple([torch.rand_like(v[0]) for v in eigenmodel_transformer._parameters.values()])\n",
        "\n",
        "X = X_transformer[:2,:2]\n",
        "def Call(*params):\n",
        "    params_ordered_dict = eigenmodel_transformer._parameters\n",
        "    param_tuple = params\n",
        "    params_dict = {k:v for k,v in zip(params_ordered_dict.keys(), param_tuple)}\n",
        "    out = functional_call(transformer_model, params_dict,  X)\n",
        "    return out\n",
        "\n",
        "torch.autograd.functional.hvp(Call, tuple(X.float()) + param_tuple, v=tuple(torch.zeros(X.shape))+v_tuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eigenmodel_transformer._parameters.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuple(X.float()) + param_tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuple(torch.zeros(X.shape))+v_tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for i in range(n_u_vectors):\n",
        "        print(f'-----{i}-----')\n",
        "        PrintActivatingExamplesTransformer(eigenmodel_transformer, X_transformer[::100,:4], 1,top_k=5, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eigenmodel_transformer.double_grad_along_u(X_transformer[:2,], u=eigenmodel_transformer._parameters).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(list(eigenmodel_transformer._parameters.values())[0]**2).sum(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    PrintFeatureValsTransformer(eigenmodel_transformer, X_transformer[::100,:10], 4, 1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GGfOzqG9Pa9C",
        "LxERVD-g8Y9C",
        "DGTF2lp-813s",
        "sHPJ8o_G7mZt",
        "CAtCqkSQbSaA",
        "0IudwqWOdYQR"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0182948587f94b9c8ee166a4857cbc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c788b73048d84d2091ba6fdb4beb032d",
            "placeholder": "​",
            "style": "IPY_MODEL_1f558314e5644b149fd81eaf5f56794d",
            "value": "Map (num_proc=10): 100%"
          }
        },
        "03c318e6b07b47d9afb7aa239aa518ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05d2418c87b44fae8bf00c426ae9eb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "091594635b134ce99c71cf1db16526c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b0250fe6ad14548bba09ed2cf78e9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1efc0432d24e75a9c2625cf0c87016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106412f5c9d8483b94fd992519a05070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0182948587f94b9c8ee166a4857cbc64",
              "IPY_MODEL_a6868646ca214b39a29a231c41a2c5da",
              "IPY_MODEL_37db266040a54bbab4ae0c78d1402c8d"
            ],
            "layout": "IPY_MODEL_da0ebc4694cf422ea7bc65f282f55c2e"
          }
        },
        "1239413aee944685ab10d95a646bb212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131e1bfb869a4c4089ec793e50a09f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f960316d5a4d10ac9363d52bcc6e71",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aac3d0eb5b334915bc9820b793f734bf",
            "value": 25000
          }
        },
        "1a7d0d238df34772b9044ff5be118e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bea7df13e8c47bcba4294bb41161f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f558314e5644b149fd81eaf5f56794d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23f7036a798c47bc9eae5e015dbaf76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28815477856d441080df64f856fce57d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290e0afedc774ea4aff3262a77ae6232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df0dcddfe864d7786dcf9a2af957689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_587a9b50f4534847bcc26bd274f12e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_70b6ba0b163141368c2c1407a3b2a408",
            "value": " 50000/50000 [00:00&lt;00:00, 168109.00 examples/s]"
          }
        },
        "2e1c71c7c3c643c08f2c68703fcfe832": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314a36ea6e5c40c9b14b798715881a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3289b9e724ac4c938678d55e86f7330a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ee488c271ca485b8304df0f30c72d52",
              "IPY_MODEL_f5d6567526c64a9f9286e480ee57e28e",
              "IPY_MODEL_2df0dcddfe864d7786dcf9a2af957689"
            ],
            "layout": "IPY_MODEL_fedda8ef165842d69469450c103ea30d"
          }
        },
        "3578dc7a53cf4708b9153f6e286ab24f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3638f52fad834bfdb0ad1fce6c3a5691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cee7aad85024e8fa0cbb7ecfaaa1cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_bc66a456264f4cfa9eb85c184e3ec27c",
            "value": " 20.5M/20.5M [00:00&lt;00:00, 327MB/s]"
          }
        },
        "37db266040a54bbab4ae0c78d1402c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28815477856d441080df64f856fce57d",
            "placeholder": "​",
            "style": "IPY_MODEL_b8f4a8958030410193aaeb95aab076a9",
            "value": " 250/250 [00:10&lt;00:00, 25.27 examples/s]"
          }
        },
        "3a8290ee014742febefa7e23d048a18a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8a8bad11664c9aaec3a3f17f013477": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1854751fa141a4891196d07f135221": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c7a40a4ce78442895b726019f672ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc0df343506435b8c108cd69d51d89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e1b1a7da8024732af0c9cdbe924b9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4136ceea9ba8471a8010de0e565a709c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420bde1bd00f498593fd8a93dc58902f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ce75556a0c4ccf9feb97e667e5b119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48ce4642b55c48e6bd23a0aba373d364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3acd3526fa4a2993e2274bdf4eae35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "541443183a6d40a38eca72898fb917d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "581fca5501054424b787bb3936fd0c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808a2d87c63249cea15492a02b34e6e6",
            "placeholder": "​",
            "style": "IPY_MODEL_1bea7df13e8c47bcba4294bb41161f4d",
            "value": "Generating train split: 100%"
          }
        },
        "587a9b50f4534847bcc26bd274f12e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c93c4db6e704cf0acb09229cf29472d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ef5b6f865b4583a46f11c6b64baf51",
            "max": 20470363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d2418c87b44fae8bf00c426ae9eb78",
            "value": 20470363
          }
        },
        "6658013b6e424c9882651c71046a97d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70a9c225586b422c89776cb4404cea3b",
            "placeholder": "​",
            "style": "IPY_MODEL_7153f5d451a344689aca4a23fd267a13",
            "value": " 21.0M/21.0M [00:00&lt;00:00, 231MB/s]"
          }
        },
        "680c99e0ead8422eade91e99cdf13ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684c3a767e3b403898a5899cf183e1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d088c6fba45c4fe2be0c3b7b31510f76",
              "IPY_MODEL_754c55b478054e6998e2790ec607f0d7",
              "IPY_MODEL_a522b6e0641e4591842153e5c42ccabd"
            ],
            "layout": "IPY_MODEL_2e1c71c7c3c643c08f2c68703fcfe832"
          }
        },
        "6abafbd7c3f64450b5af4009d52211bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_581fca5501054424b787bb3936fd0c51",
              "IPY_MODEL_131e1bfb869a4c4089ec793e50a09f01",
              "IPY_MODEL_e0055756742448959961f1653a99a81a"
            ],
            "layout": "IPY_MODEL_420bde1bd00f498593fd8a93dc58902f"
          }
        },
        "70a9c225586b422c89776cb4404cea3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b6ba0b163141368c2c1407a3b2a408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "714276a2de3644a682940268ae9ebfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7153f5d451a344689aca4a23fd267a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73e901aa3ef441da81138d2e02a0fcc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8a8bad11664c9aaec3a3f17f013477",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_714276a2de3644a682940268ae9ebfc2",
            "value": 25000
          }
        },
        "754c55b478054e6998e2790ec607f0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1239413aee944685ab10d95a646bb212",
            "max": 41996509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45ce75556a0c4ccf9feb97e667e5b119",
            "value": 41996509
          }
        },
        "78fa322680d14395a54b375cbbbb3418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe52df55cc4b4072ab0a0c1617c71175",
            "max": 7809,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_541443183a6d40a38eca72898fb917d3",
            "value": 7809
          }
        },
        "808a2d87c63249cea15492a02b34e6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86594350ac2e46eea084f223dfda1aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "877754d374964795bde8a66af5cd17b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1753904d2a14ca1adcd17b6f007f193",
            "placeholder": "​",
            "style": "IPY_MODEL_091594635b134ce99c71cf1db16526c6",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "8d71a3168e404f758443cc646dc9b55d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee488c271ca485b8304df0f30c72d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0250fe6ad14548bba09ed2cf78e9a7",
            "placeholder": "​",
            "style": "IPY_MODEL_03c318e6b07b47d9afb7aa239aa518ef",
            "value": "Generating unsupervised split: 100%"
          }
        },
        "931d0757865c42a3b6ee2cd4c56ef562": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cee7aad85024e8fa0cbb7ecfaaa1cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a522b6e0641e4591842153e5c42ccabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a7d0d238df34772b9044ff5be118e8c",
            "placeholder": "​",
            "style": "IPY_MODEL_e1520123c50f4c559bb32fa225d00e8f",
            "value": " 42.0M/42.0M [00:00&lt;00:00, 342MB/s]"
          }
        },
        "a6868646ca214b39a29a231c41a2c5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8290ee014742febefa7e23d048a18a",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb5375e0c5694c5f95483108b83a05cc",
            "value": 250
          }
        },
        "a83affc5e5a946788a7d1f1f4ab252cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d95f1d285e58481882e8b511a33580a5",
            "placeholder": "​",
            "style": "IPY_MODEL_314a36ea6e5c40c9b14b798715881a22",
            "value": "README.md: 100%"
          }
        },
        "a8ad203d8bf043ce95bbeff43691cf44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_290e0afedc774ea4aff3262a77ae6232",
            "placeholder": "​",
            "style": "IPY_MODEL_86594350ac2e46eea084f223dfda1aae",
            "value": "Generating test split: 100%"
          }
        },
        "aac3d0eb5b334915bc9820b793f734bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae7325313ac14a8ca14d5fd2fee89e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877754d374964795bde8a66af5cd17b8",
              "IPY_MODEL_5c93c4db6e704cf0acb09229cf29472d",
              "IPY_MODEL_3638f52fad834bfdb0ad1fce6c3a5691"
            ],
            "layout": "IPY_MODEL_0f1efc0432d24e75a9c2625cf0c87016"
          }
        },
        "ae9a57dbdd444595b3142ce89822c71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ce4642b55c48e6bd23a0aba373d364",
            "placeholder": "​",
            "style": "IPY_MODEL_23f7036a798c47bc9eae5e015dbaf76e",
            "value": " 25000/25000 [00:00&lt;00:00, 142066.48 examples/s]"
          }
        },
        "b8f4a8958030410193aaeb95aab076a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc66a456264f4cfa9eb85c184e3ec27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c07cc541262744ea89f027fe5d859210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a83affc5e5a946788a7d1f1f4ab252cc",
              "IPY_MODEL_78fa322680d14395a54b375cbbbb3418",
              "IPY_MODEL_e67f44172d2247729433f3889847df5b"
            ],
            "layout": "IPY_MODEL_d3323d7c1d694236a948699005f9341c"
          }
        },
        "c0e9174592b0410a8e6a9fb2c0e7278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1753904d2a14ca1adcd17b6f007f193": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46cde1438d54fa89bdbe7c093089125": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8ad203d8bf043ce95bbeff43691cf44",
              "IPY_MODEL_73e901aa3ef441da81138d2e02a0fcc0",
              "IPY_MODEL_ae9a57dbdd444595b3142ce89822c71f"
            ],
            "layout": "IPY_MODEL_8d71a3168e404f758443cc646dc9b55d"
          }
        },
        "c788b73048d84d2091ba6fdb4beb032d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7e73e9cf5347928557a754f1a46754": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb13fb61baea4fd1923f990acd31f200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680c99e0ead8422eade91e99cdf13ef0",
            "placeholder": "​",
            "style": "IPY_MODEL_3e1b1a7da8024732af0c9cdbe924b9ce",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "d088c6fba45c4fe2be0c3b7b31510f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7a40a4ce78442895b726019f672ca6",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e9174592b0410a8e6a9fb2c0e7278f",
            "value": "unsupervised-00000-of-00001.parquet: 100%"
          }
        },
        "d0ef5b6f865b4583a46f11c6b64baf51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3323d7c1d694236a948699005f9341c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f960316d5a4d10ac9363d52bcc6e71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95f1d285e58481882e8b511a33580a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0ebc4694cf422ea7bc65f282f55c2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de80c2d0e39c4e299106c8cc45ac5d37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0055756742448959961f1653a99a81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3578dc7a53cf4708b9153f6e286ab24f",
            "placeholder": "​",
            "style": "IPY_MODEL_931d0757865c42a3b6ee2cd4c56ef562",
            "value": " 25000/25000 [00:00&lt;00:00, 98860.67 examples/s]"
          }
        },
        "e1520123c50f4c559bb32fa225d00e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e67f44172d2247729433f3889847df5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4136ceea9ba8471a8010de0e565a709c",
            "placeholder": "​",
            "style": "IPY_MODEL_4b3acd3526fa4a2993e2274bdf4eae35",
            "value": " 7.81k/7.81k [00:00&lt;00:00, 636kB/s]"
          }
        },
        "e8b144c9c94c4ab5a535341579c07f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de80c2d0e39c4e299106c8cc45ac5d37",
            "max": 20979968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b1854751fa141a4891196d07f135221",
            "value": 20979968
          }
        },
        "efcde3edec9f4d0cb1288f5a1ea7f088": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d6567526c64a9f9286e480ee57e28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efcde3edec9f4d0cb1288f5a1ea7f088",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cc0df343506435b8c108cd69d51d89f",
            "value": 50000
          }
        },
        "f8cc96106385416d809cdec354f8a6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb13fb61baea4fd1923f990acd31f200",
              "IPY_MODEL_e8b144c9c94c4ab5a535341579c07f1f",
              "IPY_MODEL_6658013b6e424c9882651c71046a97d0"
            ],
            "layout": "IPY_MODEL_ca7e73e9cf5347928557a754f1a46754"
          }
        },
        "fb5375e0c5694c5f95483108b83a05cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe52df55cc4b4072ab0a0c1617c71175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fedda8ef165842d69469450c103ea30d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
