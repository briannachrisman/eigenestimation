{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import toy_models, eigenestimation_algorithm\n",
        "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "import einops\n",
        "from eigenestimation_algorithm.evaluation import PrintFeatureVals, ActivatingExamples, PrintFeatureValsTransformer, PrintActivatingExamplesTransformer\n",
        "\n",
        "importlib.reload(toy_models.xornet)\n",
        "importlib.reload(toy_models.tms)\n",
        "importlib.reload(toy_models.train)\n",
        "\n",
        "importlib.reload(eigenestimation_algorithm.train)\n",
        "importlib.reload(eigenestimation_algorithm.eigenestimation)\n",
        "importlib.reload(eigenestimation_algorithm.evaluation)\n",
        "\n",
        "importlib.reload(eigenestimation_algorithm)\n",
        "import gc\n",
        "# Toy models!\n",
        "from toy_models.xornet import XORNet, GenerateXORData\n",
        "from toy_models.tms import Autoencoder, GenerateTMSData\n",
        "from toy_models.transformer_wrapper import TransformerWrapper, DeleteParams\n",
        "\n",
        "from toy_models.train import TrainModel\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from eigenestimation_algorithm.eigenestimation import EigenEstimation\n",
        "from eigenestimation_algorithm.train import TrainEigenEstimation\n",
        "\n",
        "device='cuda'\n",
        "torch.set_default_device(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toy Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XORNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_xornet, Y_xornet, dataloader_xornet = GenerateXORData(n_repeats=100, batch_size=24)\n",
        "model_xornet = XORNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 300, Loss: 8.041374144340807e-07\n"
          ]
        }
      ],
      "source": [
        "_, _, _ =TrainModel(\n",
        "    model=model_xornet,\n",
        "    criterion=nn.MSELoss(),\n",
        "    learning_rate=.01,\n",
        "    dataloader=dataloader_xornet,\n",
        "    n_epochs=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "9Wj0Z4mdTQAO",
        "outputId": "6f33c69e-b0be-476c-aa99-de20e16a233c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 500, Loss: 0.10686875879764557\n",
            "Epoch 1000, Loss: 0.026212189346551895\n",
            "Epoch 1500, Loss: 0.039112016558647156\n",
            "Epoch 2000, Loss: 0.050186965614557266\n",
            "Epoch 2500, Loss: 0.012528285384178162\n",
            "Epoch 3000, Loss: 0.010989688336849213\n",
            "Epoch 3500, Loss: 0.028260987251996994\n",
            "Epoch 4000, Loss: 0.04411494731903076\n",
            "Epoch 4500, Loss: 0.03988216444849968\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Train TMS\n",
        "n_features = 5\n",
        "hidden_dim = 2\n",
        "n_datapoints = 2048\n",
        "sparsity = .1\n",
        "\n",
        "batch_size = 24\n",
        "learning_rate = .01\n",
        "n_epochs = 5000\n",
        "\n",
        "X_tms, Y_tms, dataloader_TMS = GenerateTMSData(\n",
        "    num_features=n_features, num_datapoints=n_datapoints, sparsity=sparsity, batch_size=batch_size)\n",
        "tms_model = Autoencoder(n_features, hidden_dim)\n",
        "_, _, _ = TrainModel(tms_model, nn.MSELoss(), learning_rate, dataloader_TMS, n_epochs=n_epochs)\n",
        "\n",
        "\n",
        "# Plot TMS representations.\n",
        "en = copy.deepcopy(tms_model.W).detach().cpu().numpy()\n",
        "\n",
        "for i in range(en.shape[1]):\n",
        "  plt.plot([0, en[0,i]], [0,en[1,i]], 'b-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGTF2lp-813s"
      },
      "source": [
        "## 2-layer transformer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "-JyFbkBhuOlR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n",
            "transformer.ln_f.weight\n"
          ]
        }
      ],
      "source": [
        "# @title Import pretrained gpt2 (2 layers)\n",
        "# Disable fused kernels (FlashAttention and memory-efficient attention)\n",
        "# We have to disable this to compute second-order gradients on transformer models.\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "\n",
        "# Ensure the math kernel is enabled (it is True by default)\n",
        "torch.backends.cuda.enable_math_sdp(True)\n",
        "\n",
        "# Load in a 2-L GPT2.\n",
        "config = GPT2Config.from_pretrained('gpt2', n_layer=2)\n",
        "gpt2 = GPT2Model.from_pretrained('gpt2', config=config)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "transformer_model = TransformerWrapper(gpt2, tokenizer)\n",
        "\n",
        "# Make the eigenestimation a little smaller but only looking at a subset of the parameters.\n",
        "# Pick a random subset of tensors to include in paramters, and turn the rest into frozen buffers.\n",
        "params_to_delete = [name for name, param in transformer_model.named_parameters()]\n",
        "params_to_delete = [p for p in params_to_delete if p!='transformer.ln_f.weight']\n",
        "\n",
        "# Delete 3/4 of the parameters.\n",
        "#for p in (params_to_delete[::20]):\n",
        "#  params_to_delete.remove(p)\n",
        "\n",
        "DeleteParams(transformer_model, params_to_delete)\n",
        "\n",
        "print(sum([p.numel() for p in transformer_model.parameters()]))\n",
        "for n,_ in transformer_model.named_parameters(): print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=10): 100%|██████████| 250/250 [00:12<00:00, 20.63 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load in data.\n",
        "imdb_dataset = load_dataset(\"imdb\", split=\"test[:1%]\")\n",
        "X_transformer= tokenize_and_concatenate(imdb_dataset, tokenizer, max_length = 24, add_bos_token=False)['tokens']\n",
        "transformer_dataloader = DataLoader(X_transformer, batch_size=24, shuffle=True,\n",
        "                                    generator=torch.Generator(device='cuda'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F3k4QTXsbgM"
      },
      "source": [
        "# Eigenestimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz2siQHyQWZ0"
      },
      "source": [
        "# Tests on Toy Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNRhg3iQ2yk"
      },
      "source": [
        "## Xornet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7gdVpNvP5oU",
        "outputId": "fdbd20b8-606b-4cab-e291-154210b81316"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:31: FutureWarning: `torch.nn.utils.stateless.functional_call` is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use `torch.func.functional_call` instead which is a drop-in replacement.\n",
            "  outputs: torch.Tensor = stateless.functional_call(self.model, parameters, (x,))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 - Total Loss: -1.887, High Hessian Loss: -1.959,  Basis Loss: 0.073\n",
            "Epoch 10 - Total Loss: -4.457, High Hessian Loss: -4.552,  Basis Loss: 0.095\n",
            "Epoch 20 - Total Loss: -4.524, High Hessian Loss: -4.590,  Basis Loss: 0.066\n",
            "Epoch 30 - Total Loss: -4.538, High Hessian Loss: -4.595,  Basis Loss: 0.056\n",
            "Epoch 40 - Total Loss: -4.555, High Hessian Loss: -4.618,  Basis Loss: 0.063\n",
            "Epoch 50 - Total Loss: -4.576, High Hessian Loss: -4.659,  Basis Loss: 0.082\n",
            "Epoch 60 - Total Loss: -4.597, High Hessian Loss: -4.703,  Basis Loss: 0.106\n",
            "Epoch 70 - Total Loss: -4.624, High Hessian Loss: -4.764,  Basis Loss: 0.139\n",
            "Epoch 80 - Total Loss: -4.648, High Hessian Loss: -4.826,  Basis Loss: 0.178\n",
            "Epoch 90 - Total Loss: -4.668, High Hessian Loss: -4.881,  Basis Loss: 0.213\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_u_vectors = 3\n",
        "batch_size = 4\n",
        "lambda_penalty = 1\n",
        "repeats = 4\n",
        "n_epochs = 100\n",
        "learning_rate = .01\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "eigenmodel_xornet = EigenEstimation(model_xornet, nn.MSELoss, n_u_vectors)\n",
        "\n",
        "dataloader_xornet_eigen = DataLoader(\n",
        "    einops.repeat(X_xornet, 's f -> (s r) f', r=repeats), batch_size=batch_size, shuffle=True,\n",
        "    generator=torch.Generator(device=device))\n",
        "\n",
        "TrainEigenEstimation(eigenmodel_xornet, dataloader_xornet_eigen, learning_rate, n_epochs, lambda_penalty)\n",
        "\n",
        "# Clear cuda cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUGWSKrfRJOv",
        "outputId": "79eaa80b-8a64-4671-c74c-301123572273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0.] --> [0.53 0.13 0.  ]\n",
            "[0. 1.] --> [5.09 0.69 0.  ]\n",
            "[1. 0.] --> [0.   0.28 6.94]\n",
            "[1. 1.] --> [1.73 2.39 0.43]\n",
            "feature 0\n",
            "[0. 1.] -> 5.0876226\n",
            "[1. 1.] -> 1.7256724\n",
            "[0. 0.] -> 0.52878517\n",
            "feature 1\n",
            "[1. 1.] -> 2.3889158\n",
            "[0. 1.] -> 0.68826383\n",
            "[1. 0.] -> 0.2794086\n",
            "feature 2\n",
            "[1. 0.] -> 6.9396763\n",
            "[1. 1.] -> 0.42713824\n",
            "[0. 1.] -> 0.0023718486\n"
          ]
        }
      ],
      "source": [
        "#@title Look at features\n",
        "PrintFeatureVals(X_xornet, eigenmodel_xornet)\n",
        "\n",
        "for f_idx in range(eigenmodel_xornet.n_u_vectors):\n",
        "  sample, val = ActivatingExamples(X_xornet, eigenmodel_xornet, f_idx, 3)\n",
        "  print('feature', f_idx)\n",
        "  for s, v in zip(sample, val):\n",
        "    print(s, '->', v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCx0rhdoTGJx"
      },
      "source": [
        "## TMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLj93rI3TIJ9",
        "outputId": "b45eb47a-3e70-4927-8785-a1722ff46080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 - Total Loss: -0.952, High Hessian Loss: -1.087,  Basis loss: 0.135\n",
            "epoch 100 - Total Loss: -6.515, High Hessian Loss: -7.165,  Basis loss: 0.650\n",
            "epoch 200 - Total Loss: -7.087, High Hessian Loss: -7.382,  Basis loss: 0.294\n",
            "epoch 300 - Total Loss: -7.549, High Hessian Loss: -7.697,  Basis loss: 0.148\n",
            "epoch 400 - Total Loss: -7.657, High Hessian Loss: -7.777,  Basis loss: 0.119\n",
            "epoch 500 - Total Loss: -7.706, High Hessian Loss: -7.812,  Basis loss: 0.107\n",
            "epoch 600 - Total Loss: -7.720, High Hessian Loss: -7.818,  Basis loss: 0.099\n",
            "epoch 700 - Total Loss: -7.731, High Hessian Loss: -7.826,  Basis loss: 0.095\n",
            "epoch 800 - Total Loss: -7.718, High Hessian Loss: -7.811,  Basis loss: 0.093\n",
            "epoch 900 - Total Loss: -7.748, High Hessian Loss: -7.841,  Basis loss: 0.093\n"
          ]
        }
      ],
      "source": [
        "#@title Train Eigenmodel\n",
        "n_u_vectors = 5\n",
        "batch_size = 24\n",
        "lambda_penalty = 1\n",
        "n_epochs = 1000\n",
        "learning_rate = .01\n",
        "\n",
        "\n",
        "dataloader = DataLoader(X_tms, batch_size=batch_size, shuffle=True,\n",
        "                               generator=torch.Generator(device='cuda'))\n",
        "eigenmodel_tms = EigenEstimation(tms_model, nn.MSELoss, n_u_vectors)\n",
        "TrainEigenEstimation(eigenmodel_tms, dataloader, learning_rate, n_epochs, lambda_penalty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLJPlPcZTIKK",
        "outputId": "f75e7cf5-9dd5-4829-f6cd-cfecaf2daa08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.   0.   0.   0.17 0.  ] --> [1.47 0.   0.   0.   0.  ]\n",
            "[0.84 0.   0.   0.   0.  ] --> [1.44 0.   0.   9.07 1.82]\n",
            "[0.   0.   0.   0.   0.64] --> [0.   1.58 5.98 0.   1.07]\n",
            "[0.   0.   0.98 0.   0.  ] --> [0.000e+00 1.000e-02 1.970e+00 2.240e+00 1.112e+01]\n",
            "[0.   0.24 0.   0.   0.  ] --> [0.01 1.99 0.   0.   0.  ]\n",
            "[0.   0.56 0.   0.   0.  ] --> [1.14 5.31 0.85 0.   0.  ]\n",
            "[0.   0.67 0.   0.   0.  ] --> [1.28 6.86 0.94 0.   0.  ]\n",
            "[0.   0.   0.   0.49 0.  ] --> [4.19 0.   0.   0.   0.  ]\n",
            "[0.   0.37 0.   0.72 0.  ] --> [8.44 4.09 0.   0.01 0.  ]\n",
            "[0.93 0.   0.   0.   0.  ] --> [ 1.56  0.    0.   10.66  2.01]\n",
            "feature 0\n",
            "[0.    0.    0.    0.991 0.   ] -> 12.03\n",
            "[0.    0.    0.    0.987 0.   ] -> 11.963\n",
            "[0.    0.    0.    0.983 0.   ] -> 11.887\n",
            "feature 1\n",
            "[0.    0.946 0.    0.    0.   ] -> 11.289\n",
            "[0.   0.93 0.   0.   0.  ] -> 11.011\n",
            "[0.    0.777 0.    0.    0.522] -> 10.621\n",
            "feature 2\n",
            "[0.    0.    0.906 0.    0.935] -> 15.038\n",
            "[0.    0.    0.    0.    0.997] -> 11.808\n",
            "[0.   0.   0.   0.   0.99] -> 11.669\n",
            "feature 3\n",
            "[0.982 0.    0.92  0.    0.   ] -> 16.618\n",
            "[0.842 0.    0.768 0.    0.   ] -> 12.824\n",
            "[0.992 0.    0.    0.    0.   ] -> 11.717\n",
            "feature 4\n",
            "[0.982 0.    0.92  0.    0.   ] -> 14.54\n",
            "[0.    0.    0.906 0.    0.935] -> 11.897\n",
            "[0.    0.    0.988 0.    0.   ] -> 11.193\n"
          ]
        }
      ],
      "source": [
        "#@title Look at features\n",
        "PrintFeatureVals(X_tms[:10], eigenmodel_tms)\n",
        "\n",
        "for f_idx in range(eigenmodel_tms.n_u_vectors):\n",
        "  sample, val = ActivatingExamples(X_tms, eigenmodel_tms, f_idx, 3)\n",
        "  print('feature', f_idx)\n",
        "  for s, v in zip(sample, val):\n",
        "    print(s.round(3), '->', v.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBq1uAotxJB_"
      },
      "source": [
        "## 2L Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KLDivergenceLoss(nn.Module):\n",
        "    def __init__(self, reduction: str = 'mean') -> None:\n",
        "        \"\"\"\n",
        "        KL Divergence Loss with a structure similar to MSELoss.\n",
        "        \n",
        "        Args:\n",
        "            reduction (str): Specifies the reduction to apply to the output:\n",
        "                             'none' | 'mean' | 'sum'. Default is 'mean'.\n",
        "        \"\"\"\n",
        "        super(KLDivergenceLoss, self).__init__()\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, preds: torch.Tensor, truth: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for KL Divergence Loss.\n",
        "        \n",
        "        Args:\n",
        "            preds (torch.Tensor): Predicted logits or probabilities (not softmaxed).\n",
        "            truth (torch.Tensor): Target probabilities.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The computed KL Divergence Loss.\n",
        "        \"\"\"\n",
        "        # Convert preds to log-probabilities\n",
        "        preds_log = torch.log_softmax(preds, dim=1)\n",
        "\n",
        "        # Compute KL divergence per sample (without reduction)\n",
        "        kl_divergence = F.kl_div(preds_log, truth, reduction='none')\n",
        "        per_sample_kl_div = kl_divergence.mean(dim=1)  # Sum over classes for each sample\n",
        "\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return per_sample_kl_div.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return per_sample_kl_div.sum()\n",
        "        else:  # 'none'\n",
        "            return per_sample_kl_div\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6973GptxH2T",
        "outputId": "bcae6bb6-fe6d-4226-8b7f-c93bb10b472f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 - Total Loss: -0.000, High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 1 - Total Loss: -0.000, High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 2 - Total Loss: -0.000, High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 3 - Total Loss: -0.000, High Hessian Loss: -0.000,  Basis Loss: 0.000\n",
            "Epoch 4 - Total Loss: -0.000, High Hessian Loss: -0.000,  Basis Loss: 0.000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[161], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m transformer_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(X_transformer[:\u001b[38;5;241m1000\u001b[39m, :\u001b[38;5;241m10\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m                                     generator\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mGenerator(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     14\u001b[0m eigenmodel_transformer \u001b[38;5;241m=\u001b[39m EigenEstimation(transformer_model, KLDivergenceLoss, n_u_vectors)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mTrainEigenEstimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenmodel_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_penalty\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/train.py:43\u001b[0m, in \u001b[0;36mTrainEigenEstimation\u001b[0;34m(eigenmodel, dataloader, lr, n_epochs, lambda_penalty)\u001b[0m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m dH_du, u_tensor \u001b[38;5;241m=\u001b[39m \u001b[43meigenmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute the loss components\u001b[39;00m\n\u001b[1;32m     46\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m dH_du\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:108\u001b[0m, in \u001b[0;36mEigenEstimation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m u_tensor: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_to_vectors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Compute the double gradient along u\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m dH_du: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_grad_along_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dH_du, u_tensor\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:61\u001b[0m, in \u001b[0;36mEigenEstimation.double_grad_along_u\u001b[0;34m(self, x, u)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdouble_grad_along_u\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, u: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]\n\u001b[1;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Compute the second derivative (Hessian) along u\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     jac: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_along_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Compute the dot product of the Jacobian and u vectors\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m     66\u001b[0m         [\n\u001b[1;32m     67\u001b[0m             einops\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         ]\n\u001b[1;32m     74\u001b[0m     )\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:604\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    603\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    606\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:399\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:44\u001b[0m, in \u001b[0;36mEigenEstimation.grad_along_u\u001b[0;34m(self, x, w0, u)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_along_u\u001b[39m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     39\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Compute the Jacobian of the loss with respect to parameters\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     grad_f: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Compute the sum over the einsum operations for each parameter\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m     47\u001b[0m         [\n\u001b[1;32m     48\u001b[0m             einops\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m         ]\n\u001b[1;32m     55\u001b[0m     )\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:724\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    722\u001b[0m     flat_jacobians_per_input \u001b[38;5;241m=\u001b[39m compute_jacobian_preallocate_and_copy()\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m     flat_jacobians_per_input \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_jacobian_stacked\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# Step 2: The returned jacobian is one big tensor per input. In this step,\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# we split each Tensor by output.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m flat_jacobians_per_input \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    729\u001b[0m     result\u001b[38;5;241m.\u001b[39msplit(flat_output_numels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m flat_jacobians_per_input\n\u001b[1;32m    731\u001b[0m ]\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:627\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn.<locals>.compute_jacobian_stacked\u001b[0;34m()\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_jacobian_stacked\u001b[39m():\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# Helper function to compute chunked Jacobian\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;66;03m# The intermediate chunked calculation are only\u001b[39;00m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;66;03m# scoped at this function level.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m flat_basis_chunk \u001b[38;5;129;01min\u001b[39;00m _chunked_standard_basis_for_(\n\u001b[1;32m    628\u001b[0m         flat_output, flat_output_numels, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size\n\u001b[1;32m    629\u001b[0m     ):\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;66;03m# sanity check.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_basis_chunk:\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:847\u001b[0m, in \u001b[0;36m_chunked_standard_basis_for_\u001b[0;34m(tensors, tensor_numels, chunk_size)\u001b[0m\n\u001b[1;32m    842\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m total_numel\n\u001b[1;32m    843\u001b[0m     chunk_numels \u001b[38;5;241m=\u001b[39m [total_numel]\n\u001b[1;32m    845\u001b[0m diag_start_indices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m--> 847\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_numels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mneg()\u001b[38;5;241m.\u001b[39munbind(),\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_idx, total_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk_numels):\n\u001b[1;32m    851\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    852\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mnew_zeros(total_numel, tensor_numel)\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor, tensor_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, tensor_numels)\n\u001b[1;32m    854\u001b[0m     )\n",
            "File \u001b[0;32m~/brianna-interpretability/eigenestimation_venv/lib/python3.10/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Train Eigenmodel\n",
        "#model = transformer_model\n",
        "n_u_vectors = 10\n",
        "batch_size = 4\n",
        "lambda_penalty = 1\n",
        "n_epochs = 10\n",
        "learning_rate = .001\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "transformer_dataloader = DataLoader(X_transformer[:1000, :10], batch_size=batch_size, shuffle=True,\n",
        "                                    generator=torch.Generator(device='cuda'))\n",
        "eigenmodel_transformer = EigenEstimation(transformer_model, KLDivergenceLoss, n_u_vectors)\n",
        "TrainEigenEstimation(eigenmodel_transformer, transformer_dataloader, learning_rate, n_epochs, lambda_penalty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PrintFeatureValsTransformer(eigenmodel_transformer, X_transformer[:100,:2], 2, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/brianna-interpretability/eigenestimation/eigenestimation_algorithm/eigenestimation.py:31: FutureWarning: `torch.nn.utils.stateless.functional_call` is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use `torch.func.functional_call` instead which is a drop-in replacement.\n",
            "  outputs: torch.Tensor = stateless.functional_call(self.model, parameters, (x,))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-1.1149e-03, -2.4052e-05, -1.7928e-04],\n",
              "        [-1.0913e-03, -9.6563e-15, -7.5721e-15]], device='cuda:0',\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()\n",
        "X = X_transformer[:2,:3]\n",
        "eigenmodel= eigenmodel_transformer\n",
        "feature_idx=1\n",
        "dH, _ = eigenmodel(X)\n",
        "feature_vals = einops.rearrange(dH, '(s t) f -> s t f', s=X.shape[0], t=X.shape[1])[:,:,feature_idx]\n",
        "feature_vals.argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "', st'"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA43OkOQKG0t"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYhFxzhL5Me9"
      },
      "source": [
        "# --------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDlI2yiI5M-1"
      },
      "source": [
        "# --------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muYnvur46mG9"
      },
      "source": [
        "# Singular Basis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTPpkqUwLKm1"
      },
      "source": [
        "# Eigenestimation_old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RO2AN2RrwdF3",
        "outputId": "0e37b425-5955-4c0b-e2b8-cdd9c61ccf98"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8f61b73211bc>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mbatch_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mvector_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mhvp_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_hvp_dot_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvectors_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;31m# Shape: [batch_size, n_vectors]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# HVP\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.func import functional_call\n",
        "import einops\n",
        "\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, model, loss, n_u_vectors):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "\n",
        "        self.loss = loss\n",
        "        self.model = model\n",
        "        self.w0 = dict(model.named_parameters())\n",
        "        self.n_u_vectors = n_u_vectors\n",
        "\n",
        "        u_dict = {\n",
        "            name: torch.nn.Parameter(torch.stack([torch.randn_like(param) for _ in range(n_u_vectors)]))\n",
        "            for name, param in self.w0.items()\n",
        "        }\n",
        "\n",
        "        for name, tensor in u_dict.items():\n",
        "            # Register each tensor as a parameter\n",
        "            self.register_parameter(name.replace('.', '__'), tensor)\n",
        "\n",
        "    def compute_loss(self, x, parameters):\n",
        "        outputs = functional_call(self.model, parameters, (x,))\n",
        "        truth = outputs.detach()\n",
        "        return self.loss(reduction='none')(outputs, truth)\n",
        "\n",
        "    def grad_along_u(self, x, w0, u):\n",
        "        # Collect parameter names\n",
        "        param_names = list(w0.keys())\n",
        "        # Collect parameters into a tuple\n",
        "        params_tuple = tuple(w0[name] for name in param_names)\n",
        "        # Collect u vectors into a tuple\n",
        "        u_tuple = tuple(u[name.replace('.', '__')] for name in param_names)\n",
        "\n",
        "        # Define the loss function\n",
        "        def loss_fn(*params_tuple):\n",
        "            parameters = dict(zip(param_names, params_tuple))\n",
        "            loss = self.compute_loss(x, parameters).sum()  # Sum over batch\n",
        "            return loss\n",
        "\n",
        "        # Compute the jvp\n",
        "        _, jvp_result = torch.autograd.functional.jvp(\n",
        "            loss_fn,\n",
        "            params_tuple,\n",
        "            u_tuple,\n",
        "            create_graph=True\n",
        "        )\n",
        "\n",
        "        grad_along_u = jvp_result  # Scalar value\n",
        "        return grad_along_u\n",
        "\n",
        "    def double_grad_along_u(self, x, u):\n",
        "        # Collect parameter names\n",
        "        param_names = list(self.w0.keys())\n",
        "        # Collect parameters into a tuple\n",
        "        params_tuple = tuple(self.w0[name] for name in param_names)\n",
        "        # Collect u vectors into a tuple\n",
        "        dots = []\n",
        "        for vector_idx in range(self.n_u_vectors):\n",
        "          u_tuple = tuple(u[name.replace('.', '__')][vector_idx] for name in param_names)\n",
        "\n",
        "          # Define the gradient function\n",
        "          def grad_fn(*params_tuple):\n",
        "              parameters = dict(zip(param_names, params_tuple))\n",
        "              loss = self.compute_loss(x, parameters)  # Sum over batch\n",
        "              grads = torch.autograd.grad(loss, params_tuple, create_graph=True)\n",
        "              return grads  # Returns a tuple of gradients\n",
        "\n",
        "          # Compute the Hessian-vector product\n",
        "          _, hvp = torch.autograd.functional.jvp(\n",
        "              grad_fn,\n",
        "              params_tuple,\n",
        "              u_tuple,\n",
        "              create_graph=False\n",
        "          )\n",
        "\n",
        "\n",
        "          # Compute the dot product between hvp and u\n",
        "          print(hvp, x)\n",
        "          dot = sum((hvp_i * u_i).sum() for hvp_i, u_i in zip(hvp, u_tuple))\n",
        "          dots.append(dot)\n",
        "        return torch.stack(dots)  # Scalar value\n",
        "\n",
        "    def normalize_parameters(self):\n",
        "        u_tensor = self.params_to_vectors(self._parameters)  # Shape: [n_u_vectors, total_param_size]\n",
        "        norms = u_tensor.norm(dim=1, keepdim=True)  # Shape: [n_u_vectors, 1]\n",
        "        normalized_params = {}\n",
        "        for name, param in self._parameters.items():\n",
        "            # Reshape norms to match the dimensions of param\n",
        "            param_shape = param.shape[1:]  # Exclude n_u_vectors dimension\n",
        "            norms_reshaped = norms.view(-1, *([1] * len(param_shape)))  # Shape: [n_u_vectors, 1, 1, ...]\n",
        "            # Normalize param\n",
        "            normalized_params[name] = param / norms_reshaped\n",
        "        return normalized_params\n",
        "\n",
        "    def params_to_vectors(self, params):\n",
        "        # Concatenate all parameters into a single tensor\n",
        "        return torch.cat([param.view(param.size(0), -1) for param in params.values()], dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Normalize u parameters\n",
        "        params = self.normalize_parameters()\n",
        "        # Compute u_tensor\n",
        "        u_tensor = self.params_to_vectors(params)\n",
        "\n",
        "        # Compute dH_du\n",
        "        dH_du = self.double_grad_along_u(x, params)\n",
        "\n",
        "        return dH_du, u_tensor\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd.functional import hvp\n",
        "from torch.func import vmap\n",
        "\n",
        "# Model definition (as before)\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        output = self.linear2(x)\n",
        "        return output\n",
        "\n",
        "model = MyModel(input_dim=10, hidden_dim=20, output_dim=1)\n",
        "\n",
        "# Inputs\n",
        "batch_size = 5\n",
        "X_batch = torch.randn(batch_size, 10)\n",
        "\n",
        "# Get model parameters\n",
        "param_names = [name for name, _ in model.named_parameters()]\n",
        "params = [param for _, param in model.named_parameters()]\n",
        "\n",
        "# Number of vectors\n",
        "n_vectors = 3\n",
        "\n",
        "# Prepare random vectors\n",
        "vectors = []\n",
        "for _ in range(n_vectors):\n",
        "    vector = [torch.randn_like(param) for param in params]\n",
        "    vectors.append(vector)\n",
        "\n",
        "# Flatten parameters and vectors\n",
        "def flatten_params(params):\n",
        "    return torch.cat([p.reshape(-1) for p in params])\n",
        "\n",
        "params_flat = flatten_params(params)\n",
        "vectors_flat = [flatten_params(v) for v in vectors]\n",
        "vectors_flat = torch.stack(vectors_flat)  # Shape: [n_vectors, total_params]\n",
        "\n",
        "# Prepare targets (using model outputs as targets for simplicity)\n",
        "with torch.no_grad():\n",
        "    y_samples = model(X_batch).squeeze(-1)  # Shape: [batch_size]\n",
        "\n",
        "# Function to compute hvp dot product per sample and vector\n",
        "def hvp_per_sample(params_flat, x_sample, y_sample, vector_flat):\n",
        "    # Unflatten parameters\n",
        "    param_idx = 0\n",
        "    params_unflat = []\n",
        "    for param in params:\n",
        "        numel = param.numel()\n",
        "        param_unflat = params_flat[param_idx:param_idx+numel].view_as(param)\n",
        "        params_unflat.append(param_unflat)\n",
        "        param_idx += numel\n",
        "\n",
        "    # Unflatten vector\n",
        "    param_idx = 0\n",
        "    vector_unflat = []\n",
        "    for param in params:\n",
        "        numel = param.numel()\n",
        "        vector_unflat.append(vector_flat[param_idx:param_idx+numel].view_as(param))\n",
        "        param_idx += numel\n",
        "\n",
        "    # Compute loss for the sample\n",
        "    def single_loss_fn(*params_unflat):\n",
        "        param_dict = dict(zip(param_names, params_unflat))\n",
        "        y_pred = torch.func.functional_call(model, param_dict, (x_sample.unsqueeze(0),))\n",
        "        loss = torch.nn.functional.mse_loss(y_pred.squeeze(0), y_sample, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    # Compute Hessian-vector product\n",
        "    _, hvp_tensor = hvp(single_loss_fn, tuple(params_unflat), tuple(vector_unflat))\n",
        "    hvp_flat = flatten_params(hvp_tensor)\n",
        "    hvp_dot = torch.dot(hvp_flat, vector_flat)\n",
        "    return hvp_dot\n",
        "\n",
        "# Vectorize over batch and vectors\n",
        "def compute_hvp_dot(x_sample, vector_flat):\n",
        "    hvp_dot = hvp_per_sample(params_flat, x_sample, x_sample, vector_flat)\n",
        "    return hvp_dot\n",
        "\n",
        "compute_hvp_dot_vmap = vmap(vmap(compute_hvp_dot, in_dims=(None, 0)), in_dims=(0, None))\n",
        "\n",
        "# Compute Hessian-vector products\n",
        "batch_indices = torch.arange(batch_size)\n",
        "vector_indices = torch.arange(n_vectors)\n",
        "hvp_results = compute_hvp_dot_vmap(X,  vectors_flat)\n",
        "# Shape: [batch_size, n_vectors]\n",
        "\n",
        "print(\"Hessian-vector products shape:\", hvp_results.shape)\n",
        "print(\"Hessian-vector products:\", hvp_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMm027O4z_TN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OidTeiczL4JM"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, k, model, criterion):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "\n",
        "        n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(k, n))\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # Normalize columns of U\n",
        "        self.U.data = self.U.data / torch.norm(self.U.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        U = self.U  # [n, k]\n",
        "        model = self.model\n",
        "        criterion = self.criterion\n",
        "        loss = criterion(self.model(X_batch), self.model(X_batch).detach())\n",
        "\n",
        "        H_vs = []\n",
        "        grads = torch.cat([g.flatten() for g in torch.autograd.grad(\n",
        "            loss, self.model.parameters(), create_graph=True)])\n",
        "        grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "        for i in range(U.shape[0]):\n",
        "\n",
        "          # Hessian-vector product\n",
        "          H_v = torch.autograd.grad(torch.dot(grads_flat,U[i,:]), self.model.parameters(), retain_graph=True)  #\n",
        "          Hv_flat = torch.dot(U[i,:] ,torch.cat([h.view(-1) for h in H_v]))\n",
        "          H_vs.append(Hv_flat) # 1 x k (k x n) = 1 x n\n",
        "\n",
        "        # d2L/dv2\n",
        "        H_vs_tensor = torch.stack(H_vs)\n",
        "\n",
        "        # Magnitude and direction of each U vector.\n",
        "        Hv_U = einops.repeat(H_vs_tensor, 'k->k n', n=U.shape[1]) * U\n",
        "\n",
        "        # Dot product of U-vectors, scaled by magnitudes.\n",
        "        dot_prods = Hv_U @ einops.rearrange(Hv_U, 'k n -> n k')\n",
        "\n",
        "        return H_vs_tensor, dot_prods\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(eigen_model, learning_rate, num_epochs, lambda_penalty, dataloader, criterion, hessian_samples):\n",
        "\n",
        "\n",
        "  # Number of parameters in the model\n",
        "  n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "\n",
        "  # Create a mask for the lower triangular part (including diagonal) to be used for dot products.\n",
        "  lower_triangular_mask = torch.tril(torch.ones(eigen_model.U.shape[0], eigen_model.U.shape[0], dtype=torch.bool), diagonal=-1)\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigen_model.parameters(), lr=learning_rate)\n",
        "  eigen_model.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    total_losses = 0\n",
        "    high_hessian_losses = 0\n",
        "    orthogonality_losses = 0\n",
        "    optimizer.zero_grad()\n",
        "    n_batches = 0\n",
        "    for X, Y in dataloader:\n",
        "\n",
        "      total_loss = 0\n",
        "      hessian_dataloader = DataLoader(TensorDataset(X, Y), batch_size=hessian_samples, shuffle=True,\n",
        "                                      generator=torch.Generator(device='cuda'))\n",
        "      n_subbatches = 0\n",
        "      for X_batch, _ in hessian_dataloader:\n",
        "        n_subbatches += 1\n",
        "        # Compute d2L/du2 and dot product of u vectors scaled by magnitudes.\n",
        "        H_vs, dot_prods = eigen_model(X_batch)\n",
        "\n",
        "        # High hessian loss\n",
        "        high_hessian_loss = -(H_vs**2).mean()\n",
        "\n",
        "        # Orthgonality Loss\n",
        "        orthogonality_loss = abs(torch.masked_select(dot_prods, lower_triangular_mask)).mean()\n",
        "\n",
        "        # Compute losses\n",
        "        total_loss = total_loss + high_hessian_loss + lambda_penalty * orthogonality_loss\n",
        "        high_hessian_losses = high_hessian_losses + high_hessian_loss\n",
        "        orthogonality_losses = orthogonality_losses + orthogonality_loss\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      n_batches += n_subbatches\n",
        "      (total_loss/n_subbatches).backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      total_losses = total_losses + total_loss\n",
        "\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch) % round(num_epochs/10) == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total total_losses: {total_losses.item()/n_batches:.3f}, \"\n",
        "              f\"High hessian Loss: {high_hessian_losses.item()/n_batches:.3f}, \"\n",
        "              f\"orthogonality Term: {orthogonality_losses.item()/n_batches:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLhniYXiLMPR"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, k, model, criterion):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "\n",
        "        n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(k, n))\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # Normalize columns of U\n",
        "        self.U.data = self.U.data / torch.norm(self.U.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        U = self.U  # [n, k]\n",
        "        model = self.model\n",
        "        criterion = self.criterion\n",
        "        loss = criterion(self.model(X_batch), self.model(X_batch).detach())\n",
        "\n",
        "        H_vs = []\n",
        "        grads = torch.cat([g.flatten() for g in torch.autograd.grad(\n",
        "            loss, self.model.parameters(), create_graph=True)])\n",
        "        grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "        for i in range(U.shape[0]):\n",
        "\n",
        "          # Hessian-vector product\n",
        "          H_v = torch.autograd.grad(torch.dot(grads_flat,U[i,:]), self.model.parameters(), retain_graph=True)  #\n",
        "          Hv_flat = torch.dot(U[i,:] ,torch.cat([h.view(-1) for h in H_v]))\n",
        "          H_vs.append(Hv_flat) # 1 x k (k x n) = 1 x n\n",
        "\n",
        "        # d2L/dv2\n",
        "        H_vs_tensor = torch.stack(H_vs)\n",
        "\n",
        "        # Magnitude and direction of each U vector.\n",
        "        Hv_U = einops.repeat(H_vs_tensor, 'k->k n', n=U.shape[1]) * U\n",
        "\n",
        "        # Dot product of U-vectors, scaled by magnitudes.\n",
        "        dot_prods = Hv_U @ einops.rearrange(Hv_U, 'k n -> n k')\n",
        "\n",
        "        return H_vs_tensor, dot_prods\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(eigen_model, learning_rate, num_epochs, lambda_penalty, dataloader, criterion, hessian_samples):\n",
        "\n",
        "\n",
        "  # Number of parameters in the model\n",
        "  n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "\n",
        "  # Create a mask for the lower triangular part (including diagonal) to be used for dot products.\n",
        "  lower_triangular_mask = torch.tril(torch.ones(eigen_model.U.shape[0], eigen_model.U.shape[0], dtype=torch.bool), diagonal=-1)\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigen_model.parameters(), lr=learning_rate)\n",
        "  eigen_model.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    total_losses = 0\n",
        "    high_hessian_losses = 0\n",
        "    orthogonality_losses = 0\n",
        "    optimizer.zero_grad()\n",
        "    n_batches = 0\n",
        "    for X, Y in dataloader:\n",
        "\n",
        "      total_loss = 0\n",
        "      hessian_dataloader = DataLoader(TensorDataset(X, Y), batch_size=hessian_samples, shuffle=True,\n",
        "                                      generator=torch.Generator(device='cuda'))\n",
        "      n_subbatches = 0\n",
        "      for X_batch, _ in hessian_dataloader:\n",
        "        n_subbatches += 1\n",
        "        # Compute d2L/du2 and dot product of u vectors scaled by magnitudes.\n",
        "        H_vs, dot_prods = eigen_model(X_batch)\n",
        "\n",
        "        # High hessian loss\n",
        "        high_hessian_loss = -(H_vs**2).mean()\n",
        "\n",
        "        # Orthgonality Loss\n",
        "        orthogonality_loss = abs(torch.masked_select(dot_prods, lower_triangular_mask)).mean()\n",
        "\n",
        "        # Compute losses\n",
        "        total_loss = total_loss + high_hessian_loss + lambda_penalty * orthogonality_loss\n",
        "        high_hessian_losses = high_hessian_losses + high_hessian_loss\n",
        "        orthogonality_losses = orthogonality_losses + orthogonality_loss\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      n_batches += n_subbatches\n",
        "      (total_loss/n_subbatches).backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      total_losses = total_losses + total_loss\n",
        "\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch) % round(num_epochs/10) == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total total_losses: {total_losses.item()/n_batches:.3f}, \"\n",
        "              f\"High hessian Loss: {high_hessian_losses.item()/n_batches:.3f}, \"\n",
        "              f\"orthogonality Term: {orthogonality_losses.item()/n_batches:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezqZtZ2vQZxt"
      },
      "source": [
        "## XORNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnrJqOTsQa-B",
        "outputId": "f047f5a3-9950-4926-d00f-41eedebc810f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1000, Loss: 0.014485098421573639\n",
            "Epoch 2000, Loss: 0.0002562731970101595\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-af678408a066>:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return model, torch.Tensor(params), torch.Tensor(losses)\n"
          ]
        }
      ],
      "source": [
        "#@title Train XORNet\n",
        "starting_params = []\n",
        "ending_params = []\n",
        "losses = []\n",
        "\n",
        "model = XORNet()\n",
        "xornet_model, params, loss = TrainModel(model, nn.MSELoss(), .01, dataloader_xornet, n_epochs=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "PfrqFV-vVhRi",
        "outputId": "b39d47b5-2af2-49c9-b5e1-b0ef717f7768"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'einops' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-242963dc06ec>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlambda_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xornet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n f -> (10 n) f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_xornet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n f -> (10 n) f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'einops' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Train eigenmodel\n",
        "model = xornet_model\n",
        "criterion = nn.MSELoss()\n",
        "k = 3\n",
        "\n",
        "batch_size = 10\n",
        "hessian_samples = 2\n",
        "learning_rate = .001\n",
        "n_eprochs = 1000\n",
        "lambda_penalty = 1\n",
        "\n",
        "X = einops.repeat(X_xornet, 'n f -> (10 n) f')\n",
        "Y = einops.repeat(Y_xornet, 'n f -> (10 n) f')\n",
        "\n",
        "X_examples = X_xornet\n",
        "Y_examples = Y_xornet\n",
        "\n",
        "\n",
        "# Set up eigenmodel and data loader.\n",
        "eigen_model = EigenEstimation(k, model,  nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X, Y),\n",
        "                        batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "\n",
        "# Train model\n",
        "TrainEigenestimation(eigen_model, learning_rate,n_eprochs, lambda_penalty,\n",
        "                     dataloader=dataloader, criterion=nn.MSELoss(), hessian_samples=hessian_samples)\n",
        "\n",
        "\n",
        "# Look up examples\n",
        "dataloader = DataLoader(TensorDataset(X_examples, Y_examples), batch_size=1,\n",
        "                        shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "print((eigen_model.U @ eigen_model.U.transpose(0,1)).detach().cpu().numpy().round(3))\n",
        "for x,_ in dataloader:\n",
        "  H, _ = eigen_model(x)\n",
        "  print(x, H.detach().cpu().numpy().round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxrTHJJ-XebY"
      },
      "source": [
        "## TMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "DGLsCvopXt18",
        "outputId": "d58879ba-5d15-488a-c29a-a77c2d058ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1000, Loss: 0.01147507969290018\n",
            "Epoch 2000, Loss: 0.015340195037424564\n",
            "Epoch 3000, Loss: 0.025658918544650078\n",
            "Epoch 4000, Loss: 0.03195729851722717\n",
            "Epoch 5000, Loss: 0.02338690683245659\n",
            "Epoch 6000, Loss: 0.006824633572250605\n",
            "Epoch 7000, Loss: 0.015202147886157036\n",
            "Epoch 8000, Loss: 0.01539059542119503\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-203-01b684f3c8d5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     num_features=n_features, num_datapoints=n_datapoints, sparsity=sparsity, batch_size=batch_size)\n\u001b[1;32m     13\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_TMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-191-af678408a066>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, criterion, learning_rate, dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \"\"\"\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Train TMS\n",
        "n_features = 5\n",
        "hidden_dim = 2\n",
        "n_datapoints = 1024\n",
        "sparsity = .05\n",
        "\n",
        "batch_size = 24\n",
        "learning_rate = .001\n",
        "n_epochs = 10000\n",
        "\n",
        "X_tms, Y_tms, dataloader_TMS = GenerateTMSData(\n",
        "    num_features=n_features, num_datapoints=n_datapoints, sparsity=sparsity, batch_size=batch_size)\n",
        "autoencoder = Autoencoder(n_features, hidden_dim)\n",
        "_, _, _ = TrainModel(autoencoder, nn.MSELoss(), learning_rate, dataloader_TMS, n_epochs=n_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "G5FpJAFb3APR",
        "outputId": "101e3324-511a-41c1-c34b-eba4155ce2e0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "en = autoencoder.W.detach().cpu().numpy()\n",
        "\n",
        "for i in range(en.shape[1]):\n",
        "  plt.plot([0, en[0,i]], [0,en[1,i]], 'b-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LznJ7LgOFpG",
        "outputId": "5b27eb7d-e3e8-4eb4-bb07-ae10a4169ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Total total_losses: -0.016, High hessian Loss: -0.016, orthogonality Term: 0.001\n",
            "Epoch [11/100], Total total_losses: -0.021, High hessian Loss: -0.021, orthogonality Term: 0.002\n",
            "Epoch [21/100], Total total_losses: -0.028, High hessian Loss: -0.028, orthogonality Term: 0.002\n",
            "Epoch [31/100], Total total_losses: -0.040, High hessian Loss: -0.040, orthogonality Term: 0.003\n",
            "Epoch [41/100], Total total_losses: -0.046, High hessian Loss: -0.047, orthogonality Term: 0.004\n",
            "Epoch [51/100], Total total_losses: -0.055, High hessian Loss: -0.056, orthogonality Term: 0.006\n",
            "Epoch [61/100], Total total_losses: -0.066, High hessian Loss: -0.066, orthogonality Term: 0.007\n",
            "Epoch [71/100], Total total_losses: -0.075, High hessian Loss: -0.076, orthogonality Term: 0.009\n",
            "Epoch [81/100], Total total_losses: -0.092, High hessian Loss: -0.093, orthogonality Term: 0.012\n",
            "Epoch [91/100], Total total_losses: -0.101, High hessian Loss: -0.102, orthogonality Term: 0.015\n"
          ]
        }
      ],
      "source": [
        "#@title Train Eigenmodel\n",
        "batch_size = 48\n",
        "hessian_samples = 2\n",
        "learning_rate = .1\n",
        "n_epochs = 100\n",
        "lambda_penalty = .1\n",
        "\n",
        "eigen_model = EigenEstimation(5, autoencoder,  nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X_tms, Y_tms),\n",
        "                        batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "\n",
        "TrainEigenestimation(eigen_model, learning_rate, n_epochs, lambda_penalty,\n",
        "                     dataloader=dataloader, criterion=nn.MSELoss(), hessian_samples=hessian_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-qMRT5Gr58d",
        "outputId": "42fb3bf9-e5ce-4422-9429-03bc72953c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.065 -0.425  0.049 -0.051  0.031 -0.046  0.644  0.045  0.102  0.083\n",
            "   0.093  0.587 -0.054  0.057  0.115]\n",
            " [ 0.054 -0.043  0.681  0.019 -0.07  -0.022  0.158  0.408 -0.112 -0.081\n",
            "   0.013  0.157 -0.527 -0.028 -0.112]\n",
            " [ 0.102  0.003  0.516 -0.044 -0.007  0.133  0.007  0.618 -0.015  0.049\n",
            "  -0.148 -0.027 -0.543 -0.002  0.039]\n",
            " [-0.03  -0.041  0.    -0.058 -0.623 -0.008 -0.019  0.009 -0.034 -0.475\n",
            "  -0.018  0.01  -0.004  0.002 -0.615]\n",
            " [-0.138  0.117  0.077  0.38   0.042 -0.15   0.059 -0.     0.126 -0.408\n",
            "   0.011 -0.734 -0.134  0.034  0.223]]\n",
            "[[ 1.     0.263  0.066 -0.121 -0.445]\n",
            " [ 0.263  1.     0.88   0.158  0.005]\n",
            " [ 0.066  0.88   1.    -0.034  0.066]\n",
            " [-0.121  0.158 -0.034  1.    -0.003]\n",
            " [-0.445  0.005  0.066 -0.003  1.   ]]\n",
            "tensor([[0., 0., 0., 0., 1.]], device='cuda:0') [0.031 0.044 0.003 1.969 0.028]\n",
            "tensor([[1., 0., 0., 0., 0.]], device='cuda:0') [0.028 0.007 0.031 0.177 0.056]\n",
            "tensor([[1., 0., 0., 0., 0.]], device='cuda:0') [0.028 0.007 0.031 0.177 0.056]\n",
            "tensor([[0., 0., 1., 0., 1.]], device='cuda:0') [0.001 0.008 0.052 0.001 0.069]\n",
            "tensor([[0., 0., 0., 1., 0.]], device='cuda:0') [0.241 0.014 0.    0.207 0.438]\n",
            "tensor([[1., 0., 0., 0., 0.]], device='cuda:0') [0.028 0.007 0.031 0.177 0.056]\n",
            "tensor([[0., 0., 0., 0., 1.]], device='cuda:0') [0.031 0.044 0.003 1.969 0.028]\n",
            "tensor([[1., 0., 0., 1., 0.]], device='cuda:0') [0.013 0.024 0.028 0.236 0.223]\n",
            "tensor([[1., 1., 0., 0., 0.]], device='cuda:0') [0. 0. 0. 0. 0.]\n",
            "tensor([[1., 0., 0., 1., 0.]], device='cuda:0') [0.013 0.024 0.028 0.236 0.223]\n",
            "tensor([[0., 0., 0., 1., 0.]], device='cuda:0') [0.241 0.014 0.    0.207 0.438]\n",
            "tensor([[0., 1., 0., 0., 0.]], device='cuda:0') [1.915 0.089 0.    0.    0.25 ]\n",
            "tensor([[0., 1., 0., 0., 0.]], device='cuda:0') [1.915 0.089 0.    0.    0.25 ]\n",
            "tensor([[1., 0., 0., 0., 0.]], device='cuda:0') [0.028 0.007 0.031 0.177 0.056]\n",
            "tensor([[0., 0., 1., 0., 0.]], device='cuda:0') [0.017 1.884 2.035 0.    0.056]\n",
            "tensor([[0., 0., 1., 0., 0.]], device='cuda:0') [0.017 1.884 2.035 0.    0.056]\n",
            "tensor([[0., 0., 1., 1., 0.]], device='cuda:0') [0.201 1.734 1.788 0.003 0.615]\n",
            "tensor([[0., 1., 0., 0., 0.]], device='cuda:0') [1.915 0.089 0.    0.    0.25 ]\n",
            "tensor([[0., 0., 0., 0., 1.]], device='cuda:0') [0.031 0.044 0.003 1.969 0.028]\n",
            "tensor([[1., 0., 0., 0., 0.]], device='cuda:0') [0.028 0.007 0.031 0.177 0.056]\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(TensorDataset(X_tms[:20,], Y_tms[:20,]), batch_size=1,\n",
        "                        shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "print(eigen_model.U.detach().cpu().numpy().round(3))\n",
        "print((eigen_model.U @ eigen_model.U.transpose(0,1)).detach().cpu().numpy().round(3))\n",
        "for x,_ in dataloader:\n",
        "  H, _ = eigen_model((x>0).float())\n",
        "  print((x>0).float(), H.detach().cpu().numpy().round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_MfhIAz-AW"
      },
      "source": [
        "## Polytope Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "XgnXgkA8xC_n",
        "outputId": "b498f702-a218-4e92-84eb-575dfdd6c48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 300, Loss: 0.005462843459099531\n",
            "Epoch 400, Loss: 0.0075173145160079\n",
            "Epoch 500, Loss: 0.001831928500905633\n",
            "Epoch 600, Loss: 0.007628620136529207\n",
            "Epoch 700, Loss: 0.008919965475797653\n",
            "Epoch 800, Loss: 0.005925615318119526\n",
            "Epoch 900, Loss: 0.006718121003359556\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Y')"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Train Polytope\n",
        "n_datapoints = 100\n",
        "X_polytope = torch.rand((n_datapoints, 1)) * 4 - 2\n",
        "Y_polytope = X_polytope**2\n",
        "dataloader_polytope = DataLoader(TensorDataset(X_polytope, Y_polytope), batch_size=48, shuffle=True,\n",
        "                                 generator=torch.Generator(device='cuda'))\n",
        "\n",
        "polytope_model = FeedForwardNN(1, 10, 1, 1)\n",
        "_, _, _ = TrainModel(polytope_model, nn.MSELoss(), .01, dataloader_polytope, n_epochs=1000)\n",
        "\n",
        "\n",
        "# Predict on X_polytope\n",
        "with torch.no_grad():\n",
        "    predictions = polytope_model(X_polytope).cpu()\n",
        "\n",
        "# Plot X and predictions\n",
        "plt.scatter(X_polytope.cpu().numpy(), Y_polytope.cpu().numpy(), label='Ground Truth')\n",
        "plt.scatter(X_polytope.cpu().numpy(), predictions.numpy(), label='Predictions', marker='.')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLuyQU1wycBB",
        "outputId": "08b4e5e6-4406-4fb2-e5b5-e204426259d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Total total_losses: -0.170, High hessian Loss: -0.188, orthogonality Term: 0.017\n",
            "Epoch [101/1000], Total total_losses: -165.690, High hessian Loss: -165.731, orthogonality Term: 0.041\n",
            "Epoch [201/1000], Total total_losses: -219.068, High hessian Loss: -220.925, orthogonality Term: 1.857\n",
            "Epoch [301/1000], Total total_losses: -247.229, High hessian Loss: -248.131, orthogonality Term: 0.902\n",
            "Epoch [401/1000], Total total_losses: -234.496, High hessian Loss: -235.633, orthogonality Term: 1.136\n",
            "Epoch [501/1000], Total total_losses: -224.945, High hessian Loss: -227.445, orthogonality Term: 2.500\n",
            "Epoch [601/1000], Total total_losses: -238.951, High hessian Loss: -240.668, orthogonality Term: 1.718\n",
            "Epoch [701/1000], Total total_losses: -239.231, High hessian Loss: -243.181, orthogonality Term: 3.951\n",
            "Epoch [801/1000], Total total_losses: -222.532, High hessian Loss: -224.532, orthogonality Term: 2.000\n",
            "Epoch [901/1000], Total total_losses: -233.587, High hessian Loss: -235.518, orthogonality Term: 1.932\n"
          ]
        }
      ],
      "source": [
        "#@title Train Eigenmodel\n",
        "batch_size = 48\n",
        "hessian_samples = 4\n",
        "learning_rate = .001\n",
        "n_epochs = 1000\n",
        "lambda_penalty = 1\n",
        "\n",
        "eigen_model = EigenEstimation(4, polytope_model,  nn.MSELoss())\n",
        "dataloader =  DataLoader(TensorDataset(X_polytope, Y_polytope), batch_size=batch_size, shuffle=True,\n",
        "                                 generator=torch.Generator(device='cuda'))\n",
        "\n",
        "# Train model\n",
        "TrainEigenestimation(eigen_model, learning_rate, n_epochs, lambda_penalty,\n",
        "                     dataloader=dataloader, criterion=nn.MSELoss(), hessian_samples=hessian_samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU-jm5KbzAZo",
        "outputId": "39228863-fb3d-4cc5-963e-d11e4f87c316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 3.431e+00 -2.700e-02  3.000e-03  1.740e-01]\n",
            " [-2.700e-02  1.000e+00  9.300e-02  2.000e-03]\n",
            " [ 3.000e-03  9.300e-02  1.000e+00 -5.200e-02]\n",
            " [ 1.740e-01  2.000e-03 -5.200e-02  1.060e+00]]\n",
            "[-2.] --> [ 0.459  0.267  0.166 75.855]\n",
            "[-1.556] --> [4.1100e-01 3.3000e-02 1.2800e-01 5.3717e+01]\n",
            "[-1.111] --> [6.8800e-01 4.0000e-03 1.2500e-01 3.5569e+01]\n",
            "[-0.667] --> [1.607e+00 4.000e-02 8.000e-03 8.863e+00]\n",
            "[-0.222] --> [3.156 0.13  0.    5.039]\n",
            "[0.222] --> [5.222 0.285 0.    2.282]\n",
            "[0.667] --> [20.819  0.787  0.     0.347]\n",
            "[1.111] --> [3.7822e+01 3.0000e-03 5.4000e-02 8.6000e-02]\n",
            "[1.556] --> [6.0097e+01 8.8000e-02 1.0000e-03 6.4000e-02]\n",
            "[2.] --> [8.5546e+01 1.4300e-01 1.0000e-02 4.8000e-02]\n"
          ]
        }
      ],
      "source": [
        "# Look up examples\n",
        "X_examples = torch.linspace(-2, 2, 10)\n",
        "dataloader = DataLoader(X_examples, batch_size=1,\n",
        "                        shuffle=False, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "print((eigen_model.U @ eigen_model.U.transpose(0,1)).detach().cpu().numpy().round(3))\n",
        "for x in dataloader:\n",
        "  H, _ = eigen_model(x)\n",
        "  print(x.detach().cpu().numpy().round(3), '-->', H.detach().cpu().numpy().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsyzyVsNJWWe",
        "outputId": "54cae94e-9268-4f07-8a26-eba4103d6f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.1377, -0.2146,  0.0892,  1.0192],\n",
            "        [ 1.1534, -0.0875, -0.0572, -0.2740],\n",
            "        [ 0.5157,  0.5628,  0.5847,  0.2438]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assume model is your pre-trained model and perturbations is a list of tensors\n",
        "# containing small values to add to each parameter in the model.\n",
        "\n",
        "# Store the original state of the model\n",
        "\n",
        "from torch.func import functional_call\n",
        "\n",
        "# Function to perturb weights using functional_call\n",
        "def PerturbWeights(model, X_batch, perturbation_direction):\n",
        "    # Get original parameters as a dictionary\n",
        "    original_params = dict(model.named_parameters())\n",
        "\n",
        "    # Create perturbed parameters\n",
        "    perturbed_params = {\n",
        "        name: param + perturb for (name, param), perturb in zip(original_params.items(), perturbation_direction)\n",
        "    }\n",
        "\n",
        "    # Evaluate the model with perturbed parameters\n",
        "    perturbed_output = functional_call(model, perturbed_params, (X_batch,))\n",
        "    return perturbed_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Gradient(nn.Module):\n",
        "    def __init__(self, model, criterion):\n",
        "        super(Gradient, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.eps = 1e-7\n",
        "        # Copy model parameters\n",
        "        # Make self.w_eps a set of parameters\n",
        "        self.w_eps = [self.eps*torch.randn_like(param).requires_grad_(True) for name, param in self.model.named_parameters()]\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "      output1 = PerturbWeights(self.model, X_batch, 0*self.w_eps).detach()#self.model(X_batch).detach()\n",
        "      output2 = PerturbWeights(self.model, X_batch, self.w_eps)\n",
        "\n",
        "      #L1 = self.criterion(output1, output1)\n",
        "      L1 = self.criterion(output1, output1)\n",
        "\n",
        "      L2 = self.criterion(output2, output1)\n",
        "      #print(f'L2 requires_grad: {L2.requires_grad}')\n",
        "\n",
        "      dw = torch.cat([param.flatten() for param in gradient_model.w_eps]).norm()\n",
        "      dL_dw = (L2-L1) / dw\n",
        "\n",
        "\n",
        "      return dL_dw\n",
        "\n",
        "gradient_model = Gradient(zero_loss_model, nn.MSELoss())\n",
        "dL_dws = []\n",
        "for i,x in enumerate(X):\n",
        "  grad = torch.autograd.grad(\n",
        "      gradient_model(X[[i],:]),\n",
        "      gradient_model.w_eps,\n",
        "      retain_graph=True)\n",
        "  dL_dw = torch.stack(grad).flatten()\n",
        "  dL_dws.append(dL_dw)\n",
        "\n",
        "dL_dw_tensor = torch.stack(dL_dws)\n",
        "print(dL_dw_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVJyOV-GZL9c",
        "outputId": "27fe7e2a-4cd6-496d-f9bd-a0f0855217ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Second derivative (Hessian-vector product) for parameter 0:\n",
            "tensor([[ 2.2313,  0.0000],\n",
            "        [-0.2531, -0.0000]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assume zero_loss_model is your pre-trained model\n",
        "model = zero_loss_model  # Replace with your actual model\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Ensure model parameters have requires_grad=True\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Step 1: Compute the loss\n",
        "X_batch = X[[2],:]\n",
        "output = model(X_batch).detach()\n",
        "loss = criterion(output, model(X_batch))\n",
        "\n",
        "# Step 2: Compute the first derivative (gradient) w.r.t. model parameters\n",
        "grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "# Step 3: Compute the second derivative (Hessian-vector product)\n",
        "# To compute the full Hessian is computationally intensive; instead, compute Hessian-vector products\n",
        "# For demonstration, we'll compute the Hessian with respect to a vector of ones\n",
        "v = [torch.ones_like(p) for p in model.parameters()]\n",
        "\n",
        "# Compute the dot product of gradients and vector v\n",
        "grad_dot_v = sum(torch.sum(g * v_i) for g, v_i in zip(grads, v))\n",
        "\n",
        "# Compute the second derivative (Hessian-vector product)\n",
        "hessian_vec = torch.autograd.grad(grad_dot_v, model.parameters(), retain_graph=True)\n",
        "\n",
        "# hessian_vec contains the second derivatives (Hessian-vector product)\n",
        "for i, hv in enumerate(hessian_vec):\n",
        "    print(f\"Second derivative (Hessian-vector product) for parameter {i}:\")\n",
        "    print(hv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbExW8YkZGz2",
        "outputId": "c95d78c8-2138-4343-895f-a97895e45745"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.8598e-08, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " gradient_model(X[[1],:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj02WJ1sWBhR",
        "outputId": "9a498fa8-e00a-4263-ea25-1b704db9be04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[ 1.1534, -0.0875],\n",
              "         [-0.0572, -0.2740]], device='cuda:0')]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs = gradient_model(X[[2],:]).backward(inputs=gradient_model.w_eps)\n",
        "[i.grad for i in gradient_model.w_eps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "iQFaXQjfVxQ9",
        "outputId": "9e719830-d2f0-4a73-f634-2731c1cc1477"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-482f12ae792e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m torch.autograd.grad(gradient_model(x),\n\u001b[0m\u001b[1;32m      2\u001b[0m                       \u001b[0mgradient_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_eps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       retain_graph=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-42d30b14d130>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerturbWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#self.model(X_batch).detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m       \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerturbWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-42d30b14d130>\u001b[0m in \u001b[0;36mPerturbWeights\u001b[0;34m(model, X_batch, perturbation_direction)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Evaluate the model with perturbed parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mperturbed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturbed_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mperturbed_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/functional_call.py\u001b[0m in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     return nn.utils.stateless._functional_call(\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mparameters_and_buffers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/stateless.py\u001b[0m in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_and_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtie_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     ):\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-14b43e2e36d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply sigmoid after first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#x = self.relu(self.fc2(x))  # Apply sigmoid after second layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.sigmoid()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ],
      "source": [
        "torch.autograd.grad(gradient_model(x),\n",
        "                      gradient_model.w_eps,\n",
        "                      retain_graph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "UL_EJxCOQM5J",
        "outputId": "e3affd67-f487-4dfd-d7df-a2a8c1da8de2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_batch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-f7c521cd9a3a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPerturbWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturbation_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_batch' is not defined"
          ]
        }
      ],
      "source": [
        "PerturbWeights(model, X_batch, perturbation_direction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "collapsed": true,
        "id": "ga4YrDzN_BSs",
        "outputId": "d9f2fc58-fd3d-4dca-ae94-405967ba90f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Model 0----\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'SGD' and 'float'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-c46b0efa5db9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'----Model {i}----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_xornet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mPlotTrajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mstarting_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-191-af678408a066>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, criterion, learning_rate, dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Randomly initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable, fused)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ):\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid learning rate: {lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'SGD' and 'float'"
          ]
        }
      ],
      "source": [
        "#@title Old SLT stuff\n",
        "starting_params = []\n",
        "ending_params = []\n",
        "losses = []\n",
        "\n",
        "for i in range(5):\n",
        "  model = XORNet()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "  print(f'----Model {i}----')\n",
        "  model, params, loss = TrainModel(model, nn.MSELoss(), optimizer, dataloader_xornet, n_epochs=100)\n",
        "  PlotTrajectories(loss, params)\n",
        "  starting_params.append(params[0])\n",
        "  ending_params.append(params[-1])\n",
        "  losses.append(loss[-1])\n",
        "  if loss[-1] < .01:\n",
        "    zero_loss_model = model\n",
        "    print(\"Zero loss model\")\n",
        "  if loss[-1] > .1:\n",
        "    nonzero_loss_model = model\n",
        "    print(\"Non-zero loss model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73uLRGKoMOSx",
        "outputId": "65e9b4c9-33d2-4af6-e7c7-e915d3f5d5cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.1197e-06, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Turn parameter list into vector\n",
        "torch.cat([param.flatten() for param in gradient_model.w_eps]).norm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS7TjiUjK717",
        "outputId": "50944539-73ce-43d0-a6ce-023c82b89f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 requires_grad: True\n",
            "tensor(0., device='cuda:0', grad_fn=<SubBackward0>)\n",
            "Gradient for w_eps[0]: tensor([[0., 0.],\n",
            "        [0., 0.]], device='cuda:0', grad_fn=<TBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "output = gradient_model(X[[3], :])\n",
        "\n",
        "# Compute gradients\n",
        "grad_w_eps = torch.autograd.grad(output, list(gradient_model.w_eps), create_graph=True)\n",
        "\n",
        "# Check the gradients for each element in w_eps\n",
        "for i, grad in enumerate(grad_w_eps):\n",
        "    print(f'Gradient for w_eps[{i}]: {grad}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnG5rt6OJAE9"
      },
      "source": [
        "## Hessian Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB6MGMciH0Nk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def HessianBatch(model, X, criterion):\n",
        "  hessians = np.array([])\n",
        "  # Compute the loss\n",
        "  outputs = model(X)\n",
        "  loss = criterion(outputs, outputs.detach())\n",
        "\n",
        "  # Compute gradients w.r.t. parameters\n",
        "  grads = torch.autograd.grad(loss, list(model.parameters()), create_graph=True)\n",
        "  flat_grad = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "  # Compute the Hessian matrix\n",
        "  num_params = flat_grad.shape[0]\n",
        "  hessian_matrix = torch.zeros(num_params, num_params)\n",
        "  for idx in range(num_params):\n",
        "    grad2 = torch.autograd.grad(flat_grad[idx], model.parameters(), retain_graph=True)\n",
        "    grad2_flat = torch.cat([g.contiguous().view(-1) for g in grad2])\n",
        "    hessian_matrix[idx] = grad2_flat.detach()\n",
        "\n",
        "    # Print the Hessian matrix\n",
        "    #print('Sample')\n",
        "    #print('gradient', flat_grad)\n",
        "    #print(X_i.data, '->', Y_i.data, outputs.data, loss)\n",
        "\n",
        "  return hessian_matrix\n",
        "\n",
        "\n",
        "\n",
        "def Hessian(model, X, criterion):\n",
        "  hessians = np.array([])\n",
        "  # Compute the loss\n",
        "  outputs = model(X)\n",
        "  loss = criterion(outputs, outputs.detach()).sum(dim=-1)\n",
        "\n",
        "  # Compute gradients w.r.t. parameters\n",
        "  flat_params = list(model.parameters())\n",
        "  numel = sum([p.numel() for p in flat_params])\n",
        "  hessian = torch.zeros((len(X), numel, numel))\n",
        "\n",
        "\n",
        "  for x_i, _ in enumerate(X):\n",
        "    grads = torch.autograd.grad(loss[x_i], flat_params, create_graph=True)\n",
        "    grads = torch.cat([g.contiguous().view(-1) for g in grads])\n",
        "    for g_i, g in enumerate(grads):\n",
        "      hessian[x_i, g_i, :] = torch.concat([i.flatten() for i in (torch.autograd.grad(g, flat_params, create_graph=True))])\n",
        "  return hessian\n",
        "\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, n, k):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(n, k))\n",
        "        self.V = nn.Parameter(torch.randn(k, n))\n",
        "\n",
        "\n",
        "    def forward(self, H):\n",
        "        # A is of shape [batch_size, n, n]\n",
        "        # Compute U^T H U for each matrix in the batch\n",
        "        U = self.U  # [n, k]\n",
        "        V = self.V\n",
        "        # Normalize U\n",
        "        #U = U / torch.norm(U, dim=1, keepdim=True)  # [n, k]\n",
        "        #U_t = U.transpose(0, 1)  # [k, n]\n",
        "        #U_t = U.transpose(0, 1)  # [k, n]\n",
        "\n",
        "\n",
        "        # Compute M = P H U\n",
        "        #UVHUV\n",
        "        M = abs(torch.matmul(V.unsqueeze(0), torch.matmul(H, U.unsqueeze(0))))  # [batch_size, k, k]\n",
        "\n",
        "        # Extract the diagonal elements of M\n",
        "        diag_elements = (torch.diagonal(M, dim1=-2, dim2=-1))  # [batch_size, k]\n",
        "        #print(diag_elements)\n",
        "\n",
        "        # Create diagonal matrices D from diag_elements\n",
        "        D = torch.zeros_like(M)\n",
        "        indices = torch.arange(M.size(-1))\n",
        "        D[:, indices, indices] = diag_elements\n",
        "\n",
        "        # Reconstruct A\n",
        "        H_reconstructed = torch.matmul(U.unsqueeze(0), torch.matmul(M, V.unsqueeze(0)))  # [batch_size, n, n]\n",
        "\n",
        "        return H_reconstructed, diag_elements\n",
        "\n",
        "\n",
        "def total_loss_function(H, H_reconstructed, diag_elements, lambda_penalty=.1):\n",
        "  # Reconstruction Loss\n",
        "  reconstruction_loss = torch.mean((H - H_reconstructed) ** 2)\n",
        "\n",
        "  # Penalty Term: L2 norm squared of diag_elements\n",
        "  penalty_term = torch.mean(torch.mean(abs(diag_elements), dim=1), dim=0)  # Mean over batch\n",
        "  #print(diag_elements, penalty_term)\n",
        "\n",
        "  # Total Loss\n",
        "  total_loss = reconstruction_loss + lambda_penalty * penalty_term\n",
        "\n",
        "  return total_loss, reconstruction_loss, penalty_term\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(model, learning_rate, num_epochs, lambda_penalty, k, dataloader, criterion):\n",
        "  # Hyperparameters\n",
        "  n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "\n",
        "  # Create model\n",
        "  eigen_model = EigenEstimation(n, k)\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigen_model.parameters(), lr=learning_rate)\n",
        "  eigen_model.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    total_losses = 0\n",
        "    reconstruction_losses = 0\n",
        "    penalty_terms = 0\n",
        "    optimizer.zero_grad()\n",
        "    for X_batch, Y_batch in dataloader:\n",
        "      #print(f'epoch{epoch}')\n",
        "\n",
        "\n",
        "      H = Hessian(model, X_batch+.0*torch.randn_like(X_batch), criterion=criterion(reduction='none'))\n",
        "      # Forward pass\n",
        "      H_reconstructed, diag_elements = eigen_model(H)\n",
        "      #print(X_batch, H_reconstructed)\n",
        "\n",
        "      #print(H, diag_elements, H_reconstructed, diag_elements)\n",
        "      # Compute loss\n",
        "      total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "          H, H_reconstructed, diag_elements, lambda_penalty\n",
        "      )\n",
        "      total_losses = total_losses + total_loss\n",
        "      reconstruction_losses = reconstruction_losses + reconstruction_loss\n",
        "      penalty_terms = penalty_terms + penalty_term\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % round(num_epochs/10) == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total total_losses: {total_losses.item():.6f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_losses.item():.6f}, \"\n",
        "              f\"Penalty Term: {penalty_terms.item():.6f}\")\n",
        "    if reconstruction_losses < .01: break\n",
        "\n",
        "  return eigen_model\n",
        "\n",
        "\n",
        "\n",
        "def ComputeDiagonals(model, eigenmodel, dataloader, criterion):\n",
        "  diag_elements_list = []\n",
        "  for X_batch, Y_batch in dataloader:\n",
        "    H = Hessian(model, X_batch, criterion=nn.MSELoss(reduction='none'))\n",
        "\n",
        "    # Forward pass\n",
        "    H_reconstructed, diag_elements = eigenmodel(H)\n",
        "\n",
        "    diag_elements_list.append(diag_elements)\n",
        "    #print(X_batch)\n",
        "    #print(H, H_reconstructed)\n",
        "    print(X_batch , '->', diag_elements.detach().numpy().round(2))\n",
        "  return torch.Tensor(torch.concat(diag_elements_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvKK4frLhiC6"
      },
      "source": [
        "## Partial Hessian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al7mY9fRQ5ai"
      },
      "outputs": [],
      "source": [
        "def PearlmutterProducts(R, L, grads_flat, params, vector):\n",
        "    ### Compute y1 = V H U r1\n",
        "    # Step 1: s1 = U r1\n",
        "    # Step 2: Compute H s1 via Hessian-vector product\n",
        "    grads_R_v = torch.dot(grads_flat, R @ vector )\n",
        "    HR_v = torch.autograd.grad(grads_R_v, params, retain_graph=True)[0]  # H_s1 ∈ ℝ^{n}\n",
        "\n",
        "    # Step 3: y1 = V H_s1\n",
        "    LHR_r = L @ HR_v.flatten()  # y1 ∈ ℝ^{k}\n",
        "    return LHR_r\n",
        "\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, k, model, criterion):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "\n",
        "        n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(n, k))\n",
        "        self.V = nn.Parameter(torch.randn(k, n))\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "    def forward(self, X_batch, orig_idxs, transformed_idxs):\n",
        "        # A is of shape [batch_size, n, n]\n",
        "        # Compute U^T H U for each matrix in the batch\n",
        "\n",
        "        # Normalize columns of U\n",
        "        self.U.data = self.U.data / torch.norm(self.U.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        # Normalize columns of V\n",
        "        self.V.data = self.V.data / torch.norm(self.V.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        U = self.U  # [n, k]\n",
        "        V = self.V\n",
        "\n",
        "\n",
        "        model = self.model\n",
        "        criterion = self.criterion\n",
        "        loss = criterion(model(X_batch), model(X_batch).detach())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)[0]\n",
        "        grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "        H_rs = []\n",
        "        UVHUV_rs = []\n",
        "        diags = []\n",
        "        for i in orig_idxs:\n",
        "\n",
        "          r = torch.zeros(U.shape[0], device='cuda')\n",
        "          r[i] = 1\n",
        "\n",
        "\n",
        "          # True Hessian-vector product\n",
        "          H_r = torch.autograd.grad(torch.dot(grads_flat, r), model.parameters(), retain_graph=True)[0]  # H_s1 ∈ ℝ^{n}\n",
        "          H_r_flat = torch.cat([h.view(-1) for h in H_r])\n",
        "          H_rs.append(H_r_flat)\n",
        "\n",
        "\n",
        "          # True Hessian-vector product\n",
        "          UVHUV_r = PearlmutterProducts(U@V, U@V, grads_flat, model.parameters(), r)\n",
        "          UVHUV_r_flat = torch.cat([h.view(-1) for h in UVHUV_r])\n",
        "          UVHUV_rs.append(UVHUV_r_flat)\n",
        "\n",
        "\n",
        "        for i in transformed_idxs:\n",
        "\n",
        "          r = torch.zeros(U.shape[1], device='cuda')\n",
        "          r[i] = 1\n",
        "\n",
        "          # Transformed Hessian-vector\n",
        "          VHU_r = PearlmutterProducts(U, V, grads_flat, model.parameters(), r)\n",
        "          VHU_r_flat = torch.cat([h.view(-1) for h in VHU_r])\n",
        "          diags.append(VHU_r_flat[i])\n",
        "\n",
        "\n",
        "        return torch.stack(H_rs), torch.stack(UVHUV_rs),  torch.stack(diags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKAd6lfLgKY7"
      },
      "outputs": [],
      "source": [
        "#@ Train\n",
        "def total_loss_function(H, H_reconstructed, diag_elements, lambda_penalty=.1):\n",
        "  # Reconstruction Loss\n",
        "  reconstruction_loss = ((H - H_reconstructed) ** 2).mean()\n",
        "\n",
        "  # Penalty Term: L2 norm squared of diag_elements\n",
        "  penalty_term = abs(diag_elements).mean()  # Mean over batch\n",
        "  #print(diag_elements, penalty_term)\n",
        "\n",
        "  # Total Loss\n",
        "  total_loss = reconstruction_loss + lambda_penalty * penalty_term\n",
        "\n",
        "  return total_loss, reconstruction_loss, penalty_term\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(\n",
        "    eigenmodel, lambda_penalty, dataloader, hessian_samples,\n",
        "    learning_rate, num_epochs, n_params_per_batch, n_transformed_params_per_batch):\n",
        "  # Hyperparameters\n",
        "\n",
        "  k = eigenmodel.U.shape[1]\n",
        "  n = eigenmodel.U.shape[0] # Dimension of the matrices\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigenmodel.parameters(), lr=learning_rate)\n",
        "  eigenmodel.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    reconstruction_losses = 0\n",
        "    penalty_terms = 0\n",
        "    total_losses = 0\n",
        "    n_batch = 0\n",
        "    for X_superbatch, _ in dataloader:\n",
        "      L = 0\n",
        "\n",
        "\n",
        "      sub_dataloader = DataLoader(TensorDataset(X_superbatch), hessian_samples, shuffle=True,\n",
        "                                  generator=torch.Generator(device='cuda'))\n",
        "\n",
        "      for X_batch in sub_dataloader:\n",
        "        n_batch = n_batch + 1\n",
        "        orig_idxs = np.random.choice(n, n_params_per_batch, replace=False)\n",
        "        transformed_idxs = np.random.choice(k, n_transformed_params_per_batch, replace=False)\n",
        "        H, H_reconstructed, diag_elements = eigenmodel(X_batch[0], orig_idxs, transformed_idxs)\n",
        "\n",
        "        # Compute loss\n",
        "        total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "            H, H_reconstructed, diag_elements, lambda_penalty\n",
        "        )\n",
        "        L = L + total_loss\n",
        "\n",
        "        total_losses = total_losses + total_loss\n",
        "        reconstruction_losses = reconstruction_losses + reconstruction_loss\n",
        "        penalty_terms = penalty_terms + penalty_term\n",
        "\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      L.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % round(num_epochs/100) == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "    if reconstruction_losses < .01:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHPJ8o_G7mZt"
      },
      "source": [
        "# Differentiating model with respect to params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "AfNOVXn27pA4",
        "outputId": "c8a472ec-6750-47f0-fb9a-70c56ae70952"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-122-80123a509ba5>:75: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  fmodel, fmodel_params = make_functional(zero_loss_model)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Expected all elements of parameter_and_buffer_dicts to be dictionaries",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-80123a509ba5>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_functional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_loss_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mPerturbWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#gradient_model = Gradient(zero_loss_model, nn.MSELoss())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-80123a509ba5>\u001b[0m in \u001b[0;36mPerturbWeights\u001b[0;34m(fmodel, w0, w_eps, X_batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperturb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mperturbed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#    perturbed_output = functional_call(model, param_dict, X_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/functional_call.py\u001b[0m in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_and_buffer_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_and_buffer_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;34m\"Expected all elements of parameter_and_buffer_dicts to be dictionaries\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected all elements of parameter_and_buffer_dicts to be dictionaries"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assume `model` is your pre-trained model and `perturbations` is a list of tensors\n",
        "# containing small values to add to each parameter in the model.\n",
        "\n",
        "# Store the original state of the model\n",
        "\n",
        "from torch.func import functional_call\n",
        "\n",
        "def PerturbWeightsOld(model, X_batch, perturbation_direction):#, loss_fn=nn.MSELoss(reduction='none')):\n",
        "\n",
        "    # Create perturbed parameters\n",
        "    perturbed_params = {name: param + perturb\n",
        "                        for (name, param), perturb  in zip(model.named_parameters(), perturbation_direction)}\n",
        "\n",
        "    # Evaluate the model with perturbed parameters\n",
        "    perturbed_output = functional_call(model, perturbed_params, X_batch)\n",
        "\n",
        "\n",
        "    return perturbed_output\n",
        "\n",
        "\n",
        "def PerturbWeights(fmodel, w0, w_eps, X_batch):\n",
        "    param_dict = {name: param + perturb for (name, param), perturb in zip(model.named_parameters(), w_eps)}\n",
        "    perturbed_output = functional_call(model, param_dict, X_batch)\n",
        "    return perturbed_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Gradient(nn.Module):\n",
        "    def __init__(self, model, criterion):\n",
        "        super(Gradient, self).__init__()\n",
        "\n",
        "        self.fmodel, self.fmodel_params = make_functional(model)\n",
        "\n",
        "        self.criterion = criterion\n",
        "\n",
        "        # Copy model parameters\n",
        "        # Make self.w_eps a set of parameters\n",
        "        self.w_eps = nn.ParameterList(\n",
        "            [nn.Parameter(torch.zeros_like(param)) for name, param in self.model.named_parameters()]\n",
        "        )\n",
        "\n",
        "    def fmodel_forward(self, params, x):\n",
        "      return self.fmodel(params, x)\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "      output1 = self.model(X_batch, fmodel_params).detach()\n",
        "\n",
        "      output2 = PerturbWeights(self.fmodel, X_batch, self.w_eps)\n",
        "\n",
        "      #L1 = self.criterion(output1, output1)\n",
        "\n",
        "      L2 = self.criterion(output2, output1)\n",
        "      print(L2.requires_grad)  # Should output: True\n",
        "\n",
        "      L2.backward()\n",
        "      for p in self.w_eps:\n",
        "          print(p.grad)  # Should not be None or zero\n",
        "\n",
        "      dL_dw = (L2)\n",
        "      return dL_dw\n",
        "\n",
        "\n",
        "gradient_model = Gradient(zero_loss_model, nn.MSELoss())\n",
        "#gradient_model(X).backward()\n",
        "\n",
        "print(gradient_model(X[[3],:]))\n",
        "\n",
        "#torch.autograd.grad(gradient_model(X[[3],:]), gradient_model.w_eps, create_graph=True)\n",
        "\n",
        "#grad_w_eps = torch.autograd.grad(gradient_model(X[[3], :]), list(gradient_model.w_eps), create_graph=True)\n",
        "#grad_w_eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRPbtcmIFXoa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKf-7yk7DpEj",
        "outputId": "a6a70a07-e9be-4f04-af26-80a625ee3916"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 0.],\n",
              "         [0., 0.]], device='cuda:0', grad_fn=<TBackward0>),)"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to perturb weights\n",
        "def PerturbWeights(model, X_batch, perturbation_direction):\n",
        "    # Get the original parameters\n",
        "    original_params = dict(model.named_parameters())\n",
        "\n",
        "    # Create perturbed parameters\n",
        "    perturbed_params = {\n",
        "        name: param + perturb\n",
        "        for (name, param), perturb in zip(original_params.items(), perturbation_direction)\n",
        "    }\n",
        "\n",
        "    # Get buffers\n",
        "    original_buffers = dict(model.named_buffers())\n",
        "\n",
        "    # Combine perturbed parameters and original buffers\n",
        "    params_and_buffers = {**perturbed_params, **original_buffers}\n",
        "\n",
        "    # Evaluate the model with perturbed parameters\n",
        "    perturbed_output = functional_call(model, params_and_buffers, (X_batch,))\n",
        "    return perturbed_output\n",
        "\n",
        "# Gradient model\n",
        "class Gradient(nn.Module):\n",
        "    def __init__(self, model, criterion):\n",
        "        super(Gradient, self).__init__()\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "\n",
        "        # Initialize w_eps with requires_grad=True\n",
        "        self.w_eps = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(torch.zeros_like(param))\n",
        "                for param in self.model.parameters()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # Detach to prevent gradients flowing back to the model's parameters\n",
        "        output1 = self.model(X_batch).detach()\n",
        "\n",
        "        # Forward pass with perturbed parameters\n",
        "        output2 = PerturbWeights(self.model, X_batch, self.w_eps)\n",
        "\n",
        "        # Compute loss\n",
        "        L2 = self.criterion(output2, output1)\n",
        "        return L2\n",
        "\n",
        "# Instantiate the model and criterion\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Instantiate the Gradient model\n",
        "gradient_model = Gradient(zero_loss_model, criterion)\n",
        "\n",
        "# Sample input data\n",
        "#X = torch.randn(4, 10)\n",
        "\n",
        "# Forward pass\n",
        "output = gradient_model(X)\n",
        "\n",
        "# Compute gradients\n",
        "grad_w_eps = torch.autograd.grad(output, list(gradient_model.w_eps), create_graph=True)\n",
        "grad_w_eps\n",
        "# Check the gradients\n",
        "#for i, grad in enumerate(grad_w_eps):\n",
        "#    print(f'Gradient for w_eps[{i}]: {grad.norm().item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "dRZ69eF8CSDQ",
        "outputId": "62c4bb12-c91b-46d3-ad4d-578f08fa76ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-124-1162606811d4>:9: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  fmodel, fmodel_params = make_functional(zero_loss_model)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Expected all elements of parameter_and_buffer_dicts to be dictionaries",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-1162606811d4>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_functional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_loss_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mPerturbWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-1162606811d4>\u001b[0m in \u001b[0;36mPerturbWeights\u001b[0;34m(fmodel, w0, w_eps, X_batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperturb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mperturbed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#    perturbed_output = functional_call(model, param_dict, X_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/functional_call.py\u001b[0m in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_and_buffer_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_and_buffer_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;34m\"Expected all elements of parameter_and_buffer_dicts to be dictionaries\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected all elements of parameter_and_buffer_dicts to be dictionaries"
          ]
        }
      ],
      "source": [
        "def PerturbWeights(fmodel, w0, w_eps, X_batch):\n",
        "    param_dict = {name: param + perturb for (name, param), perturb in zip(w0, w_eps)}\n",
        "    #print\n",
        "    perturbed_output = functional_call(fmodel, w0, X_batch)\n",
        "\n",
        "#    perturbed_output = functional_call(model, param_dict, X_batch)\n",
        "    return perturbed_output\n",
        "\n",
        "fmodel, fmodel_params = make_functional(zero_loss_model)\n",
        "\n",
        "PerturbWeights(fmodel, fmodel_params, fmodel_params, X[[1],:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5leOTDiCdDj",
        "outputId": "c7cee53f-e79b-4ae4-c6c0-ce52fef7bf4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[-0.7024,  1.1023],\n",
              "         [ 1.2751, -1.8069]], device='cuda:0', requires_grad=True),)"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fmodel_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgrg4ja2BUaK",
        "outputId": "9db7ac2f-eb98-4a92-db69-34e0da6b2f9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[-0.7024,  1.1023],\n",
              "         [ 1.2751, -1.8069]], device='cuda:0', requires_grad=True),)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnU9IJO7-qZi",
        "outputId": "99896d6d-bf15-4057-c3dc-fba1196e8dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "from torch.func import make_functional\n",
        "from torch.func import make_functional\n",
        "\n",
        "fmodel, params = make_functional(model)\n",
        "\n",
        "def fmodel_forward(params, x):\n",
        "    return fmodel(params, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0kfrvqg8Nvq"
      },
      "outputs": [],
      "source": [
        "a = PerturbWeights(zero_loss_model, X, nn.ParameterList(\n",
        "            [.000000*torch.rand_like(param) for name, param in zero_loss_model.named_parameters()]\n",
        "            )).sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJnD_s04alt",
        "outputId": "7c8841be-8b30-4c74-d52b-4d78e87d89ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0., device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "tensor(0., device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "tensor(0., device='cuda:0', grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 0.],\n",
              "         [0., 0.]], device='cuda:0', grad_fn=<TBackward0>),)"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "#PerturbWeights(gradient_model, X, gradient_model.w_eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gn_mh7H6go6",
        "outputId": "03e30c68-8bf4-4377-ef7a-cb0c863bcdf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0., device='cuda:0', grad_fn=<SubBackward0>)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gradient_model(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6NCLi7e6DWh",
        "outputId": "872ac315-ce6a-4ddf-f886-278ec9bfe202"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 0.],\n",
              "         [0., 0.]], device='cuda:0'),)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#for i in gradient_model.named_parameters(): print(i)\n",
        "\n",
        "# Compute the gradient of gradient_model with respect to w_eps using autograd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAtCqkSQbSaA"
      },
      "source": [
        "# Adding Vector to Weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoQ3ri1cogKH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Assume `model` is your pre-trained model and `perturbations` is a list of tensors\n",
        "# containing small values to add to each parameter in the model.\n",
        "\n",
        "# Store the original state of the model\n",
        "\n",
        "from torch.func import functional_call\n",
        "\n",
        "def PerturbWeights(model, X, perturbation_direction, eps=1e-5, loss_fn=nn.MSELoss(reduction='none')):\n",
        "    # Get the original outputs\n",
        "    orig_output = model(X)  # `X` is your input tensor\n",
        "\n",
        "    # Create perturbed parameters\n",
        "    perturbed_params = {name: param + eps * perturb\n",
        "                        for (name, param), perturb in zip(model.named_parameters(), perturbation_direction)}\n",
        "\n",
        "    # Evaluate the model with perturbed parameters\n",
        "    perturbed_output = functional_call(model, perturbed_params, X)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss_value = loss_fn(perturbed_output, orig_output) / (eps**2)\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PerturbationEstimation(nn.Module):\n",
        "    def __init__(self, model, k, criterion, eps=1e-6):\n",
        "        super(PerturbationEstimation, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.eps = eps\n",
        "        self.criterion = criterion\n",
        "\n",
        "        # Initialize U as a list of k directions\n",
        "        self.U = nn.ModuleList()\n",
        "        for _ in range(k):\n",
        "            # For each direction, create a nn.ParameterList of parameters matching the model's parameters\n",
        "            u_direction = nn.ParameterList()\n",
        "            for param in model.parameters():\n",
        "                u_param = nn.Parameter(torch.randn_like(param))\n",
        "                u_direction.append(u_param)\n",
        "            self.U.append(u_direction)\n",
        "\n",
        "\n",
        "        # Initialize U with Xavier initialization and normalize\n",
        "        for u_direction in self.U:\n",
        "            for u_param in u_direction:\n",
        "                nn.init.xavier_normal_(u_param)\n",
        "                u_param.data /= u_param.data.norm() + 1e-8  # Normalize to prevent division by zero\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "\n",
        "        # Initialize U with Xavier initialization and normalize\n",
        "        for u_direction in self.U:\n",
        "            for u_param in u_direction:\n",
        "                u_param.data /= u_param.data.norm() + 1e-8  # Normalize to prevent division by zero\n",
        "\n",
        "\n",
        "\n",
        "        U = self.U  # [n, k]\n",
        "\n",
        "\n",
        "\n",
        "        gradients = []\n",
        "        for u in U:\n",
        "          gradient = PerturbWeights(self.model,X_batch, u, eps=1e-6)\n",
        "          gradients.append(gradient.flatten())\n",
        "\n",
        "\n",
        "        gradient_stack = torch.stack(gradients)\n",
        "        # Noramlize each row of gradient stack\n",
        "        gradient_stack = gradient_stack #/ (gradient_stack.sum(dim=0,keepdim=True) + 1e-10)\n",
        "        return gradient_stack\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def TrainPerturbationEstimation(\n",
        "    perturb_model, lambda_penalty, dataloader,\n",
        "    learning_rate, num_epochs):\n",
        "\n",
        "    # Collect all parameters from U\n",
        "    all_params = [p for u_direction in perturb_model.U for p in u_direction]\n",
        "\n",
        "    optimizer = torch.optim.Adam(all_params, lr=learning_rate)\n",
        "    perturb_model.train()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_high_hessian_loss = 0\n",
        "        total_sparsity_loss = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        for X_batch, _ in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            k_mat = perturb_model(X_batch)\n",
        "            high_hessian_loss = (k_mat**2).var()\n",
        "            sparsity_loss =  abs(all_params @ all_params.transpose(0,1)).sum()\n",
        "            L = (-1 * high_hessian_loss) + (lambda_penalty * sparsity_loss)\n",
        "\n",
        "            L.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += L.item()\n",
        "            total_high_hessian_loss += high_hessian_loss.item()\n",
        "            total_sparsity_loss += sparsity_loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "\n",
        "\n",
        "        # Print losses\n",
        "        if (epoch + 1) % max(round(num_epochs/10), 1) == 0 or epoch == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                  f\"Total Loss: {total_loss/n_batches:.4f}, \"\n",
        "                  f\"L2 loss: {total_high_hessian_loss/n_batches:.4f}, \"\n",
        "                  f\"L1: {total_sparsity_loss/n_batches:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MiaUobXhsuNc",
        "outputId": "ca258c23-7d9d-470a-df35-eb00c1825656"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'ModuleList' object has no attribute 'transpose'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5f0d1f4e2c72>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                         batch_size=4, shuffle=True, generator=torch.Generator(device='cuda'))\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTrainPerturbationEstimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d67483fabe15>\u001b[0m in \u001b[0;36mTrainPerturbationEstimation\u001b[0;34m(perturb_model, lambda_penalty, dataloader, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mk_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperturb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mhigh_hessian_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk_mat\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0msparsity_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mperturb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhigh_hessian_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlambda_penalty\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ModuleList' object has no attribute 'transpose'"
          ]
        }
      ],
      "source": [
        "perturb_model = PerturbationEstimation(zero_loss_model, 5, nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)),\n",
        "                        batch_size=4, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "TrainPerturbationEstimation(perturb_model, .2, dataloader, learning_rate=.001, num_epochs=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_jrglDJ5Emj",
        "outputId": "a037de50-82ee-4d55-e852-8e6d19c64fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]], device='cuda:0') ->\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5116, 0.5116, 0.3553, 0.3553, 0.5116],\n",
            "        [0.2274, 0.5116, 0.3553, 0.1279, 0.0888],\n",
            "        [0.7729, 0.9381, 0.0000, 0.8535, 0.6004]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for x_, y_ in DataLoader(\n",
        "    TensorDataset(X, Y),\n",
        "    batch_size=4, shuffle=False, generator=torch.Generator(device='cuda')):\n",
        "    print(x_, '->')\n",
        "    print(perturb_model(x_).transpose(0,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBNtcYDXcBhb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#@ Train\n",
        "def total_loss_function(H, H_reconstructed, diag_elements, lambda_penalty=.1):\n",
        "  # Reconstruction Loss\n",
        "  reconstruction_loss = ((H - H_reconstructed) ** 2).mean()\n",
        "\n",
        "  # Penalty Term: L2 norm squared of diag_elements\n",
        "  penalty_term = abs(diag_elements).mean()  # Mean over batch\n",
        "  #print(diag_elements, penalty_term)\n",
        "\n",
        "  # Total Loss\n",
        "  total_loss = reconstruction_loss + lambda_penalty * penalty_term\n",
        "\n",
        "  return total_loss, reconstruction_loss, penalty_term\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(\n",
        "    eigenmodel, lambda_penalty, dataloader, hessian_samples,\n",
        "    learning_rate, num_epochs):\n",
        "  # Hyperparameters\n",
        "\n",
        "  k = eigenmodel.U.shape[1]\n",
        "  n = eigenmodel.U.shape[0] # Dimension of the matrices\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigenmodel.parameters(), lr=learning_rate)\n",
        "  eigenmodel.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    reconstruction_losses = 0\n",
        "    penalty_terms = 0\n",
        "    total_losses = 0\n",
        "    n_batch = 0\n",
        "    for X_superbatch, _ in dataloader:\n",
        "      L = 0\n",
        "\n",
        "      sub_dataloader = DataLoader(TensorDataset(X_superbatch), hessian_samples, shuffle=True,\n",
        "                                  generator=torch.Generator(device='cuda'))\n",
        "\n",
        "      for X_batch in sub_dataloader:\n",
        "        n_batch = n_batch + 1\n",
        "        H, H_reconstructed, diag_elements = eigenmodel(X_batch[0])# + .01 * torch.randn_like(X_batch[0]))\n",
        "\n",
        "        # Compute loss\n",
        "        total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "            H, H_reconstructed, diag_elements, lambda_penalty\n",
        "        )\n",
        "        L = L + total_loss\n",
        "\n",
        "        total_losses = total_losses + total_loss\n",
        "        reconstruction_losses = reconstruction_losses + reconstruction_loss\n",
        "        penalty_terms = penalty_terms + penalty_term\n",
        "\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      L.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % round(num_epochs/100) == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "    if reconstruction_losses < .01:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMAauuzwbU3G"
      },
      "outputs": [],
      "source": [
        "#@title Model\n",
        "\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, k, model, criterion, eps=1e-5):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "\n",
        "        n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(n, k))\n",
        "\n",
        "        # Initialize self.U with xavier\n",
        "        nn.init.xavier_normal_(self.U)\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # A is of shape [batch_size, n, n]\n",
        "        # Compute U^T H U for each matrix in the batch\n",
        "\n",
        "        # Normalize columns of U\n",
        "        #self.U.data = self.U.data / torch.norm(self.U.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        # Normalize columns of V\n",
        "        #self.V.data = self.V.data / torch.norm(self.V.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        U = self.U  # [n, k]\n",
        "        # Add model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model = self.model\n",
        "        criterion = self.criterion\n",
        "        loss = criterion(model(X_batch), model(X_batch).detach())\n",
        "\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "        grads_flat = torch.cat([g.view(-1) for grad in grads for g in grad])\n",
        "\n",
        "        H_list = []\n",
        "        # True Hessian-vector product\n",
        "        for g in grads_flat:\n",
        "          H_i = torch.autograd.grad(g, model.parameters(), retain_graph=True)  # H_s1 ∈ ℝ^{n}\n",
        "          H_i_flat = torch.cat([h.view(-1) for hh in H_i for h in hh])\n",
        "          H_list.append(H_i_flat)\n",
        "        H = torch.stack(H_list)\n",
        "\n",
        "        # Transformed Hessian\n",
        "        UHU = abs(U.transpose(0,1) @ H @ U)\n",
        "\n",
        "        # Get diagonal elements of VHU\n",
        "        diag_UHU = torch.diagonal(UHU)\n",
        "\n",
        "\n",
        "        H_r = V.transpose(0,1) @ UHU @ V\n",
        "\n",
        "        return H, H_r, torch.diagonal(UHU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@ Train\n",
        "def total_loss_function(H, H_reconstructed, diag_elements, lambda_penalty=.1):\n",
        "  # Reconstruction Loss\n",
        "  reconstruction_loss = ((H - H_reconstructed) ** 2).mean()\n",
        "\n",
        "  # Penalty Term: L2 norm squared of diag_elements\n",
        "  penalty_term = abs(diag_elements).mean()  # Mean over batch\n",
        "  #print(diag_elements, penalty_term)\n",
        "\n",
        "  # Total Loss\n",
        "  total_loss = reconstruction_loss + lambda_penalty * penalty_term\n",
        "\n",
        "  return total_loss, reconstruction_loss, penalty_term\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(\n",
        "    eigenmodel, lambda_penalty, dataloader, hessian_samples,\n",
        "    learning_rate, num_epochs):\n",
        "  # Hyperparameters\n",
        "\n",
        "  k = eigenmodel.U.shape[1]\n",
        "  n = eigenmodel.U.shape[0] # Dimension of the matrices\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigenmodel.parameters(), lr=learning_rate)\n",
        "  eigenmodel.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    reconstruction_losses = 0\n",
        "    penalty_terms = 0\n",
        "    total_losses = 0\n",
        "    n_batch = 0\n",
        "    for X_superbatch, _ in dataloader:\n",
        "      L = 0\n",
        "\n",
        "      sub_dataloader = DataLoader(TensorDataset(X_superbatch), hessian_samples, shuffle=True,\n",
        "                                  generator=torch.Generator(device='cuda'))\n",
        "\n",
        "      for X_batch in sub_dataloader:\n",
        "        n_batch = n_batch + 1\n",
        "        H, H_reconstructed, diag_elements = eigenmodel(X_batch[0])# + .01 * torch.randn_like(X_batch[0]))\n",
        "\n",
        "        # Compute loss\n",
        "        total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "            H, H_reconstructed, diag_elements, lambda_penalty\n",
        "        )\n",
        "        L = L + total_loss\n",
        "\n",
        "        total_losses = total_losses + total_loss\n",
        "        reconstruction_losses = reconstruction_losses + reconstruction_loss\n",
        "        penalty_terms = penalty_terms + penalty_term\n",
        "\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      L.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % round(num_epochs/100) == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "    if reconstruction_losses < .01:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejA636cnVC25"
      },
      "source": [
        "# Full Hessian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XL11QGaPheL"
      },
      "outputs": [],
      "source": [
        "#@title Model\n",
        "\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, k, model, criterion):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "\n",
        "        n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(n, k))\n",
        "        self.V = nn.Parameter(torch.randn(k, n))\n",
        "\n",
        "        # Initialize self.U with xavier\n",
        "        nn.init.xavier_normal_(self.U)\n",
        "        nn.init.xavier_normal_(self.V)\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # A is of shape [batch_size, n, n]\n",
        "        # Compute U^T H U for each matrix in the batch\n",
        "\n",
        "        # Normalize columns of U\n",
        "        #self.U.data = self.U.data / torch.norm(self.U.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        # Normalize columns of V\n",
        "        #self.V.data = self.V.data / torch.norm(self.V.data, dim=1, keepdim=True)  # [n, k]\n",
        "\n",
        "        U = self.U  # [n, k]\n",
        "        V = self.V #.transpose(0,1)\n",
        "\n",
        "\n",
        "        model = self.model\n",
        "        criterion = self.criterion\n",
        "        loss = criterion(model(X_batch), model(X_batch).detach())\n",
        "\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "        grads_flat = torch.cat([g.view(-1) for grad in grads for g in grad])\n",
        "\n",
        "        H_list = []\n",
        "        # True Hessian-vector product\n",
        "        for g in grads_flat:\n",
        "          H_i = torch.autograd.grad(g, model.parameters(), retain_graph=True)  # H_s1 ∈ ℝ^{n}\n",
        "          H_i_flat = torch.cat([h.view(-1) for hh in H_i for h in hh])\n",
        "          H_list.append(H_i_flat)\n",
        "        H = torch.stack(H_list)\n",
        "\n",
        "        # Transformed Hessian\n",
        "        UHU = abs(U.transpose(0,1) @ H @ U)\n",
        "\n",
        "        # Get diagonal elements of VHU\n",
        "        diag_UHU = torch.diagonal(UHU)\n",
        "\n",
        "\n",
        "        H_r = V.transpose(0,1) @ UHU @ V\n",
        "\n",
        "        return H, H_r, torch.diagonal(UHU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@ Train\n",
        "def total_loss_function(H, H_reconstructed, diag_elements, lambda_penalty=.1):\n",
        "  # Reconstruction Loss\n",
        "  reconstruction_loss = ((H - H_reconstructed) ** 2).mean()\n",
        "\n",
        "  # Penalty Term: L2 norm squared of diag_elements\n",
        "  penalty_term = abs(diag_elements).mean()  # Mean over batch\n",
        "  #print(diag_elements, penalty_term)\n",
        "\n",
        "  # Total Loss\n",
        "  total_loss = reconstruction_loss + lambda_penalty * penalty_term\n",
        "\n",
        "  return total_loss, reconstruction_loss, penalty_term\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(\n",
        "    eigenmodel, lambda_penalty, dataloader, hessian_samples,\n",
        "    learning_rate, num_epochs):\n",
        "  # Hyperparameters\n",
        "\n",
        "  k = eigenmodel.U.shape[1]\n",
        "  n = eigenmodel.U.shape[0] # Dimension of the matrices\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigenmodel.parameters(), lr=learning_rate)\n",
        "  eigenmodel.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    reconstruction_losses = 0\n",
        "    penalty_terms = 0\n",
        "    total_losses = 0\n",
        "    n_batch = 0\n",
        "    for X_superbatch, _ in dataloader:\n",
        "      L = 0\n",
        "\n",
        "      sub_dataloader = DataLoader(TensorDataset(X_superbatch), hessian_samples, shuffle=True,\n",
        "                                  generator=torch.Generator(device='cuda'))\n",
        "\n",
        "      for X_batch in sub_dataloader:\n",
        "        n_batch = n_batch + 1\n",
        "        H, H_reconstructed, diag_elements = eigenmodel(X_batch[0])# + .01 * torch.randn_like(X_batch[0]))\n",
        "\n",
        "        # Compute loss\n",
        "        total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "            H, H_reconstructed, diag_elements, lambda_penalty\n",
        "        )\n",
        "        L = L + total_loss\n",
        "\n",
        "        total_losses = total_losses + total_loss\n",
        "        reconstruction_losses = reconstruction_losses + reconstruction_loss\n",
        "        penalty_terms = penalty_terms + penalty_term\n",
        "\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      L.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % round(num_epochs/100) == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "              f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "    if reconstruction_losses < .0001:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total mean total_loss: {total_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Reconstruction Loss: {reconstruction_losses.item()/n_batch:.2f}, \"\n",
        "          f\"Penalty Term: {penalty_terms.item()/n_batch:.2f}\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGocb9MPJ-Hh"
      },
      "source": [
        "## Some tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KvtOxP_OQ6Ig",
        "outputId": "93022b50-2062-4414-876b-9be05cb4778c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Total mean total_loss: 0.13, Reconstruction Loss: 0.13, Penalty Term: 0.42\n",
            "Epoch [10/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.37\n",
            "Epoch [20/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.36\n",
            "Epoch [30/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.35\n",
            "Epoch [40/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.35\n",
            "Epoch [50/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.35\n",
            "Epoch [60/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.35\n",
            "Epoch [70/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.35\n",
            "Epoch [80/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.36\n",
            "Epoch [90/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.36\n",
            "Epoch [100/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.37\n",
            "Epoch [110/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.38\n",
            "Epoch [120/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.39\n",
            "Epoch [130/1000], Total mean total_loss: 0.04, Reconstruction Loss: 0.04, Penalty Term: 0.40\n",
            "Epoch [140/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.41\n",
            "Epoch [150/1000], Total mean total_loss: 0.04, Reconstruction Loss: 0.04, Penalty Term: 0.42\n",
            "Epoch [160/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.43\n",
            "Epoch [170/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.43\n",
            "Epoch [180/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.44\n",
            "Epoch [190/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.44\n",
            "Epoch [200/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.45\n",
            "Epoch [210/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.45\n",
            "Epoch [220/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.46\n",
            "Epoch [230/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.46\n",
            "Epoch [240/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.46\n",
            "Epoch [250/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.46\n",
            "Epoch [260/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.46\n",
            "Epoch [270/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.46\n",
            "Epoch [280/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.46\n",
            "Epoch [290/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.46\n",
            "Epoch [300/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.47\n",
            "Epoch [310/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.47\n",
            "Epoch [320/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.47\n",
            "Epoch [330/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.47\n",
            "Epoch [340/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.47\n",
            "Epoch [350/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.47\n",
            "Epoch [360/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.47\n",
            "Epoch [370/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.47\n",
            "Epoch [380/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.48\n",
            "Epoch [390/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.48\n",
            "Epoch [400/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.48\n",
            "Epoch [410/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.48\n",
            "Epoch [420/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.49\n",
            "Epoch [430/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.49\n",
            "Epoch [440/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.49\n",
            "Epoch [450/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.50\n",
            "Epoch [460/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.50\n",
            "Epoch [470/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.51\n",
            "Epoch [480/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.51\n",
            "Epoch [490/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.51\n",
            "Epoch [500/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.52\n",
            "Epoch [510/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.52\n",
            "Epoch [520/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.52\n",
            "Epoch [530/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.53\n",
            "Epoch [540/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.53\n",
            "Epoch [550/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.53\n",
            "Epoch [560/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.53\n",
            "Epoch [570/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.53\n",
            "Epoch [580/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [590/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [600/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [610/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [620/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [630/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [640/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [650/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [660/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [670/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.54\n",
            "Epoch [680/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [690/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [700/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [710/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [720/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [730/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [740/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [750/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [760/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [770/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [780/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [790/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [800/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [810/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [820/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [830/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [840/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [850/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [860/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [870/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [880/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n",
            "Epoch [886/1000], Total mean total_loss: 0.00, Reconstruction Loss: 0.00, Penalty Term: 0.55\n"
          ]
        }
      ],
      "source": [
        "#@title Same numnber of parameters, 0 sparsity.\n",
        "eigenmodel = EigenEstimation(4, zero_loss_model, nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)),\n",
        "                        batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "TrainEigenestimation(\n",
        "    eigenmodel, dataloader=dataloader, hessian_samples = 4,\n",
        "    lambda_penalty=0, learning_rate=.01, num_epochs=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5V0N-HIkk8u",
        "outputId": "9b012b19-b860-4908-be87-a54cecfcd5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3054,  0.9218, -0.6874, -0.2352],\n",
            "        [ 0.4070, -0.5366, -0.6183, -0.9631],\n",
            "        [-0.0799,  0.2264, -0.0362, -0.2087],\n",
            "        [-0.8878, -0.4217,  0.2710,  0.5448]], device='cuda:0',\n",
            "       requires_grad=True) Parameter containing:\n",
            "tensor([[ 0.5296,  0.1321,  0.6968, -0.6676],\n",
            "        [-0.7195,  0.7839, -0.0750,  0.5463],\n",
            "        [-0.7789, -1.1151,  0.2912, -0.1511],\n",
            "        [ 0.0551,  0.0878, -1.1217, -0.8270]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "----- \n",
            "sample [[1. 1.]]\n",
            "loss (tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>), tensor(0.3674, device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[0.67 0.67 0.52 0.52]\n",
            " [0.67 0.67 0.52 0.52]\n",
            " [0.52 0.52 0.4  0.4 ]\n",
            " [0.52 0.52 0.4  0.4 ]]\n",
            "[[0.63 0.64 0.46 0.55]\n",
            " [0.64 0.65 0.47 0.55]\n",
            " [0.46 0.47 0.34 0.4 ]\n",
            " [0.55 0.55 0.4  0.48]]\n",
            "[0.   0.04 0.84 0.59]\n",
            "----- \n",
            "sample [[0. 0.]]\n",
            "loss (tensor(0., device='cuda:0', grad_fn=<AddBackward0>), tensor(0., device='cuda:0', grad_fn=<MeanBackward0>), tensor(0., device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[0. 0. 0. 0.]\n",
            "----- \n",
            "sample [[1. 0.]]\n",
            "loss (tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.3896e-06, device='cuda:0', grad_fn=<MeanBackward0>), tensor(0.8894, device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[ 2.52  0.   -0.29 -0.  ]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.29 -0.    0.03  0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n",
            "[[ 2.51 -0.   -0.29 -0.  ]\n",
            " [-0.    0.    0.    0.  ]\n",
            " [-0.29  0.    0.03  0.  ]\n",
            " [-0.    0.    0.    0.  ]]\n",
            "[0.25 2.02 1.18 0.11]\n",
            "----- \n",
            "sample [[0. 1.]]\n",
            "loss (tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.4364e-05, device='cuda:0', grad_fn=<MeanBackward0>), tensor(0.9570, device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[ 0.    0.    0.    0.  ]\n",
            " [ 0.    0.02 -0.   -0.25]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.   -0.25  0.    2.53]]\n",
            "[[ 0.    0.   -0.   -0.01]\n",
            " [ 0.    0.02 -0.   -0.25]\n",
            " [-0.   -0.    0.    0.  ]\n",
            " [-0.01 -0.25  0.    2.51]]\n",
            "[2.17 0.34 0.28 1.03]\n"
          ]
        }
      ],
      "source": [
        "print(eigenmodel.U, eigenmodel.V)\n",
        "\n",
        "for X_batch, _ in DataLoader(TensorDataset(X, Y),\n",
        "           batch_size=1, shuffle=True, generator=torch.Generator(device='cuda')):\n",
        "  print('-----', '\\nsample', X_batch.cpu().numpy())\n",
        "  H, H_r, diag = eigenmodel(X_batch)\n",
        "  print('loss', total_loss_function(H, H_r, diag, lambda_penalty=.1))\n",
        "  print(H.cpu().detach().numpy().round(2))\n",
        "\n",
        "  print(H_r.cpu().detach().numpy().round(2))\n",
        "  print(diag.cpu().detach().numpy().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "UKyufLiJKbAk",
        "outputId": "14c0881c-9c2c-4fd0-99cf-5531c1f04c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Total mean total_loss: 0.16, Reconstruction Loss: 0.16, Penalty Term: 0.63\n",
            "Epoch [10/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.58\n",
            "Epoch [20/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.58\n",
            "Epoch [30/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [40/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [50/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [60/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.58\n",
            "Epoch [70/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [80/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [90/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.58\n",
            "Epoch [100/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.58\n",
            "Epoch [110/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.58\n",
            "Epoch [120/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [130/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [140/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.58\n",
            "Epoch [150/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.58\n",
            "Epoch [160/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.59\n",
            "Epoch [170/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.59\n",
            "Epoch [180/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.59\n",
            "Epoch [190/1000], Total mean total_loss: 0.06, Reconstruction Loss: 0.06, Penalty Term: 0.60\n",
            "Epoch [200/1000], Total mean total_loss: 0.07, Reconstruction Loss: 0.07, Penalty Term: 0.60\n",
            "Epoch [210/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.61\n",
            "Epoch [220/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.62\n",
            "Epoch [230/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.63\n",
            "Epoch [240/1000], Total mean total_loss: 0.04, Reconstruction Loss: 0.04, Penalty Term: 0.63\n",
            "Epoch [250/1000], Total mean total_loss: 0.05, Reconstruction Loss: 0.05, Penalty Term: 0.64\n",
            "Epoch [260/1000], Total mean total_loss: 0.04, Reconstruction Loss: 0.04, Penalty Term: 0.65\n",
            "Epoch [270/1000], Total mean total_loss: 0.04, Reconstruction Loss: 0.04, Penalty Term: 0.66\n",
            "Epoch [280/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.67\n",
            "Epoch [290/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.69\n",
            "Epoch [300/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.70\n",
            "Epoch [310/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.70\n",
            "Epoch [320/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.72\n",
            "Epoch [330/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.73\n",
            "Epoch [340/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.74\n",
            "Epoch [350/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.75\n",
            "Epoch [360/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.75\n",
            "Epoch [370/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.76\n",
            "Epoch [380/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.77\n",
            "Epoch [390/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.03, Penalty Term: 0.77\n",
            "Epoch [400/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.78\n",
            "Epoch [410/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.78\n",
            "Epoch [420/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.79\n",
            "Epoch [430/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.79\n",
            "Epoch [440/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.80\n",
            "Epoch [450/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.80\n",
            "Epoch [460/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.81\n",
            "Epoch [470/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.81\n",
            "Epoch [480/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.81\n",
            "Epoch [490/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.82\n",
            "Epoch [500/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.82\n",
            "Epoch [510/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.82\n",
            "Epoch [520/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.83\n",
            "Epoch [530/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.83\n",
            "Epoch [540/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.83\n",
            "Epoch [550/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.83\n",
            "Epoch [560/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.84\n",
            "Epoch [570/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.84\n",
            "Epoch [580/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.84\n",
            "Epoch [590/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.84\n",
            "Epoch [600/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.84\n",
            "Epoch [610/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.85\n",
            "Epoch [620/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.85\n",
            "Epoch [630/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.85\n",
            "Epoch [640/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.85\n",
            "Epoch [650/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.85\n",
            "Epoch [660/1000], Total mean total_loss: 0.02, Reconstruction Loss: 0.02, Penalty Term: 0.85\n",
            "Epoch [670/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.85\n",
            "Epoch [680/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [690/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [700/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [710/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [720/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [730/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [740/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.86\n",
            "Epoch [750/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [760/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [770/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [780/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [790/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [800/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [810/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.87\n",
            "Epoch [820/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [830/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [840/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [850/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [860/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [870/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [880/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [890/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [900/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [910/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.88\n",
            "Epoch [920/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.89\n",
            "Epoch [930/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.89\n",
            "Epoch [940/1000], Total mean total_loss: 0.01, Reconstruction Loss: 0.01, Penalty Term: 0.89\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-28ed8dbbea30>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)),\n\u001b[1;32m      4\u001b[0m                         batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n\u001b[0;32m----> 5\u001b[0;31m TrainEigenestimation(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0meigenmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     lambda_penalty=0, learning_rate=.01, num_epochs=1000)\n",
            "\u001b[0;32m<ipython-input-54-020ce802b706>\u001b[0m in \u001b[0;36mTrainEigenestimation\u001b[0;34m(eigenmodel, lambda_penalty, dataloader, hessian_samples, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \"\"\"\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title 3 parameters (rank of H), 0 sparsity.\n",
        "eigenmodel = EigenEstimation(3, zero_loss_model, nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)),\n",
        "                        batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "TrainEigenestimation(\n",
        "    eigenmodel, dataloader=dataloader, hessian_samples = 4,\n",
        "    lambda_penalty=0, learning_rate=.01, num_epochs=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pd4WElM9pgf",
        "outputId": "ba5373a9-0a7e-4205-f7bb-5aa6a4b0bc9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.9134,  0.2151,  0.9125],\n",
            "        [-1.2489,  0.0213,  0.5229],\n",
            "        [ 0.2781,  0.2804, -0.7165],\n",
            "        [-0.1143,  0.6276,  0.4683]], device='cuda:0', requires_grad=True) Parameter containing:\n",
            "tensor([[-0.3400, -0.2996, -0.7191,  0.3661],\n",
            "        [ 0.5058, -0.0479, -0.6262, -1.5563],\n",
            "        [-0.7321,  0.1143,  0.9512, -0.0541]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "----- \n",
            "sample [[1. 1.]]\n",
            "loss (tensor(0.1917, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0451, device='cuda:0', grad_fn=<MeanBackward0>), tensor(1.4659, device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[0.67 0.67 0.52 0.52]\n",
            " [0.67 0.67 0.52 0.52]\n",
            " [0.52 0.52 0.4  0.4 ]\n",
            " [0.52 0.52 0.4  0.4 ]]\n",
            "[[0.85 0.39 0.66 0.59]\n",
            " [0.39 0.18 0.3  0.27]\n",
            " [0.66 0.3  0.51 0.46]\n",
            " [0.59 0.27 0.46 0.42]]\n",
            "[2.77 0.59 1.03]\n",
            "----- \n",
            "sample [[0. 0.]]\n",
            "loss (tensor(0., device='cuda:0', grad_fn=<AddBackward0>), tensor(0., device='cuda:0', grad_fn=<MeanBackward0>), tensor(0., device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[0. 0. 0.]\n",
            "----- \n",
            "sample [[1. 0.]]\n",
            "loss (tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>), tensor(1.6058, device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[ 2.52  0.   -0.29 -0.  ]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.29 -0.    0.03  0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n",
            "[[ 2.3   0.43 -0.36 -0.02]\n",
            " [ 0.43  0.08 -0.07 -0.  ]\n",
            " [-0.36 -0.07  0.06  0.  ]\n",
            " [-0.02 -0.    0.    0.  ]]\n",
            "[2.25 0.08 2.49]\n",
            "----- \n",
            "sample [[0. 1.]]\n",
            "loss (tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>), tensor(0.4757, device='cuda:0', grad_fn=<MeanBackward0>))\n",
            "[[ 0.    0.    0.    0.  ]\n",
            " [ 0.    0.02 -0.   -0.25]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.   -0.25  0.    2.53]]\n",
            "[[ 0.    0.   -0.   -0.02]\n",
            " [ 0.    0.   -0.   -0.04]\n",
            " [-0.   -0.    0.    0.  ]\n",
            " [-0.02 -0.04  0.    2.49]]\n",
            "[0.   0.99 0.44]\n"
          ]
        }
      ],
      "source": [
        "print(eigenmodel.U, eigenmodel.V)\n",
        "\n",
        "for X_batch, _ in DataLoader(TensorDataset(X, Y),\n",
        "           batch_size=1, shuffle=True, generator=torch.Generator(device='cuda')):\n",
        "  print('-----', '\\nsample', X_batch.cpu().numpy())\n",
        "  H, H_r, diag = eigenmodel(X_batch)\n",
        "  print('loss', total_loss_function(H, H_r, diag, lambda_penalty=.1))\n",
        "  print(H.cpu().detach().numpy().round(2))\n",
        "\n",
        "  print(H_r.cpu().detach().numpy().round(2))\n",
        "  print(diag.cpu().detach().numpy().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DCMQKqpQnzxM",
        "outputId": "cfff7ed8-b1d9-455a-a98a-70ec27e6ae39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Total mean total_loss: 0.22, Reconstruction Loss: 0.22, Penalty Term: 0.57\n",
            "Epoch [10/1000], Total mean total_loss: 0.15, Reconstruction Loss: 0.14, Penalty Term: 0.54\n",
            "Epoch [20/1000], Total mean total_loss: 0.16, Reconstruction Loss: 0.15, Penalty Term: 0.52\n",
            "Epoch [30/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.51\n",
            "Epoch [40/1000], Total mean total_loss: 0.14, Reconstruction Loss: 0.13, Penalty Term: 0.49\n",
            "Epoch [50/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.49\n",
            "Epoch [60/1000], Total mean total_loss: 0.15, Reconstruction Loss: 0.14, Penalty Term: 0.48\n",
            "Epoch [70/1000], Total mean total_loss: 0.13, Reconstruction Loss: 0.12, Penalty Term: 0.47\n",
            "Epoch [80/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.47\n",
            "Epoch [90/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.11, Penalty Term: 0.47\n",
            "Epoch [100/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.10, Penalty Term: 0.46\n",
            "Epoch [110/1000], Total mean total_loss: 0.13, Reconstruction Loss: 0.13, Penalty Term: 0.46\n",
            "Epoch [120/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.10, Penalty Term: 0.46\n",
            "Epoch [130/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.11, Penalty Term: 0.46\n",
            "Epoch [140/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.46\n",
            "Epoch [150/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.10, Penalty Term: 0.45\n",
            "Epoch [160/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.11, Penalty Term: 0.45\n",
            "Epoch [170/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.10, Penalty Term: 0.45\n",
            "Epoch [180/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.10, Penalty Term: 0.45\n",
            "Epoch [190/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.11, Penalty Term: 0.45\n",
            "Epoch [200/1000], Total mean total_loss: 0.14, Reconstruction Loss: 0.14, Penalty Term: 0.45\n",
            "Epoch [210/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.10, Penalty Term: 0.45\n",
            "Epoch [220/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.45\n",
            "Epoch [230/1000], Total mean total_loss: 0.13, Reconstruction Loss: 0.13, Penalty Term: 0.45\n",
            "Epoch [240/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.11, Penalty Term: 0.45\n",
            "Epoch [250/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.45\n",
            "Epoch [260/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.11, Penalty Term: 0.45\n",
            "Epoch [270/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.11, Penalty Term: 0.45\n",
            "Epoch [280/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.08, Penalty Term: 0.45\n",
            "Epoch [290/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.09, Penalty Term: 0.45\n",
            "Epoch [300/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.10, Penalty Term: 0.45\n",
            "Epoch [310/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.09, Penalty Term: 0.46\n",
            "Epoch [320/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.09, Penalty Term: 0.46\n",
            "Epoch [330/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.46\n",
            "Epoch [340/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.10, Penalty Term: 0.46\n",
            "Epoch [350/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.10, Penalty Term: 0.46\n",
            "Epoch [360/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.08, Penalty Term: 0.46\n",
            "Epoch [370/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.11, Penalty Term: 0.46\n",
            "Epoch [380/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.09, Penalty Term: 0.46\n",
            "Epoch [390/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.11, Penalty Term: 0.46\n",
            "Epoch [400/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.07, Penalty Term: 0.46\n",
            "Epoch [410/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.08, Penalty Term: 0.46\n",
            "Epoch [420/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.46\n",
            "Epoch [430/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.07, Penalty Term: 0.46\n",
            "Epoch [440/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.11, Penalty Term: 0.46\n",
            "Epoch [450/1000], Total mean total_loss: 0.12, Reconstruction Loss: 0.12, Penalty Term: 0.46\n",
            "Epoch [460/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.09, Penalty Term: 0.46\n",
            "Epoch [470/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.10, Penalty Term: 0.46\n",
            "Epoch [480/1000], Total mean total_loss: 0.11, Reconstruction Loss: 0.11, Penalty Term: 0.46\n",
            "Epoch [490/1000], Total mean total_loss: 0.09, Reconstruction Loss: 0.09, Penalty Term: 0.46\n",
            "Epoch [500/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.07, Penalty Term: 0.46\n",
            "Epoch [510/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.46\n",
            "Epoch [520/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.08, Penalty Term: 0.46\n",
            "Epoch [530/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.09, Penalty Term: 0.46\n",
            "Epoch [540/1000], Total mean total_loss: 0.10, Reconstruction Loss: 0.09, Penalty Term: 0.46\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-729eac4d64f8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)),\n\u001b[1;32m      4\u001b[0m                         batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n\u001b[0;32m----> 5\u001b[0;31m TrainEigenestimation(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0meigenmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     lambda_penalty=.01, learning_rate=.001, num_epochs=1000)\n",
            "\u001b[0;32m<ipython-input-54-020ce802b706>\u001b[0m in \u001b[0;36mTrainEigenestimation\u001b[0;34m(eigenmodel, lambda_penalty, dataloader, hessian_samples, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_reconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigenmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# + .01 * torch.randn_like(X_batch[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-020ce802b706>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Transformed Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mUHU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Get diagonal elements of VHU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title 3 parameters (rank of H), .1 sparsity.\n",
        "eigenmodel = EigenEstimation(3, zero_loss_model, nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)),\n",
        "                        batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "TrainEigenestimation(\n",
        "    eigenmodel, dataloader=dataloader, hessian_samples = 4,\n",
        "    lambda_penalty=.01, learning_rate=.001, num_epochs=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBDEuMQOK4OQ",
        "outputId": "e031cf44-c4d4-4c30-c7e9-e2594b55c0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0540,  0.4627,  0.7007],\n",
            "        [-0.3748,  0.0718, -0.1068],\n",
            "        [ 0.1991, -0.1161,  0.5900],\n",
            "        [-1.0060, -0.0070, -0.3055]], device='cuda:0', requires_grad=True) Parameter containing:\n",
            "tensor([[-0.6125, -0.0417,  0.2098,  0.0033],\n",
            "        [-0.3033, -0.5102, -0.0684,  0.5683],\n",
            "        [ 1.1168,  0.9540,  0.1320,  0.3752]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "----- \n",
            "sample [[1. 1.]]\n",
            "[[0.67 0.67 0.52 0.52]\n",
            " [0.67 0.67 0.52 0.52]\n",
            " [0.52 0.52 0.4  0.4 ]\n",
            " [0.52 0.52 0.4  0.4 ]]\n",
            "[[0.01 0.04 0.03 0.05]\n",
            " [0.04 0.17 0.1  0.19]\n",
            " [0.03 0.1  0.06 0.11]\n",
            " [0.05 0.19 0.11 0.21]]\n",
            "[0.75 0.13 0.44]\n",
            "----- \n",
            "sample [[0. 0.]]\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[0. 0. 0.]\n",
            "----- \n",
            "sample [[1. 0.]]\n",
            "[[ 2.52  0.   -0.29 -0.  ]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.29 -0.    0.03  0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n",
            "[[0.67 0.47 0.09 0.66]\n",
            " [0.47 0.32 0.06 0.46]\n",
            " [0.09 0.06 0.01 0.09]\n",
            " [0.66 0.46 0.09 0.65]]\n",
            "[0.01 0.57 1.01]\n",
            "----- \n",
            "sample [[0. 1.]]\n",
            "[[ 0.    0.    0.    0.  ]\n",
            " [ 0.    0.02 -0.   -0.25]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.   -0.25  0.    2.53]]\n",
            "[[ 0.18 -0.16 -0.16 -0.08]\n",
            " [-0.16  0.14  0.14  0.07]\n",
            " [-0.16  0.14  0.15  0.07]\n",
            " [-0.08  0.07  0.07  0.04]]\n",
            "[2.37 0.   0.22]\n"
          ]
        }
      ],
      "source": [
        "print(eigenmodel.U, eigenmodel.V)\n",
        "\n",
        "for X_batch, _ in DataLoader(TensorDataset(X, Y),\n",
        "           batch_size=1, shuffle=True, generator=torch.Generator(device='cuda')):\n",
        "  print('-----', '\\nsample', X_batch.cpu().numpy())\n",
        "  H, H_r, diag = eigenmodel(X_batch)\n",
        "  print(H.cpu().detach().numpy().round(2))\n",
        "\n",
        "  print(H_r.cpu().detach().numpy().round(2))\n",
        "  print(diag.cpu().detach().numpy().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqmOo0yaRT20"
      },
      "source": [
        "## TMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDdFX2MpRGN4"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: reduces dimensionality\n",
        "        self.encoder = nn.Linear(num_features, hidden_dim, bias=False)\n",
        "        # Decoder: reconstructs the input\n",
        "        self.decoder = nn.Linear(hidden_dim, num_features, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "def TrainTMS(model, criterion, optimizer, dataloader, n_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        inputs = batch[0]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "    avg_loss = total_loss / num_data_points\n",
        "    if epoch %10 == 0:\n",
        "      print(f'Epoch [{epoch}/], Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "jZGH33_JRQ5H",
        "outputId": "e2c04a59-c111-4727-99be-6afd14ea193e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/], Loss: 0.5375\n",
            "Epoch [10/], Loss: 0.3266\n",
            "Epoch [20/], Loss: 0.3260\n",
            "Epoch [30/], Loss: 0.3253\n",
            "Epoch [40/], Loss: 0.3248\n",
            "Epoch [50/], Loss: 0.3242\n",
            "Epoch [60/], Loss: 0.3235\n",
            "Epoch [70/], Loss: 0.3226\n",
            "Epoch [80/], Loss: 0.3217\n",
            "Epoch [90/], Loss: 0.3210\n",
            "Epoch [100/], Loss: 0.3205\n",
            "Epoch [110/], Loss: 0.3202\n",
            "Epoch [120/], Loss: 0.3200\n",
            "Epoch [130/], Loss: 0.3200\n",
            "Epoch [140/], Loss: 0.3199\n",
            "Epoch [150/], Loss: 0.3199\n",
            "Epoch [160/], Loss: 0.3199\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-b81db133d573>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mTrainTMS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-158-1cdbfc3b0928>\u001b[0m in \u001b[0;36mTrainTMS\u001b[0;34m(model, criterion, optimizer, dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Model\n",
        "# Data parameters\n",
        "num_features = 5       # Number of unique features\n",
        "feature_sparsity = 0.1    # Proportion of non-zero elements in each feature vector\n",
        "num_data_points = 10000   # Total number of data points\n",
        "\n",
        "# Model parameters\n",
        "hidden_dim = 2           # Dimension of the hidden layer (bottleneck)\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 200\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "# Initialize feature vectors\n",
        "feature_vectors = np.random.randn(num_data_points, num_features)\n",
        "feature_vectors = feature_vectors * (np.random.randn(num_data_points, num_features) < feature_sparsity)\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "data_inputs = np.array(feature_vectors)\n",
        "data_labels = np.array(feature_vectors)\n",
        "\n",
        "\n",
        "autoencoder = Autoencoder(num_features, hidden_dim)\n",
        "\n",
        "X = torch.Tensor(data_inputs).to('cuda')\n",
        "Y = torch.Tensor(data_labels).to('cuda')\n",
        "\n",
        "dataloader = DataLoader(TensorDataset(X, Y), batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "TrainTMS(autoencoder, nn.MSELoss(), optimizer, dataloader, n_epochs=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "Z_5_OigpV8ij",
        "outputId": "0e4a092f-b30f-42c4-dfea-666760f439c2"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "inconsistent tensor size, expected tensor [10] and src [20] to have the same number of elements, but got 10 and 20 elements respectively",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-8931d97c3c23>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meigen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEigenEstimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mTrainEigenestimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigen_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#eigen_model = EigenEstimation(k, zero_loss_model, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-151-d7d864b09616>\u001b[0m in \u001b[0;36mTrainEigenestimation\u001b[0;34m(eigen_model, learning_rate, num_epochs, lambda_penalty, dataloader, criterion)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0mH_vs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_prods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigen_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m       \u001b[0mhigh_hessian_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH_vs\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0morthogonality_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_prods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-151-d7d864b09616>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0;31m# Hessian-vector product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m           \u001b[0mH_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0mHv_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [10] and src [20] to have the same number of elements, but got 10 and 20 elements respectively"
          ]
        }
      ],
      "source": [
        "eigen_model = EigenEstimation(5, autoencoder,  nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X, Y), batch_size=2, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "TrainEigenestimation(eigen_model, .001, 1000, 1,  dataloader=dataloader, criterion=nn.MSELoss())\n",
        "\n",
        "#eigen_model = EigenEstimation(k, zero_loss_model, criterion)\n",
        "dataloader = DataLoader(TensorDataset(X, Y),\n",
        "                        batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "print(eigen_model.U)\n",
        "for x,_ in dataloader:\n",
        "  H, _ = eigen_model(x)\n",
        "  print(x, H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Ud7q_ROyR9YZ",
        "outputId": "c5566740-a6ad-4b5a-f2ff-b4d3183ec132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Total mean total_loss: 0.08, Reconstruction Loss: 0.04, Penalty Term: 0.40\n",
            "Epoch [10/1000], Total mean total_loss: 0.03, Reconstruction Loss: 0.02, Penalty Term: 0.15\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-368b13b0aa93>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                         batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m TrainEigenestimation(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0meigenmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     lambda_penalty=.1, learning_rate=.01, num_epochs=1000)\n",
            "\u001b[0;32m<ipython-input-54-020ce802b706>\u001b[0m in \u001b[0;36mTrainEigenestimation\u001b[0;34m(eigenmodel, lambda_penalty, dataloader, hessian_samples, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_reconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigenmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# + .01 * torch.randn_like(X_batch[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-020ce802b706>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# True Hessian-vector product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m           \u001b[0mH_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# H_s1 ∈ ℝ^{n}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m           \u001b[0mH_i_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH_i\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mH_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_i_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0moverridable_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverridable_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         return handle_torch_function(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0moverridable_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    434\u001b[0m         )\n\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title 5 parameters (rank of H), .01 sparsity.\n",
        "eigenmodel = EigenEstimation(5, autoencoder, nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(X, Y),\n",
        "                        batch_size=16, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "TrainEigenestimation(\n",
        "    eigenmodel, dataloader=dataloader, hessian_samples = 4,\n",
        "    lambda_penalty=.1, learning_rate=.01, num_epochs=1000)\n",
        "\n",
        "for X_batch, _ in DataLoader(TensorDataset(X[:10], Y[:10]),\n",
        "           batch_size=1, shuffle=True, generator=torch.Generator(device='cuda')):\n",
        "    print(X_batch, eigenmodel(X_batch)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRaUMrowU19s",
        "outputId": "60f6efde-657d-4512-e2b6-f5c71b6384a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3730, -0.1318,  0.1710,  0.0796,  0.0595],\n",
            "        [ 0.1385, -0.1128, -0.0637, -0.3266,  0.2329],\n",
            "        [-0.0276,  0.2294,  0.1672, -0.0011,  0.3512],\n",
            "        [ 0.2205,  0.0445,  0.0323, -0.1483,  0.0417],\n",
            "        [-0.0623,  0.1128, -0.1724,  0.0544, -0.3555],\n",
            "        [-0.2653,  0.0354, -0.1947,  0.1489, -0.1757],\n",
            "        [-0.1417, -0.1948, -0.4158, -0.3539,  0.3050],\n",
            "        [-0.2612, -0.2383,  0.3923, -0.1724, -0.4092],\n",
            "        [-0.1710, -0.0538,  0.0740,  0.3083, -0.2742],\n",
            "        [ 0.0908,  0.0367,  0.0853,  0.0757,  0.0159],\n",
            "        [-0.1792, -0.1689, -0.4134, -0.0027, -0.3678],\n",
            "        [ 0.0095,  0.1218, -0.2800,  0.1017, -0.0273],\n",
            "        [-0.1683,  0.0312,  0.0566,  0.0946, -0.2122],\n",
            "        [-0.3407, -0.0411, -0.1251,  0.0422, -0.1775],\n",
            "        [ 0.4106,  0.0290,  0.1725,  0.2702, -0.0504],\n",
            "        [ 0.1868,  0.3221,  0.0051,  0.1976, -0.0613],\n",
            "        [-0.5837,  0.0959, -0.0047,  0.3979,  0.0273],\n",
            "        [-0.1923,  0.1820, -0.4635,  0.3667,  0.2863],\n",
            "        [-0.0526, -0.2091,  0.1869,  0.4487, -0.4212],\n",
            "        [ 0.2068, -0.2154, -0.0691,  0.2149, -0.0688]], device='cuda:0',\n",
            "       requires_grad=True) Parameter containing:\n",
            "tensor([[-6.4260e-02, -1.8545e-01,  2.5203e-01, -4.8474e-02,  2.9493e-01,\n",
            "         -3.0457e-02, -1.7512e-01, -1.3313e-01,  2.8734e-01,  5.9968e-02,\n",
            "          1.9660e-01, -5.8990e-02,  1.6948e-01, -5.6353e-02,  4.2499e-01,\n",
            "          4.9318e-02,  2.0273e-01, -1.8211e-01, -3.4028e-02, -3.8672e-03],\n",
            "        [-2.1041e-01, -1.2179e-01,  4.5799e-02,  1.3776e-01, -1.9406e-01,\n",
            "          3.8081e-02, -1.5266e-01,  1.9637e-01, -7.2831e-04,  2.5322e-01,\n",
            "         -1.9910e-01, -7.7396e-02, -3.1799e-03,  1.1660e-02,  7.3792e-01,\n",
            "          1.4479e-02, -1.3327e-01, -2.1518e-02, -1.5951e-01, -2.2123e-01],\n",
            "        [ 6.3866e-02, -7.9100e-01,  6.7591e-01,  8.2594e-02,  4.4184e-02,\n",
            "          2.1525e-02, -1.9936e-01, -3.8265e-01, -1.4204e-01, -4.5220e-01,\n",
            "          3.7959e-01,  3.8177e-01,  4.0745e-03, -3.4320e-02, -1.5176e-01,\n",
            "         -8.9050e-02, -1.0872e-02,  2.5487e-02, -3.3237e-02,  1.1828e-01],\n",
            "        [ 1.8468e-02,  2.0893e-01, -3.0870e-01,  2.2152e-01,  2.1730e-02,\n",
            "         -3.5712e-01,  3.8272e-02, -3.3167e-01, -2.5330e-01,  4.2649e-01,\n",
            "          1.1584e-02, -1.7189e-01, -3.5201e-01,  1.8650e-01, -1.7172e-01,\n",
            "          2.4636e-01, -3.5273e-01, -1.8538e-01, -2.6660e-01, -7.1695e-02],\n",
            "        [ 3.3890e-01,  1.8409e-01,  7.5740e-02, -1.6873e-01, -3.5745e-01,\n",
            "          1.5463e-01,  4.3280e-01, -2.4885e-01, -2.1138e-01, -3.6109e-01,\n",
            "          6.5821e-01,  1.7089e-01,  4.2672e-01,  1.3381e-01,  6.1554e-03,\n",
            "          2.3748e-01, -1.0792e-01,  8.6295e-02,  1.9151e-01, -2.3464e-01]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "----- \n",
            "sample [[ 0.    0.36  0.63  0.   -0.08]]\n",
            "[0.03 0.01 0.03 0.04 0.04]\n",
            "----- \n",
            "sample [[ 0.33 -0.19 -0.   -0.16  0.  ]]\n",
            "[0.02 0.   0.   0.01 0.01]\n",
            "----- \n",
            "sample [[-0.   -0.   -0.21 -0.   -0.  ]]\n",
            "[0. 0. 0. 0. 0.]\n",
            "----- \n",
            "sample [[ 0.23  0.   -1.04 -0.   -0.78]]\n",
            "[0.15 0.04 0.11 0.03 0.08]\n",
            "----- \n",
            "sample [[-0.   -0.39  2.28  2.7   0.44]]\n",
            "[0.62 0.31 0.82 0.51 0.73]\n",
            "----- \n",
            "sample [[-0.   -0.    0.12 -0.17  0.  ]]\n",
            "[0. 0. 0. 0. 0.]\n",
            "----- \n",
            "sample [[-0.   -3.27 -1.85 -0.09  0.  ]]\n",
            "[1.03 0.67 0.85 1.64 1.39]\n",
            "----- \n",
            "sample [[-0.  0.  0.  0.  0.]]\n",
            "[0. 0. 0. 0. 0.]\n",
            "----- \n",
            "sample [[-0.43  0.    0.06  0.26 -0.  ]]\n",
            "[0.02 0.   0.01 0.01 0.01]\n",
            "----- \n",
            "sample [[ 0.   -1.02  0.36  0.   -0.  ]]\n",
            "[0.1  0.05 0.09 0.09 0.12]\n"
          ]
        }
      ],
      "source": [
        "print(eigenmodel.U, eigenmodel.V)\n",
        "\n",
        "for X_batch, _ in DataLoader(TensorDataset(X[:10], Y[:10]),\n",
        "           batch_size=1, shuffle=True, generator=torch.Generator(device='cuda')):\n",
        "  print('-----', '\\nsample', X_batch.cpu().numpy().round(2))\n",
        "  H, H_r, diag = eigenmodel(X_batch)\n",
        "  #print(H.cpu().detach().numpy().round(2))\n",
        "\n",
        "  #print(H_r.cpu().detach().numpy().round(2))\n",
        "  print(diag.cpu().detach().numpy().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "yFjvv4n9EK_x",
        "outputId": "bcd6f66f-9398-4bfa-a777-6481bf08f0b1"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (2x5 and 2x2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-2d70f140b2f2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzero_loss_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-14b43e2e36d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply sigmoid after first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m#x = self.relu(self.fc2(x))  # Apply sigmoid after second layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.sigmoid()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x5 and 2x2)"
          ]
        }
      ],
      "source": [
        "zero_loss_model(X[:2,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-IPIgF1DwNy",
        "outputId": "5c4639f9-0644-40a8-8d6d-a074356fab15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.2127, device='cuda:0')\n",
            "tensor(2.2856, device='cuda:0')\n",
            "tensor(2.8269, device='cuda:0')\n",
            "tensor(3.7592, device='cuda:0')\n",
            "tensor(0.0999, device='cuda:0')\n",
            "tensor(0.4014, device='cuda:0')\n",
            "tensor(1.1250, device='cuda:0')\n",
            "tensor(0.4729, device='cuda:0')\n",
            "tensor(0.1261, device='cuda:0')\n",
            "tensor(0.0224, device='cuda:0')\n",
            "tensor(0.2870, device='cuda:0')\n",
            "tensor(0.3234, device='cuda:0')\n",
            "tensor(1.8133, device='cuda:0')\n",
            "tensor(0.9049, device='cuda:0')\n",
            "tensor(0.1137, device='cuda:0')\n",
            "tensor(3.5765, device='cuda:0')\n",
            "tensor(0.2678, device='cuda:0')\n",
            "tensor(1.0457, device='cuda:0')\n",
            "tensor(1.7362, device='cuda:0')\n",
            "tensor(0.0444, device='cuda:0')\n",
            "tensor(3.7711, device='cuda:0')\n",
            "tensor(0.4289, device='cuda:0')\n",
            "tensor(0.0126, device='cuda:0')\n",
            "tensor(0.0443, device='cuda:0')\n",
            "tensor(1.6539, device='cuda:0')\n",
            "tensor(8.0503, device='cuda:0')\n",
            "tensor(0.1769, device='cuda:0')\n",
            "tensor(3.4968, device='cuda:0')\n",
            "tensor(2.5120, device='cuda:0')\n",
            "tensor(0.0116, device='cuda:0')\n",
            "tensor(0.0202, device='cuda:0')\n",
            "tensor(0.1043, device='cuda:0')\n",
            "tensor(0.5565, device='cuda:0')\n",
            "tensor(0.0281, device='cuda:0')\n",
            "tensor(0.6171, device='cuda:0')\n",
            "tensor(0.0009, device='cuda:0')\n",
            "tensor(0.0462, device='cuda:0')\n",
            "tensor(1.3458, device='cuda:0')\n",
            "tensor(0.4921, device='cuda:0')\n",
            "tensor(2.8026, device='cuda:0')\n",
            "tensor(0.5438, device='cuda:0')\n",
            "tensor(0.1696, device='cuda:0')\n",
            "tensor(0.0636, device='cuda:0')\n",
            "tensor(0.3148, device='cuda:0')\n",
            "tensor(0.0004, device='cuda:0')\n",
            "tensor(0.4646, device='cuda:0')\n",
            "tensor(3.0148, device='cuda:0')\n",
            "tensor(0.9988, device='cuda:0')\n",
            "tensor(1.0171, device='cuda:0')\n",
            "tensor(2.1717, device='cuda:0')\n",
            "tensor(0.2007, device='cuda:0')\n",
            "tensor(0.5095, device='cuda:0')\n",
            "tensor(0.5563, device='cuda:0')\n",
            "tensor(1.4940, device='cuda:0')\n",
            "tensor(0.3867, device='cuda:0')\n",
            "tensor(0.0006, device='cuda:0')\n",
            "tensor(2.0018, device='cuda:0')\n",
            "tensor(0.0008, device='cuda:0')\n",
            "tensor(3.2441, device='cuda:0')\n",
            "tensor(0.4428, device='cuda:0')\n",
            "tensor(0.4226, device='cuda:0')\n",
            "tensor(0.1732, device='cuda:0')\n",
            "tensor(0.6316, device='cuda:0')\n",
            "tensor(0.7980, device='cuda:0')\n",
            "tensor(0.4862, device='cuda:0')\n",
            "tensor(2.0894, device='cuda:0')\n",
            "tensor(0.0316, device='cuda:0')\n",
            "tensor(4.4000, device='cuda:0')\n",
            "tensor(0.9068, device='cuda:0')\n",
            "tensor(0.3879, device='cuda:0')\n",
            "tensor(0.7853, device='cuda:0')\n",
            "tensor(4.2462, device='cuda:0')\n",
            "tensor(1.0636, device='cuda:0')\n",
            "tensor(0.0506, device='cuda:0')\n",
            "tensor(2.3951, device='cuda:0')\n",
            "tensor(0.8777, device='cuda:0')\n",
            "tensor(0.2981, device='cuda:0')\n",
            "tensor(11.4021, device='cuda:0')\n",
            "tensor(0.0322, device='cuda:0')\n",
            "tensor(6.2914, device='cuda:0')\n",
            "tensor(6.3189, device='cuda:0')\n",
            "tensor(3.9580, device='cuda:0')\n",
            "tensor(1.1168, device='cuda:0')\n",
            "tensor(0.1187, device='cuda:0')\n",
            "tensor(0.5432, device='cuda:0')\n",
            "tensor(0.0654, device='cuda:0')\n",
            "tensor(3.1259, device='cuda:0')\n",
            "tensor(0.0175, device='cuda:0')\n",
            "tensor(0.8867, device='cuda:0')\n",
            "tensor(3.3245, device='cuda:0')\n",
            "tensor(0.6400, device='cuda:0')\n",
            "tensor(1.1230, device='cuda:0')\n",
            "tensor(0.0589, device='cuda:0')\n",
            "tensor(1.6660, device='cuda:0')\n",
            "tensor(1.6241, device='cuda:0')\n",
            "tensor(0.1340, device='cuda:0')\n",
            "tensor(0.0766, device='cuda:0')\n",
            "tensor(2.6080, device='cuda:0')\n",
            "tensor(0.0539, device='cuda:0')\n",
            "tensor(0.7308, device='cuda:0')\n",
            "tensor(2.3617, device='cuda:0')\n",
            "tensor(0.7631, device='cuda:0')\n",
            "tensor(1.3690, device='cuda:0')\n",
            "tensor(8.5070e-07, device='cuda:0')\n",
            "tensor(1.1265, device='cuda:0')\n",
            "tensor(9.1175, device='cuda:0')\n",
            "tensor(0.2322, device='cuda:0')\n",
            "tensor(0.0214, device='cuda:0')\n",
            "tensor(2.5216, device='cuda:0')\n",
            "tensor(14.7861, device='cuda:0')\n",
            "tensor(0.0137, device='cuda:0')\n",
            "tensor(0.8005, device='cuda:0')\n",
            "tensor(7.2662, device='cuda:0')\n",
            "tensor(0.0049, device='cuda:0')\n",
            "tensor(11.6292, device='cuda:0')\n",
            "tensor(0.0233, device='cuda:0')\n",
            "tensor(0.0126, device='cuda:0')\n",
            "tensor(1.7828, device='cuda:0')\n",
            "tensor(0.2869, device='cuda:0')\n",
            "tensor(0.0247, device='cuda:0')\n",
            "tensor(1.2097, device='cuda:0')\n",
            "tensor(1.5884, device='cuda:0')\n",
            "tensor(0.6812, device='cuda:0')\n",
            "tensor(0.5008, device='cuda:0')\n",
            "tensor(7.7058, device='cuda:0')\n",
            "tensor(0.5136, device='cuda:0')\n",
            "tensor(0.0635, device='cuda:0')\n",
            "tensor(2.6461, device='cuda:0')\n",
            "tensor(0.1618, device='cuda:0')\n",
            "tensor(1.3965, device='cuda:0')\n",
            "tensor(0.4196, device='cuda:0')\n",
            "tensor(0.3174, device='cuda:0')\n",
            "tensor(1.0464, device='cuda:0')\n",
            "tensor(1.1861, device='cuda:0')\n",
            "tensor(0.2505, device='cuda:0')\n",
            "tensor(0.5110, device='cuda:0')\n",
            "tensor(0.5771, device='cuda:0')\n",
            "tensor(0.5565, device='cuda:0')\n",
            "tensor(0.0023, device='cuda:0')\n",
            "tensor(0.5847, device='cuda:0')\n",
            "tensor(1.9300, device='cuda:0')\n",
            "tensor(5.7444, device='cuda:0')\n",
            "tensor(5.5668, device='cuda:0')\n",
            "tensor(0.2474, device='cuda:0')\n",
            "tensor(1.0505, device='cuda:0')\n",
            "tensor(1.7235, device='cuda:0')\n",
            "tensor(0.0228, device='cuda:0')\n",
            "tensor(0.0432, device='cuda:0')\n",
            "tensor(0.0323, device='cuda:0')\n",
            "tensor(2.4122, device='cuda:0')\n",
            "tensor(0.0470, device='cuda:0')\n",
            "tensor(0.6139, device='cuda:0')\n",
            "tensor(0.0047, device='cuda:0')\n",
            "tensor(2.6934, device='cuda:0')\n",
            "tensor(4.5913, device='cuda:0')\n",
            "tensor(3.3333, device='cuda:0')\n",
            "tensor(0.1938, device='cuda:0')\n",
            "tensor(0.0007, device='cuda:0')\n",
            "tensor(0.0830, device='cuda:0')\n",
            "tensor(2.5096, device='cuda:0')\n",
            "tensor(0.2568, device='cuda:0')\n",
            "tensor(0.7518, device='cuda:0')\n",
            "tensor(1.2239, device='cuda:0')\n",
            "tensor(0.0346, device='cuda:0')\n",
            "tensor(3.2403, device='cuda:0')\n",
            "tensor(2.1477, device='cuda:0')\n",
            "tensor(5.4310, device='cuda:0')\n",
            "tensor(0.0155, device='cuda:0')\n",
            "tensor(0.4407, device='cuda:0')\n",
            "tensor(0.0582, device='cuda:0')\n",
            "tensor(0.4874, device='cuda:0')\n",
            "tensor(0.9044, device='cuda:0')\n",
            "tensor(4.8939, device='cuda:0')\n",
            "tensor(0.7763, device='cuda:0')\n",
            "tensor(0.1514, device='cuda:0')\n",
            "tensor(0.5142, device='cuda:0')\n",
            "tensor(0.0183, device='cuda:0')\n",
            "tensor(1.2407, device='cuda:0')\n",
            "tensor(0.6635, device='cuda:0')\n",
            "tensor(0.0345, device='cuda:0')\n",
            "tensor(0.8336, device='cuda:0')\n",
            "tensor(0.9980, device='cuda:0')\n",
            "tensor(0.1593, device='cuda:0')\n",
            "tensor(1.5759, device='cuda:0')\n",
            "tensor(2.4152, device='cuda:0')\n",
            "tensor(4.3792, device='cuda:0')\n",
            "tensor(0.1563, device='cuda:0')\n",
            "tensor(2.7000, device='cuda:0')\n",
            "tensor(0.1466, device='cuda:0')\n",
            "tensor(0.0034, device='cuda:0')\n",
            "tensor(3.1511, device='cuda:0')\n",
            "tensor(0.1712, device='cuda:0')\n",
            "tensor(2.7440, device='cuda:0')\n",
            "tensor(0.3505, device='cuda:0')\n",
            "tensor(0.2906, device='cuda:0')\n",
            "tensor(1.2442, device='cuda:0')\n",
            "tensor(0.3549, device='cuda:0')\n",
            "tensor(0.0268, device='cuda:0')\n",
            "tensor(0.1017, device='cuda:0')\n",
            "tensor(0.6315, device='cuda:0')\n",
            "tensor(1.4086, device='cuda:0')\n",
            "tensor(0.0064, device='cuda:0')\n",
            "tensor(2.0859, device='cuda:0')\n",
            "tensor(4.1530, device='cuda:0')\n",
            "tensor(2.3808, device='cuda:0')\n",
            "tensor(0.7895, device='cuda:0')\n",
            "tensor(0.3810, device='cuda:0')\n",
            "tensor(1.9002, device='cuda:0')\n",
            "tensor(0.4329, device='cuda:0')\n",
            "tensor(0.1748, device='cuda:0')\n",
            "tensor(1.8293, device='cuda:0')\n",
            "tensor(0.0138, device='cuda:0')\n",
            "tensor(0.1973, device='cuda:0')\n",
            "tensor(0.5171, device='cuda:0')\n",
            "tensor(0.0078, device='cuda:0')\n",
            "tensor(0.9841, device='cuda:0')\n",
            "tensor(0.2799, device='cuda:0')\n",
            "tensor(1.8789, device='cuda:0')\n",
            "tensor(0.8425, device='cuda:0')\n",
            "tensor(1.0090, device='cuda:0')\n",
            "tensor(0.5302, device='cuda:0')\n",
            "tensor(0.8220, device='cuda:0')\n",
            "tensor(0.5760, device='cuda:0')\n",
            "tensor(0.1155, device='cuda:0')\n",
            "tensor(0.2809, device='cuda:0')\n",
            "tensor(0.2144, device='cuda:0')\n",
            "tensor(0.0279, device='cuda:0')\n",
            "tensor(0.0124, device='cuda:0')\n",
            "tensor(4.0863, device='cuda:0')\n",
            "tensor(9.2331, device='cuda:0')\n",
            "tensor(1.1165, device='cuda:0')\n",
            "tensor(0.0018, device='cuda:0')\n",
            "tensor(0.3171, device='cuda:0')\n",
            "tensor(0.5183, device='cuda:0')\n",
            "tensor(0.1873, device='cuda:0')\n",
            "tensor(6.2599, device='cuda:0')\n",
            "tensor(0.0037, device='cuda:0')\n",
            "tensor(2.5351, device='cuda:0')\n",
            "tensor(0.4786, device='cuda:0')\n",
            "tensor(0.4702, device='cuda:0')\n",
            "tensor(0.7398, device='cuda:0')\n",
            "tensor(1.1556, device='cuda:0')\n",
            "tensor(0.0429, device='cuda:0')\n",
            "tensor(0.2212, device='cuda:0')\n",
            "tensor(0.0015, device='cuda:0')\n",
            "tensor(0.3383, device='cuda:0')\n",
            "tensor(0.0031, device='cuda:0')\n",
            "tensor(0.4368, device='cuda:0')\n",
            "tensor(1.4997, device='cuda:0')\n",
            "tensor(0.5481, device='cuda:0')\n",
            "tensor(1.8510, device='cuda:0')\n",
            "tensor(0.5839, device='cuda:0')\n",
            "tensor(3.6439, device='cuda:0')\n",
            "tensor(0.1882, device='cuda:0')\n",
            "tensor(0.6717, device='cuda:0')\n",
            "tensor(0.7588, device='cuda:0')\n",
            "tensor(0.2908, device='cuda:0')\n",
            "tensor(0.0082, device='cuda:0')\n",
            "tensor(0.3960, device='cuda:0')\n",
            "tensor(0.1005, device='cuda:0')\n",
            "tensor(0.0743, device='cuda:0')\n",
            "tensor(2.5533, device='cuda:0')\n",
            "tensor(5.2854, device='cuda:0')\n",
            "tensor(1.6108, device='cuda:0')\n",
            "tensor(1.8188, device='cuda:0')\n",
            "tensor(2.8331, device='cuda:0')\n",
            "tensor(0.6132, device='cuda:0')\n",
            "tensor(1.5326, device='cuda:0')\n",
            "tensor(5.5751, device='cuda:0')\n",
            "tensor(2.7167, device='cuda:0')\n",
            "tensor(1.6222, device='cuda:0')\n",
            "tensor(2.4028, device='cuda:0')\n",
            "tensor(0.2352, device='cuda:0')\n",
            "tensor(4.9289, device='cuda:0')\n",
            "tensor(0.6197, device='cuda:0')\n",
            "tensor(1.7610, device='cuda:0')\n",
            "tensor(0.1156, device='cuda:0')\n",
            "tensor(0.6292, device='cuda:0')\n",
            "tensor(0.1592, device='cuda:0')\n",
            "tensor(0.2265, device='cuda:0')\n",
            "tensor(1.7095, device='cuda:0')\n",
            "tensor(0.1534, device='cuda:0')\n",
            "tensor(2.3973, device='cuda:0')\n",
            "tensor(0.0018, device='cuda:0')\n",
            "tensor(0.4880, device='cuda:0')\n",
            "tensor(0.0528, device='cuda:0')\n",
            "tensor(4.6848, device='cuda:0')\n",
            "tensor(0.2628, device='cuda:0')\n",
            "tensor(3.6534, device='cuda:0')\n",
            "tensor(1.0528, device='cuda:0')\n",
            "tensor(0.3172, device='cuda:0')\n",
            "tensor(0.4839, device='cuda:0')\n",
            "tensor(0.2574, device='cuda:0')\n",
            "tensor(0.5490, device='cuda:0')\n",
            "tensor(2.9342, device='cuda:0')\n",
            "tensor(2.6738, device='cuda:0')\n",
            "tensor(0.6597, device='cuda:0')\n",
            "tensor(2.4778, device='cuda:0')\n",
            "tensor(0.4558, device='cuda:0')\n",
            "tensor(0.4244, device='cuda:0')\n",
            "tensor(0.0268, device='cuda:0')\n",
            "tensor(3.0791, device='cuda:0')\n",
            "tensor(6.9774, device='cuda:0')\n",
            "tensor(0.0920, device='cuda:0')\n",
            "tensor(0.7416, device='cuda:0')\n",
            "tensor(0.0056, device='cuda:0')\n",
            "tensor(0.4733, device='cuda:0')\n",
            "tensor(1.9042, device='cuda:0')\n",
            "tensor(0.1163, device='cuda:0')\n",
            "tensor(0.0731, device='cuda:0')\n",
            "tensor(1.2944, device='cuda:0')\n",
            "tensor(3.6997, device='cuda:0')\n",
            "tensor(0.0203, device='cuda:0')\n",
            "tensor(0.0069, device='cuda:0')\n",
            "tensor(1.7509, device='cuda:0')\n",
            "tensor(1.7835, device='cuda:0')\n",
            "tensor(1.3495, device='cuda:0')\n",
            "tensor(0.0126, device='cuda:0')\n",
            "tensor(0.3556, device='cuda:0')\n",
            "tensor(0.1809, device='cuda:0')\n",
            "tensor(0.3378, device='cuda:0')\n",
            "tensor(5.6965, device='cuda:0')\n",
            "tensor(0.5216, device='cuda:0')\n",
            "tensor(0.0098, device='cuda:0')\n",
            "tensor(5.9116, device='cuda:0')\n",
            "tensor(2.9934, device='cuda:0')\n",
            "tensor(6.5920, device='cuda:0')\n",
            "tensor(1.7717, device='cuda:0')\n",
            "tensor(0.0216, device='cuda:0')\n",
            "tensor(0.6123, device='cuda:0')\n",
            "tensor(2.6102, device='cuda:0')\n",
            "tensor(0.9633, device='cuda:0')\n",
            "tensor(1.1216, device='cuda:0')\n",
            "tensor(6.5923e-05, device='cuda:0')\n",
            "tensor(0.3604, device='cuda:0')\n",
            "tensor(0.0529, device='cuda:0')\n",
            "tensor(0.5047, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(1.5021, device='cuda:0')\n",
            "tensor(0.0224, device='cuda:0')\n",
            "tensor(0.0789, device='cuda:0')\n",
            "tensor(1.4729, device='cuda:0')\n",
            "tensor(0.2385, device='cuda:0')\n",
            "tensor(2.4326, device='cuda:0')\n",
            "tensor(2.4387, device='cuda:0')\n",
            "tensor(0.0108, device='cuda:0')\n",
            "tensor(2.3154, device='cuda:0')\n",
            "tensor(1.6101, device='cuda:0')\n",
            "tensor(0.7171, device='cuda:0')\n",
            "tensor(0.3256, device='cuda:0')\n",
            "tensor(0.5835, device='cuda:0')\n",
            "tensor(0.3375, device='cuda:0')\n",
            "tensor(0.2178, device='cuda:0')\n",
            "tensor(0.0209, device='cuda:0')\n",
            "tensor(0.0909, device='cuda:0')\n",
            "tensor(1.2848, device='cuda:0')\n",
            "tensor(0.1000, device='cuda:0')\n",
            "tensor(0.1888, device='cuda:0')\n",
            "tensor(4.7086, device='cuda:0')\n",
            "tensor(2.1075, device='cuda:0')\n",
            "tensor(0.6450, device='cuda:0')\n",
            "tensor(0.0033, device='cuda:0')\n",
            "tensor(0.1557, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.3450, device='cuda:0')\n",
            "tensor(2.0743, device='cuda:0')\n",
            "tensor(1.5050, device='cuda:0')\n",
            "tensor(0.0751, device='cuda:0')\n",
            "tensor(0.0714, device='cuda:0')\n",
            "tensor(2.2136, device='cuda:0')\n",
            "tensor(0.9701, device='cuda:0')\n",
            "tensor(0.0031, device='cuda:0')\n",
            "tensor(2.3417, device='cuda:0')\n",
            "tensor(0.8253, device='cuda:0')\n",
            "tensor(5.6398, device='cuda:0')\n",
            "tensor(4.7623e-05, device='cuda:0')\n",
            "tensor(0.5023, device='cuda:0')\n",
            "tensor(1.4633, device='cuda:0')\n",
            "tensor(1.3579, device='cuda:0')\n",
            "tensor(1.1384, device='cuda:0')\n",
            "tensor(1.3958, device='cuda:0')\n",
            "tensor(2.8986, device='cuda:0')\n",
            "tensor(1.0041, device='cuda:0')\n",
            "tensor(3.1400, device='cuda:0')\n",
            "tensor(4.3625, device='cuda:0')\n",
            "tensor(4.2626, device='cuda:0')\n",
            "tensor(0.0604, device='cuda:0')\n",
            "tensor(0.7643, device='cuda:0')\n",
            "tensor(0.9470, device='cuda:0')\n",
            "tensor(4.6584, device='cuda:0')\n",
            "tensor(0.7655, device='cuda:0')\n",
            "tensor(0.2106, device='cuda:0')\n",
            "tensor(1.0900, device='cuda:0')\n",
            "tensor(0.9874, device='cuda:0')\n",
            "tensor(1.4261, device='cuda:0')\n",
            "tensor(0.1382, device='cuda:0')\n",
            "tensor(0.2678, device='cuda:0')\n",
            "tensor(0.3955, device='cuda:0')\n",
            "tensor(0.0772, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(1.2755, device='cuda:0')\n",
            "tensor(0.0446, device='cuda:0')\n",
            "tensor(2.1907, device='cuda:0')\n",
            "tensor(0.3236, device='cuda:0')\n",
            "tensor(0.1269, device='cuda:0')\n",
            "tensor(2.1890, device='cuda:0')\n",
            "tensor(1.8829, device='cuda:0')\n",
            "tensor(0.0122, device='cuda:0')\n",
            "tensor(0.9629, device='cuda:0')\n",
            "tensor(1.1012, device='cuda:0')\n",
            "tensor(0.2517, device='cuda:0')\n",
            "tensor(0.9187, device='cuda:0')\n",
            "tensor(1.4469, device='cuda:0')\n",
            "tensor(0.2114, device='cuda:0')\n",
            "tensor(3.5303, device='cuda:0')\n",
            "tensor(1.3027, device='cuda:0')\n",
            "tensor(0.1808, device='cuda:0')\n",
            "tensor(0.2991, device='cuda:0')\n",
            "tensor(0.0008, device='cuda:0')\n",
            "tensor(2.8752, device='cuda:0')\n",
            "tensor(3.4689, device='cuda:0')\n",
            "tensor(6.4898, device='cuda:0')\n",
            "tensor(5.1430, device='cuda:0')\n",
            "tensor(0.8046, device='cuda:0')\n",
            "tensor(1.5959, device='cuda:0')\n",
            "tensor(0.0122, device='cuda:0')\n",
            "tensor(0.2587, device='cuda:0')\n",
            "tensor(0.0160, device='cuda:0')\n",
            "tensor(1.3635, device='cuda:0')\n",
            "tensor(3.2129, device='cuda:0')\n",
            "tensor(1.5863, device='cuda:0')\n",
            "tensor(0.4788, device='cuda:0')\n",
            "tensor(1.2843, device='cuda:0')\n",
            "tensor(2.3414, device='cuda:0')\n",
            "tensor(0.7521, device='cuda:0')\n",
            "tensor(2.2997, device='cuda:0')\n",
            "tensor(0.0407, device='cuda:0')\n",
            "tensor(0.0198, device='cuda:0')\n",
            "tensor(0.2389, device='cuda:0')\n",
            "tensor(6.5195, device='cuda:0')\n",
            "tensor(0.3516, device='cuda:0')\n",
            "tensor(3.0690, device='cuda:0')\n",
            "tensor(0.0037, device='cuda:0')\n",
            "tensor(1.1600, device='cuda:0')\n",
            "tensor(1.3455, device='cuda:0')\n",
            "tensor(0.1942, device='cuda:0')\n",
            "tensor(2.4946, device='cuda:0')\n",
            "tensor(0.6494, device='cuda:0')\n",
            "tensor(3.7401, device='cuda:0')\n",
            "tensor(0.7521, device='cuda:0')\n",
            "tensor(1.7651, device='cuda:0')\n",
            "tensor(0.6007, device='cuda:0')\n",
            "tensor(1.0897, device='cuda:0')\n",
            "tensor(1.3159, device='cuda:0')\n",
            "tensor(0.0433, device='cuda:0')\n",
            "tensor(0.0767, device='cuda:0')\n",
            "tensor(0.0320, device='cuda:0')\n",
            "tensor(0.5747, device='cuda:0')\n",
            "tensor(2.7144, device='cuda:0')\n",
            "tensor(0.6771, device='cuda:0')\n",
            "tensor(0.4457, device='cuda:0')\n",
            "tensor(2.5102, device='cuda:0')\n",
            "tensor(0.5889, device='cuda:0')\n",
            "tensor(0.1085, device='cuda:0')\n",
            "tensor(0.6147, device='cuda:0')\n",
            "tensor(0.0148, device='cuda:0')\n",
            "tensor(0.1030, device='cuda:0')\n",
            "tensor(0.0573, device='cuda:0')\n",
            "tensor(0.2204, device='cuda:0')\n",
            "tensor(0.5069, device='cuda:0')\n",
            "tensor(0.7243, device='cuda:0')\n",
            "tensor(1.3184, device='cuda:0')\n",
            "tensor(0.0671, device='cuda:0')\n",
            "tensor(0.8770, device='cuda:0')\n",
            "tensor(2.9556, device='cuda:0')\n",
            "tensor(0.4746, device='cuda:0')\n",
            "tensor(4.5163, device='cuda:0')\n",
            "tensor(0.4254, device='cuda:0')\n",
            "tensor(0.5483, device='cuda:0')\n",
            "tensor(0.0934, device='cuda:0')\n",
            "tensor(0.0317, device='cuda:0')\n",
            "tensor(4.3806, device='cuda:0')\n",
            "tensor(0.6706, device='cuda:0')\n",
            "tensor(8.2816, device='cuda:0')\n",
            "tensor(0.5823, device='cuda:0')\n",
            "tensor(2.3055, device='cuda:0')\n",
            "tensor(0.9462, device='cuda:0')\n",
            "tensor(0.0023, device='cuda:0')\n",
            "tensor(0.9202, device='cuda:0')\n",
            "tensor(0.1338, device='cuda:0')\n",
            "tensor(4.2944, device='cuda:0')\n",
            "tensor(0.2806, device='cuda:0')\n",
            "tensor(0.0304, device='cuda:0')\n",
            "tensor(0.6708, device='cuda:0')\n",
            "tensor(2.3721, device='cuda:0')\n",
            "tensor(0.3118, device='cuda:0')\n",
            "tensor(1.6312, device='cuda:0')\n",
            "tensor(0.2154, device='cuda:0')\n",
            "tensor(2.4275, device='cuda:0')\n",
            "tensor(0.7228, device='cuda:0')\n",
            "tensor(0.1421, device='cuda:0')\n",
            "tensor(0.6751, device='cuda:0')\n",
            "tensor(0.5193, device='cuda:0')\n",
            "tensor(0.2350, device='cuda:0')\n",
            "tensor(0.1103, device='cuda:0')\n",
            "tensor(3.6319, device='cuda:0')\n",
            "tensor(3.0904, device='cuda:0')\n",
            "tensor(0.3803, device='cuda:0')\n",
            "tensor(3.4231, device='cuda:0')\n",
            "tensor(1.2572, device='cuda:0')\n",
            "tensor(0.2015, device='cuda:0')\n",
            "tensor(1.8145, device='cuda:0')\n",
            "tensor(3.5145, device='cuda:0')\n",
            "tensor(0.3621, device='cuda:0')\n",
            "tensor(0.2951, device='cuda:0')\n",
            "tensor(2.6118, device='cuda:0')\n",
            "tensor(0.8436, device='cuda:0')\n",
            "tensor(0.2293, device='cuda:0')\n",
            "tensor(7.6997, device='cuda:0')\n",
            "tensor(1.9432, device='cuda:0')\n",
            "tensor(4.8180, device='cuda:0')\n",
            "tensor(1.5904, device='cuda:0')\n",
            "tensor(1.2219, device='cuda:0')\n",
            "tensor(0.6780, device='cuda:0')\n",
            "tensor(0.6829, device='cuda:0')\n",
            "tensor(2.5807, device='cuda:0')\n",
            "tensor(0.0185, device='cuda:0')\n",
            "tensor(0.3096, device='cuda:0')\n",
            "tensor(10.0678, device='cuda:0')\n",
            "tensor(0.1691, device='cuda:0')\n",
            "tensor(1.0515, device='cuda:0')\n",
            "tensor(1.9073, device='cuda:0')\n",
            "tensor(2.9317, device='cuda:0')\n",
            "tensor(0.9724, device='cuda:0')\n",
            "tensor(0.2172, device='cuda:0')\n",
            "tensor(2.1606, device='cuda:0')\n",
            "tensor(0.7592, device='cuda:0')\n",
            "tensor(0.3610, device='cuda:0')\n",
            "tensor(0.0877, device='cuda:0')\n",
            "tensor(0.1232, device='cuda:0')\n",
            "tensor(0.0198, device='cuda:0')\n",
            "tensor(3.2259, device='cuda:0')\n",
            "tensor(4.4293, device='cuda:0')\n",
            "tensor(2.0609, device='cuda:0')\n",
            "tensor(2.2176, device='cuda:0')\n",
            "tensor(2.8353, device='cuda:0')\n",
            "tensor(0.8702, device='cuda:0')\n",
            "tensor(0.1007, device='cuda:0')\n",
            "tensor(1.0513, device='cuda:0')\n",
            "tensor(3.7310, device='cuda:0')\n",
            "tensor(2.1798, device='cuda:0')\n",
            "tensor(7.7640, device='cuda:0')\n",
            "tensor(0.5518, device='cuda:0')\n",
            "tensor(6.2319, device='cuda:0')\n",
            "tensor(0.0366, device='cuda:0')\n",
            "tensor(0.1859, device='cuda:0')\n",
            "tensor(0.6064, device='cuda:0')\n",
            "tensor(0.0092, device='cuda:0')\n",
            "tensor(0.2757, device='cuda:0')\n",
            "tensor(2.3810, device='cuda:0')\n",
            "tensor(3.9869, device='cuda:0')\n",
            "tensor(1.0910, device='cuda:0')\n",
            "tensor(0.9248, device='cuda:0')\n",
            "tensor(1.1468, device='cuda:0')\n",
            "tensor(1.1824, device='cuda:0')\n",
            "tensor(0.0136, device='cuda:0')\n",
            "tensor(1.1158, device='cuda:0')\n",
            "tensor(0.0495, device='cuda:0')\n",
            "tensor(0.1422, device='cuda:0')\n",
            "tensor(1.0966, device='cuda:0')\n",
            "tensor(0.4691, device='cuda:0')\n",
            "tensor(0.7348, device='cuda:0')\n",
            "tensor(0.0101, device='cuda:0')\n",
            "tensor(0.9757, device='cuda:0')\n",
            "tensor(4.9095, device='cuda:0')\n",
            "tensor(1.6090, device='cuda:0')\n",
            "tensor(0.2865, device='cuda:0')\n",
            "tensor(5.0688, device='cuda:0')\n",
            "tensor(0.5220, device='cuda:0')\n",
            "tensor(6.1904, device='cuda:0')\n",
            "tensor(1.3334, device='cuda:0')\n",
            "tensor(0.1150, device='cuda:0')\n",
            "tensor(0.6769, device='cuda:0')\n",
            "tensor(2.4964, device='cuda:0')\n",
            "tensor(0.3269, device='cuda:0')\n",
            "tensor(7.4716, device='cuda:0')\n",
            "tensor(5.8438, device='cuda:0')\n",
            "tensor(0.3265, device='cuda:0')\n",
            "tensor(0.0576, device='cuda:0')\n",
            "tensor(1.2244, device='cuda:0')\n",
            "tensor(0.0859, device='cuda:0')\n",
            "tensor(0.0882, device='cuda:0')\n",
            "tensor(0.0426, device='cuda:0')\n",
            "tensor(2.7205, device='cuda:0')\n",
            "tensor(4.9209, device='cuda:0')\n",
            "tensor(9.1384, device='cuda:0')\n",
            "tensor(1.2093, device='cuda:0')\n",
            "tensor(1.7778, device='cuda:0')\n",
            "tensor(6.3615, device='cuda:0')\n",
            "tensor(3.1978, device='cuda:0')\n",
            "tensor(0.0375, device='cuda:0')\n",
            "tensor(0.0338, device='cuda:0')\n",
            "tensor(0.7304, device='cuda:0')\n",
            "tensor(2.2474, device='cuda:0')\n",
            "tensor(0.0111, device='cuda:0')\n",
            "tensor(5.0406, device='cuda:0')\n",
            "tensor(1.3472, device='cuda:0')\n",
            "tensor(2.4794, device='cuda:0')\n",
            "tensor(1.0938, device='cuda:0')\n",
            "tensor(0.2323, device='cuda:0')\n",
            "tensor(4.1178, device='cuda:0')\n",
            "tensor(6.5218e-06, device='cuda:0')\n",
            "tensor(1.3948, device='cuda:0')\n",
            "tensor(0.7532, device='cuda:0')\n",
            "tensor(4.9649, device='cuda:0')\n",
            "tensor(0.0529, device='cuda:0')\n",
            "tensor(2.5201, device='cuda:0')\n",
            "tensor(0.9419, device='cuda:0')\n",
            "tensor(0.1546, device='cuda:0')\n",
            "tensor(3.5981, device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0')\n",
            "tensor(0.3824, device='cuda:0')\n",
            "tensor(1.3792, device='cuda:0')\n",
            "tensor(0.4368, device='cuda:0')\n",
            "tensor(2.0787, device='cuda:0')\n",
            "tensor(0.7108, device='cuda:0')\n",
            "tensor(2.2248, device='cuda:0')\n",
            "tensor(0.6145, device='cuda:0')\n",
            "tensor(1.2091, device='cuda:0')\n",
            "tensor(0.1745, device='cuda:0')\n",
            "tensor(3.6821, device='cuda:0')\n",
            "tensor(0.6706, device='cuda:0')\n",
            "tensor(2.3609, device='cuda:0')\n",
            "tensor(0.3344, device='cuda:0')\n",
            "tensor(1.1950, device='cuda:0')\n",
            "tensor(0.0406, device='cuda:0')\n",
            "tensor(1.1384, device='cuda:0')\n",
            "tensor(1.5720, device='cuda:0')\n",
            "tensor(0.3823, device='cuda:0')\n",
            "tensor(12.8088, device='cuda:0')\n",
            "tensor(1.8390, device='cuda:0')\n",
            "tensor(0.5952, device='cuda:0')\n",
            "tensor(0.0043, device='cuda:0')\n",
            "tensor(1.8167, device='cuda:0')\n",
            "tensor(1.5431, device='cuda:0')\n",
            "tensor(0.4025, device='cuda:0')\n",
            "tensor(0.1290, device='cuda:0')\n",
            "tensor(0.3323, device='cuda:0')\n",
            "tensor(1.0571, device='cuda:0')\n",
            "tensor(0.8015, device='cuda:0')\n",
            "tensor(0.0313, device='cuda:0')\n",
            "tensor(1.5070, device='cuda:0')\n",
            "tensor(1.7221, device='cuda:0')\n",
            "tensor(0.8504, device='cuda:0')\n",
            "tensor(2.5239, device='cuda:0')\n",
            "tensor(0.2397, device='cuda:0')\n",
            "tensor(10.0380, device='cuda:0')\n",
            "tensor(0.1167, device='cuda:0')\n",
            "tensor(0.7020, device='cuda:0')\n",
            "tensor(1.7417, device='cuda:0')\n",
            "tensor(1.2621, device='cuda:0')\n",
            "tensor(0.1300, device='cuda:0')\n",
            "tensor(1.6304, device='cuda:0')\n",
            "tensor(0.1717, device='cuda:0')\n",
            "tensor(0.0085, device='cuda:0')\n",
            "tensor(0.6550, device='cuda:0')\n",
            "tensor(1.9373, device='cuda:0')\n",
            "tensor(1.1236, device='cuda:0')\n",
            "tensor(0.2087, device='cuda:0')\n",
            "tensor(0.0377, device='cuda:0')\n",
            "tensor(1.1074, device='cuda:0')\n",
            "tensor(2.2358, device='cuda:0')\n",
            "tensor(0.0006, device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0')\n",
            "tensor(5.2742, device='cuda:0')\n",
            "tensor(0.2829, device='cuda:0')\n",
            "tensor(1.3310, device='cuda:0')\n",
            "tensor(6.3016, device='cuda:0')\n",
            "tensor(0.1233, device='cuda:0')\n",
            "tensor(0.8850, device='cuda:0')\n",
            "tensor(0.1118, device='cuda:0')\n",
            "tensor(1.7440, device='cuda:0')\n",
            "tensor(1.0003, device='cuda:0')\n",
            "tensor(2.0498, device='cuda:0')\n",
            "tensor(0.2954, device='cuda:0')\n",
            "tensor(0.1651, device='cuda:0')\n",
            "tensor(1.2666, device='cuda:0')\n",
            "tensor(1.7684, device='cuda:0')\n",
            "tensor(0.7134, device='cuda:0')\n",
            "tensor(0.0906, device='cuda:0')\n",
            "tensor(3.6546, device='cuda:0')\n",
            "tensor(0.9216, device='cuda:0')\n",
            "tensor(0.0563, device='cuda:0')\n",
            "tensor(0.9649, device='cuda:0')\n",
            "tensor(0.9368, device='cuda:0')\n",
            "tensor(3.1183, device='cuda:0')\n",
            "tensor(0.9141, device='cuda:0')\n",
            "tensor(1.5426, device='cuda:0')\n",
            "tensor(2.7993, device='cuda:0')\n",
            "tensor(2.9409, device='cuda:0')\n",
            "tensor(0.8148, device='cuda:0')\n",
            "tensor(2.5454, device='cuda:0')\n",
            "tensor(0.1520, device='cuda:0')\n",
            "tensor(0.2359, device='cuda:0')\n",
            "tensor(0.1732, device='cuda:0')\n",
            "tensor(0.3937, device='cuda:0')\n",
            "tensor(3.6224, device='cuda:0')\n",
            "tensor(0.1127, device='cuda:0')\n",
            "tensor(1.6221, device='cuda:0')\n",
            "tensor(0.5669, device='cuda:0')\n",
            "tensor(3.2348, device='cuda:0')\n",
            "tensor(2.8229, device='cuda:0')\n",
            "tensor(0.9187, device='cuda:0')\n",
            "tensor(0.1617, device='cuda:0')\n",
            "tensor(0.1092, device='cuda:0')\n",
            "tensor(1.7508, device='cuda:0')\n",
            "tensor(0.4550, device='cuda:0')\n",
            "tensor(0.2948, device='cuda:0')\n",
            "tensor(0.0173, device='cuda:0')\n",
            "tensor(0.0243, device='cuda:0')\n",
            "tensor(4.3913, device='cuda:0')\n",
            "tensor(0.0629, device='cuda:0')\n",
            "tensor(0.1772, device='cuda:0')\n",
            "tensor(5.6587, device='cuda:0')\n",
            "tensor(0.6674, device='cuda:0')\n",
            "tensor(0.0574, device='cuda:0')\n",
            "tensor(11.3475, device='cuda:0')\n",
            "tensor(0.0710, device='cuda:0')\n",
            "tensor(0.0093, device='cuda:0')\n",
            "tensor(3.8427, device='cuda:0')\n",
            "tensor(7.0039, device='cuda:0')\n",
            "tensor(1.2575, device='cuda:0')\n",
            "tensor(6.2364, device='cuda:0')\n",
            "tensor(2.7105, device='cuda:0')\n",
            "tensor(0.8354, device='cuda:0')\n",
            "tensor(0.5276, device='cuda:0')\n",
            "tensor(3.3411, device='cuda:0')\n",
            "tensor(0.2229, device='cuda:0')\n",
            "tensor(5.4621, device='cuda:0')\n",
            "tensor(3.1328, device='cuda:0')\n",
            "tensor(0.0004, device='cuda:0')\n",
            "tensor(0.1293, device='cuda:0')\n",
            "tensor(0.8262, device='cuda:0')\n",
            "tensor(2.2275, device='cuda:0')\n",
            "tensor(2.4652, device='cuda:0')\n",
            "tensor(0.4590, device='cuda:0')\n",
            "tensor(0.0768, device='cuda:0')\n",
            "tensor(2.0634, device='cuda:0')\n",
            "tensor(0.4403, device='cuda:0')\n",
            "tensor(5.5125, device='cuda:0')\n",
            "tensor(1.0592, device='cuda:0')\n",
            "tensor(2.2219, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.2472, device='cuda:0')\n",
            "tensor(0.2451, device='cuda:0')\n",
            "tensor(0.5403, device='cuda:0')\n",
            "tensor(7.2023, device='cuda:0')\n",
            "tensor(0.3890, device='cuda:0')\n",
            "tensor(2.4916, device='cuda:0')\n",
            "tensor(1.0731, device='cuda:0')\n",
            "tensor(0.8425, device='cuda:0')\n",
            "tensor(0.0713, device='cuda:0')\n",
            "tensor(0.0054, device='cuda:0')\n",
            "tensor(0.3860, device='cuda:0')\n",
            "tensor(2.4002, device='cuda:0')\n",
            "tensor(2.9474, device='cuda:0')\n",
            "tensor(2.5519, device='cuda:0')\n",
            "tensor(5.2985, device='cuda:0')\n",
            "tensor(0.7920, device='cuda:0')\n",
            "tensor(6.7087, device='cuda:0')\n",
            "tensor(0.0197, device='cuda:0')\n",
            "tensor(2.4291, device='cuda:0')\n",
            "tensor(2.8853, device='cuda:0')\n",
            "tensor(1.0697, device='cuda:0')\n",
            "tensor(1.3914, device='cuda:0')\n",
            "tensor(4.0299, device='cuda:0')\n",
            "tensor(2.0340, device='cuda:0')\n",
            "tensor(0.3992, device='cuda:0')\n",
            "tensor(0.0015, device='cuda:0')\n",
            "tensor(0.5575, device='cuda:0')\n",
            "tensor(0.1398, device='cuda:0')\n",
            "tensor(0.9791, device='cuda:0')\n",
            "tensor(0.1327, device='cuda:0')\n",
            "tensor(0.0518, device='cuda:0')\n",
            "tensor(2.2665, device='cuda:0')\n",
            "tensor(0.0351, device='cuda:0')\n",
            "tensor(0.5877, device='cuda:0')\n",
            "tensor(0.8440, device='cuda:0')\n",
            "tensor(2.2392, device='cuda:0')\n",
            "tensor(0.0305, device='cuda:0')\n",
            "tensor(0.2540, device='cuda:0')\n",
            "tensor(4.5008, device='cuda:0')\n",
            "tensor(0.9745, device='cuda:0')\n",
            "tensor(0.8971, device='cuda:0')\n",
            "tensor(2.1766, device='cuda:0')\n",
            "tensor(0.3093, device='cuda:0')\n",
            "tensor(0.0858, device='cuda:0')\n",
            "tensor(4.0173, device='cuda:0')\n",
            "tensor(1.6841, device='cuda:0')\n",
            "tensor(6.3185, device='cuda:0')\n",
            "tensor(0.5658, device='cuda:0')\n",
            "tensor(0.0052, device='cuda:0')\n",
            "tensor(0.1463, device='cuda:0')\n",
            "tensor(1.0538, device='cuda:0')\n",
            "tensor(0.0217, device='cuda:0')\n",
            "tensor(0.4479, device='cuda:0')\n",
            "tensor(0.1190, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(3.1858, device='cuda:0')\n",
            "tensor(0.7634, device='cuda:0')\n",
            "tensor(7.8898, device='cuda:0')\n",
            "tensor(0.4379, device='cuda:0')\n",
            "tensor(0.9101, device='cuda:0')\n",
            "tensor(0.0290, device='cuda:0')\n",
            "tensor(0.0281, device='cuda:0')\n",
            "tensor(0.2747, device='cuda:0')\n",
            "tensor(0.5577, device='cuda:0')\n",
            "tensor(0.1254, device='cuda:0')\n",
            "tensor(0.1939, device='cuda:0')\n",
            "tensor(0.4516, device='cuda:0')\n",
            "tensor(0.7188, device='cuda:0')\n",
            "tensor(5.1727, device='cuda:0')\n",
            "tensor(0.0009, device='cuda:0')\n",
            "tensor(0.1133, device='cuda:0')\n",
            "tensor(2.1757, device='cuda:0')\n",
            "tensor(0.0007, device='cuda:0')\n",
            "tensor(0.3470, device='cuda:0')\n",
            "tensor(8.6341, device='cuda:0')\n",
            "tensor(0.0107, device='cuda:0')\n",
            "tensor(1.8815, device='cuda:0')\n",
            "tensor(0.1669, device='cuda:0')\n",
            "tensor(10.3673, device='cuda:0')\n",
            "tensor(0.0195, device='cuda:0')\n",
            "tensor(0.0277, device='cuda:0')\n",
            "tensor(0.1241, device='cuda:0')\n",
            "tensor(0.1542, device='cuda:0')\n",
            "tensor(0.3154, device='cuda:0')\n",
            "tensor(0.0166, device='cuda:0')\n",
            "tensor(0.8216, device='cuda:0')\n",
            "tensor(0.0852, device='cuda:0')\n",
            "tensor(0.4425, device='cuda:0')\n",
            "tensor(2.5058, device='cuda:0')\n",
            "tensor(2.1426, device='cuda:0')\n",
            "tensor(0.9141, device='cuda:0')\n",
            "tensor(2.1050, device='cuda:0')\n",
            "tensor(6.9401e-05, device='cuda:0')\n",
            "tensor(3.7037, device='cuda:0')\n",
            "tensor(0.0858, device='cuda:0')\n",
            "tensor(3.3104, device='cuda:0')\n",
            "tensor(0.1178, device='cuda:0')\n",
            "tensor(0.5003, device='cuda:0')\n",
            "tensor(0.0227, device='cuda:0')\n",
            "tensor(3.1293, device='cuda:0')\n",
            "tensor(0.3271, device='cuda:0')\n",
            "tensor(0.0710, device='cuda:0')\n",
            "tensor(0.0058, device='cuda:0')\n",
            "tensor(0.8701, device='cuda:0')\n",
            "tensor(0.0051, device='cuda:0')\n",
            "tensor(0.0562, device='cuda:0')\n",
            "tensor(1.1209, device='cuda:0')\n",
            "tensor(0.0006, device='cuda:0')\n",
            "tensor(0.2734, device='cuda:0')\n",
            "tensor(0.6278, device='cuda:0')\n",
            "tensor(0.0367, device='cuda:0')\n",
            "tensor(0.0135, device='cuda:0')\n",
            "tensor(3.4292, device='cuda:0')\n",
            "tensor(0.2347, device='cuda:0')\n",
            "tensor(0.6915, device='cuda:0')\n",
            "tensor(0.6437, device='cuda:0')\n",
            "tensor(0.7168, device='cuda:0')\n",
            "tensor(0.2869, device='cuda:0')\n",
            "tensor(4.3549, device='cuda:0')\n",
            "tensor(0.3329, device='cuda:0')\n",
            "tensor(0.6381, device='cuda:0')\n",
            "tensor(0.9452, device='cuda:0')\n",
            "tensor(5.1329, device='cuda:0')\n",
            "tensor(1.0610, device='cuda:0')\n",
            "tensor(0.3560, device='cuda:0')\n",
            "tensor(1.2789, device='cuda:0')\n",
            "tensor(0.3888, device='cuda:0')\n",
            "tensor(0.0539, device='cuda:0')\n",
            "tensor(1.1021, device='cuda:0')\n",
            "tensor(0.1523, device='cuda:0')\n",
            "tensor(1.2955, device='cuda:0')\n",
            "tensor(4.3422, device='cuda:0')\n",
            "tensor(0.3454, device='cuda:0')\n",
            "tensor(0.2085, device='cuda:0')\n",
            "tensor(1.4143, device='cuda:0')\n",
            "tensor(0.0005, device='cuda:0')\n",
            "tensor(0.0420, device='cuda:0')\n",
            "tensor(0.0049, device='cuda:0')\n",
            "tensor(3.6450, device='cuda:0')\n",
            "tensor(0.0026, device='cuda:0')\n",
            "tensor(0.0303, device='cuda:0')\n",
            "tensor(1.9882, device='cuda:0')\n",
            "tensor(1.3967, device='cuda:0')\n",
            "tensor(1.1119, device='cuda:0')\n",
            "tensor(1.4856, device='cuda:0')\n",
            "tensor(0.2851, device='cuda:0')\n",
            "tensor(3.0935, device='cuda:0')\n",
            "tensor(0.1489, device='cuda:0')\n",
            "tensor(0.1537, device='cuda:0')\n",
            "tensor(0.1087, device='cuda:0')\n",
            "tensor(0.0095, device='cuda:0')\n",
            "tensor(1.2974, device='cuda:0')\n",
            "tensor(0.2395, device='cuda:0')\n",
            "tensor(0.3576, device='cuda:0')\n",
            "tensor(0.0211, device='cuda:0')\n",
            "tensor(0.0823, device='cuda:0')\n",
            "tensor(0.4345, device='cuda:0')\n",
            "tensor(3.0037, device='cuda:0')\n",
            "tensor(0.3277, device='cuda:0')\n",
            "tensor(0.2461, device='cuda:0')\n",
            "tensor(2.7617, device='cuda:0')\n",
            "tensor(0.0058, device='cuda:0')\n",
            "tensor(1.4447, device='cuda:0')\n",
            "tensor(0.1365, device='cuda:0')\n",
            "tensor(1.2020, device='cuda:0')\n",
            "tensor(0.2696, device='cuda:0')\n",
            "tensor(1.0394, device='cuda:0')\n",
            "tensor(0.6963, device='cuda:0')\n",
            "tensor(0.0009, device='cuda:0')\n",
            "tensor(0.2108, device='cuda:0')\n",
            "tensor(0.6492, device='cuda:0')\n",
            "tensor(0.2453, device='cuda:0')\n",
            "tensor(0.0020, device='cuda:0')\n",
            "tensor(4.7912, device='cuda:0')\n",
            "tensor(0.6180, device='cuda:0')\n",
            "tensor(1.3123, device='cuda:0')\n",
            "tensor(0.5517, device='cuda:0')\n",
            "tensor(4.7499, device='cuda:0')\n",
            "tensor(1.2071, device='cuda:0')\n",
            "tensor(8.3183, device='cuda:0')\n",
            "tensor(4.2787, device='cuda:0')\n",
            "tensor(0.7775, device='cuda:0')\n",
            "tensor(0.3656, device='cuda:0')\n",
            "tensor(4.1295, device='cuda:0')\n",
            "tensor(0.4226, device='cuda:0')\n",
            "tensor(0.0900, device='cuda:0')\n",
            "tensor(1.5466, device='cuda:0')\n",
            "tensor(0.0008, device='cuda:0')\n",
            "tensor(0.4765, device='cuda:0')\n",
            "tensor(0.7077, device='cuda:0')\n",
            "tensor(1.9636, device='cuda:0')\n",
            "tensor(5.6973, device='cuda:0')\n",
            "tensor(0.7887, device='cuda:0')\n",
            "tensor(0.0168, device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0')\n",
            "tensor(0.0004, device='cuda:0')\n",
            "tensor(1.1338, device='cuda:0')\n",
            "tensor(0.5377, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.3100, device='cuda:0')\n",
            "tensor(0.6843, device='cuda:0')\n",
            "tensor(0.0214, device='cuda:0')\n",
            "tensor(0.8152, device='cuda:0')\n",
            "tensor(0.0185, device='cuda:0')\n",
            "tensor(4.5089, device='cuda:0')\n",
            "tensor(0.5370, device='cuda:0')\n",
            "tensor(0.2754, device='cuda:0')\n",
            "tensor(0.3780, device='cuda:0')\n",
            "tensor(0.1174, device='cuda:0')\n",
            "tensor(0.0131, device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0')\n",
            "tensor(0.2183, device='cuda:0')\n",
            "tensor(3.1795, device='cuda:0')\n",
            "tensor(0.9713, device='cuda:0')\n",
            "tensor(0.7966, device='cuda:0')\n",
            "tensor(1.3340, device='cuda:0')\n",
            "tensor(0.4455, device='cuda:0')\n",
            "tensor(0.5610, device='cuda:0')\n",
            "tensor(3.0131, device='cuda:0')\n",
            "tensor(0.1398, device='cuda:0')\n",
            "tensor(1.5556, device='cuda:0')\n",
            "tensor(0.0130, device='cuda:0')\n",
            "tensor(2.4248, device='cuda:0')\n",
            "tensor(0.7985, device='cuda:0')\n",
            "tensor(2.8060, device='cuda:0')\n",
            "tensor(0.9228, device='cuda:0')\n",
            "tensor(4.9752, device='cuda:0')\n",
            "tensor(1.7925, device='cuda:0')\n",
            "tensor(0.0526, device='cuda:0')\n",
            "tensor(1.4540, device='cuda:0')\n",
            "tensor(0.2857, device='cuda:0')\n",
            "tensor(2.6379, device='cuda:0')\n",
            "tensor(0.0350, device='cuda:0')\n",
            "tensor(2.1978, device='cuda:0')\n",
            "tensor(0.4620, device='cuda:0')\n",
            "tensor(0.0064, device='cuda:0')\n",
            "tensor(1.6034e-05, device='cuda:0')\n",
            "tensor(0.0182, device='cuda:0')\n",
            "tensor(1.8563, device='cuda:0')\n",
            "tensor(0.4014, device='cuda:0')\n",
            "tensor(2.6200, device='cuda:0')\n",
            "tensor(0.5169, device='cuda:0')\n",
            "tensor(5.7208, device='cuda:0')\n",
            "tensor(0.2856, device='cuda:0')\n",
            "tensor(5.9501, device='cuda:0')\n",
            "tensor(3.4431, device='cuda:0')\n",
            "tensor(0.4511, device='cuda:0')\n",
            "tensor(0.3682, device='cuda:0')\n",
            "tensor(4.1896e-05, device='cuda:0')\n",
            "tensor(0.4966, device='cuda:0')\n",
            "tensor(0.0013, device='cuda:0')\n",
            "tensor(2.1655, device='cuda:0')\n",
            "tensor(0.1210, device='cuda:0')\n",
            "tensor(3.2818, device='cuda:0')\n",
            "tensor(0.0554, device='cuda:0')\n",
            "tensor(0.0253, device='cuda:0')\n",
            "tensor(0.0192, device='cuda:0')\n",
            "tensor(3.8072, device='cuda:0')\n",
            "tensor(0.0597, device='cuda:0')\n",
            "tensor(3.6980, device='cuda:0')\n",
            "tensor(10.0320, device='cuda:0')\n",
            "tensor(0.0936, device='cuda:0')\n",
            "tensor(0.0754, device='cuda:0')\n",
            "tensor(0.0016, device='cuda:0')\n",
            "tensor(0.8669, device='cuda:0')\n",
            "tensor(0.0325, device='cuda:0')\n",
            "tensor(0.0755, device='cuda:0')\n",
            "tensor(0.0141, device='cuda:0')\n",
            "tensor(3.6166, device='cuda:0')\n",
            "tensor(0.2719, device='cuda:0')\n",
            "tensor(1.0818, device='cuda:0')\n",
            "tensor(1.3274, device='cuda:0')\n",
            "tensor(0.0612, device='cuda:0')\n",
            "tensor(0.0256, device='cuda:0')\n",
            "tensor(1.2136, device='cuda:0')\n",
            "tensor(0.3388, device='cuda:0')\n",
            "tensor(0.3014, device='cuda:0')\n",
            "tensor(1.8775, device='cuda:0')\n",
            "tensor(2.8547, device='cuda:0')\n",
            "tensor(0.1302, device='cuda:0')\n",
            "tensor(0.9976, device='cuda:0')\n",
            "tensor(0.1835, device='cuda:0')\n",
            "tensor(0.5730, device='cuda:0')\n",
            "tensor(4.9780, device='cuda:0')\n",
            "tensor(0.3446, device='cuda:0')\n",
            "tensor(0.2708, device='cuda:0')\n",
            "tensor(0.3980, device='cuda:0')\n",
            "tensor(0.0692, device='cuda:0')\n",
            "tensor(1.7369, device='cuda:0')\n",
            "tensor(1.9042e-05, device='cuda:0')\n",
            "tensor(1.8031, device='cuda:0')\n",
            "tensor(3.9725e-05, device='cuda:0')\n",
            "tensor(0.6982, device='cuda:0')\n",
            "tensor(0.0317, device='cuda:0')\n",
            "tensor(0.0276, device='cuda:0')\n",
            "tensor(1.1354, device='cuda:0')\n",
            "tensor(0.3220, device='cuda:0')\n",
            "tensor(0.2983, device='cuda:0')\n",
            "tensor(1.5495, device='cuda:0')\n",
            "tensor(0.8088, device='cuda:0')\n",
            "tensor(1.4294, device='cuda:0')\n",
            "tensor(0.5703, device='cuda:0')\n",
            "tensor(0.0890, device='cuda:0')\n",
            "tensor(0.0818, device='cuda:0')\n",
            "tensor(5.5514, device='cuda:0')\n",
            "tensor(1.1371, device='cuda:0')\n",
            "tensor(1.6789, device='cuda:0')\n",
            "tensor(3.8424, device='cuda:0')\n",
            "tensor(0.2777, device='cuda:0')\n",
            "tensor(0.4736, device='cuda:0')\n",
            "tensor(1.7076, device='cuda:0')\n",
            "tensor(0.3414, device='cuda:0')\n",
            "tensor(1.5091, device='cuda:0')\n",
            "tensor(2.2551, device='cuda:0')\n",
            "tensor(0.0619, device='cuda:0')\n",
            "tensor(0.0563, device='cuda:0')\n",
            "tensor(1.1986, device='cuda:0')\n",
            "tensor(0.6016, device='cuda:0')\n",
            "tensor(0.0385, device='cuda:0')\n",
            "tensor(0.0882, device='cuda:0')\n",
            "tensor(5.1610, device='cuda:0')\n",
            "tensor(0.0451, device='cuda:0')\n",
            "tensor(0.7490, device='cuda:0')\n",
            "tensor(0.5988, device='cuda:0')\n",
            "tensor(3.9897, device='cuda:0')\n",
            "tensor(5.1020, device='cuda:0')\n",
            "tensor(0.9494, device='cuda:0')\n",
            "tensor(1.0754, device='cuda:0')\n",
            "tensor(0.1304, device='cuda:0')\n",
            "tensor(0.0211, device='cuda:0')\n",
            "tensor(2.5329, device='cuda:0')\n",
            "tensor(0.0171, device='cuda:0')\n",
            "tensor(3.6664, device='cuda:0')\n",
            "tensor(1.5439, device='cuda:0')\n",
            "tensor(0.0156, device='cuda:0')\n",
            "tensor(0.6958, device='cuda:0')\n",
            "tensor(0.2825, device='cuda:0')\n",
            "tensor(2.7308, device='cuda:0')\n",
            "tensor(0.1804, device='cuda:0')\n",
            "tensor(0.1763, device='cuda:0')\n",
            "tensor(3.6772, device='cuda:0')\n",
            "tensor(0.9268, device='cuda:0')\n",
            "tensor(0.0540, device='cuda:0')\n",
            "tensor(3.3874, device='cuda:0')\n",
            "tensor(0.2905, device='cuda:0')\n",
            "tensor(0.0005, device='cuda:0')\n",
            "tensor(0.1082, device='cuda:0')\n",
            "tensor(0.9289, device='cuda:0')\n",
            "tensor(0.4024, device='cuda:0')\n",
            "tensor(3.8550, device='cuda:0')\n",
            "tensor(0.0839, device='cuda:0')\n",
            "tensor(2.7760, device='cuda:0')\n",
            "tensor(0.5654, device='cuda:0')\n",
            "tensor(2.5631, device='cuda:0')\n",
            "tensor(0.2427, device='cuda:0')\n",
            "tensor(2.2942, device='cuda:0')\n",
            "tensor(0.8978, device='cuda:0')\n",
            "tensor(1.9958, device='cuda:0')\n",
            "tensor(0.3245, device='cuda:0')\n",
            "tensor(0.5550, device='cuda:0')\n",
            "tensor(0.1634, device='cuda:0')\n",
            "tensor(0.6960, device='cuda:0')\n",
            "tensor(0.3389, device='cuda:0')\n",
            "tensor(2.7330, device='cuda:0')\n",
            "tensor(0.7032, device='cuda:0')\n",
            "tensor(1.1078, device='cuda:0')\n",
            "tensor(2.3327, device='cuda:0')\n",
            "tensor(0.3230, device='cuda:0')\n",
            "tensor(0.3034, device='cuda:0')\n",
            "tensor(0.1170, device='cuda:0')\n",
            "tensor(3.1029, device='cuda:0')\n",
            "tensor(0.0074, device='cuda:0')\n",
            "tensor(0.9765, device='cuda:0')\n",
            "tensor(0.0407, device='cuda:0')\n",
            "tensor(0.0060, device='cuda:0')\n",
            "tensor(1.0760, device='cuda:0')\n",
            "tensor(1.1604, device='cuda:0')\n",
            "tensor(1.1267, device='cuda:0')\n",
            "tensor(0.2120, device='cuda:0')\n",
            "tensor(1.1132, device='cuda:0')\n",
            "tensor(1.9407, device='cuda:0')\n",
            "tensor(0.1035, device='cuda:0')\n",
            "tensor(4.0169, device='cuda:0')\n",
            "tensor(1.1860, device='cuda:0')\n",
            "tensor(2.0298, device='cuda:0')\n",
            "tensor(0.3079, device='cuda:0')\n",
            "tensor(0.8457, device='cuda:0')\n",
            "tensor(3.4341, device='cuda:0')\n",
            "tensor(0.2521, device='cuda:0')\n",
            "tensor(0.2004, device='cuda:0')\n",
            "tensor(2.6085, device='cuda:0')\n",
            "tensor(0.3523, device='cuda:0')\n",
            "tensor(0.5511, device='cuda:0')\n",
            "tensor(0.5291, device='cuda:0')\n",
            "tensor(1.3328, device='cuda:0')\n",
            "tensor(0.7935, device='cuda:0')\n",
            "tensor(3.9151, device='cuda:0')\n",
            "tensor(0.4921, device='cuda:0')\n",
            "tensor(0.1419, device='cuda:0')\n",
            "tensor(0.1204, device='cuda:0')\n",
            "tensor(1.4864, device='cuda:0')\n",
            "tensor(0.0091, device='cuda:0')\n",
            "tensor(0.0135, device='cuda:0')\n",
            "tensor(2.9751, device='cuda:0')\n",
            "tensor(1.2196, device='cuda:0')\n",
            "tensor(2.6182, device='cuda:0')\n",
            "tensor(0.2597, device='cuda:0')\n",
            "tensor(0.0596, device='cuda:0')\n",
            "tensor(0.0115, device='cuda:0')\n",
            "tensor(0.0557, device='cuda:0')\n",
            "tensor(1.5420, device='cuda:0')\n",
            "tensor(1.0679, device='cuda:0')\n",
            "tensor(0.2671, device='cuda:0')\n",
            "tensor(0.7684, device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0')\n",
            "tensor(1.0209, device='cuda:0')\n",
            "tensor(9.8105, device='cuda:0')\n",
            "tensor(0.0404, device='cuda:0')\n",
            "tensor(0.4940, device='cuda:0')\n",
            "tensor(2.9065, device='cuda:0')\n",
            "tensor(7.1427, device='cuda:0')\n",
            "tensor(0.0048, device='cuda:0')\n",
            "tensor(0.4856, device='cuda:0')\n",
            "tensor(0.1198, device='cuda:0')\n",
            "tensor(0.4068, device='cuda:0')\n",
            "tensor(0.2645, device='cuda:0')\n",
            "tensor(2.5769, device='cuda:0')\n",
            "tensor(0.2291, device='cuda:0')\n",
            "tensor(1.2372, device='cuda:0')\n",
            "tensor(1.2130, device='cuda:0')\n",
            "tensor(1.2089, device='cuda:0')\n",
            "tensor(0.5227, device='cuda:0')\n",
            "tensor(0.3093, device='cuda:0')\n",
            "tensor(0.0001, device='cuda:0')\n",
            "tensor(0.9261, device='cuda:0')\n",
            "tensor(0.1029, device='cuda:0')\n",
            "tensor(8.4879, device='cuda:0')\n",
            "tensor(0.0030, device='cuda:0')\n",
            "tensor(0.0107, device='cuda:0')\n",
            "tensor(1.2698, device='cuda:0')\n",
            "tensor(0.0052, device='cuda:0')\n",
            "tensor(0.0068, device='cuda:0')\n",
            "tensor(0.2802, device='cuda:0')\n",
            "tensor(0.0958, device='cuda:0')\n",
            "tensor(0.3037, device='cuda:0')\n",
            "tensor(1.7858, device='cuda:0')\n",
            "tensor(0.0384, device='cuda:0')\n",
            "tensor(0.8484, device='cuda:0')\n",
            "tensor(5.6963, device='cuda:0')\n",
            "tensor(0.2133, device='cuda:0')\n",
            "tensor(2.6748, device='cuda:0')\n",
            "tensor(1.0832, device='cuda:0')\n",
            "tensor(3.7423, device='cuda:0')\n",
            "tensor(0.3435, device='cuda:0')\n",
            "tensor(1.0239, device='cuda:0')\n",
            "tensor(2.9605, device='cuda:0')\n",
            "tensor(4.4129, device='cuda:0')\n",
            "tensor(4.4198, device='cuda:0')\n",
            "tensor(3.5774, device='cuda:0')\n",
            "tensor(0.1071, device='cuda:0')\n",
            "tensor(4.0057, device='cuda:0')\n",
            "tensor(1.5402, device='cuda:0')\n",
            "tensor(0.0624, device='cuda:0')\n",
            "tensor(0.0101, device='cuda:0')\n",
            "tensor(5.0061e-05, device='cuda:0')\n",
            "tensor(0.0910, device='cuda:0')\n",
            "tensor(3.0533, device='cuda:0')\n",
            "tensor(2.2317, device='cuda:0')\n",
            "tensor(0.3227, device='cuda:0')\n",
            "tensor(0.0671, device='cuda:0')\n",
            "tensor(1.5383, device='cuda:0')\n",
            "tensor(0.0625, device='cuda:0')\n",
            "tensor(1.4135, device='cuda:0')\n",
            "tensor(0.0206, device='cuda:0')\n",
            "tensor(3.8212, device='cuda:0')\n",
            "tensor(2.1440, device='cuda:0')\n",
            "tensor(0.6496, device='cuda:0')\n",
            "tensor(0.0394, device='cuda:0')\n",
            "tensor(0.0370, device='cuda:0')\n",
            "tensor(2.1725, device='cuda:0')\n",
            "tensor(0.0640, device='cuda:0')\n",
            "tensor(0.9840, device='cuda:0')\n",
            "tensor(0.0486, device='cuda:0')\n",
            "tensor(7.1270, device='cuda:0')\n",
            "tensor(2.1270, device='cuda:0')\n",
            "tensor(2.9921, device='cuda:0')\n",
            "tensor(0.0075, device='cuda:0')\n",
            "tensor(0.0181, device='cuda:0')\n",
            "tensor(1.7959, device='cuda:0')\n",
            "tensor(0.0539, device='cuda:0')\n",
            "tensor(0.1718, device='cuda:0')\n",
            "tensor(0.8526, device='cuda:0')\n",
            "tensor(3.3991, device='cuda:0')\n",
            "tensor(1.2102, device='cuda:0')\n",
            "tensor(0.8483, device='cuda:0')\n",
            "tensor(0.0444, device='cuda:0')\n",
            "tensor(1.1167, device='cuda:0')\n",
            "tensor(0.5993, device='cuda:0')\n",
            "tensor(0.1389, device='cuda:0')\n",
            "tensor(2.0854, device='cuda:0')\n",
            "tensor(0.0929, device='cuda:0')\n",
            "tensor(0.5787, device='cuda:0')\n",
            "tensor(0.0204, device='cuda:0')\n",
            "tensor(1.7160, device='cuda:0')\n",
            "tensor(0.1406, device='cuda:0')\n",
            "tensor(3.0267, device='cuda:0')\n",
            "tensor(0.1172, device='cuda:0')\n",
            "tensor(1.4569, device='cuda:0')\n",
            "tensor(1.6420, device='cuda:0')\n",
            "tensor(0.3999, device='cuda:0')\n",
            "tensor(0.8154, device='cuda:0')\n",
            "tensor(0.0124, device='cuda:0')\n",
            "tensor(0.0430, device='cuda:0')\n",
            "tensor(0.0106, device='cuda:0')\n",
            "tensor(0.1622, device='cuda:0')\n",
            "tensor(0.1404, device='cuda:0')\n",
            "tensor(0.1099, device='cuda:0')\n",
            "tensor(1.1417, device='cuda:0')\n",
            "tensor(0.2271, device='cuda:0')\n",
            "tensor(0.4288, device='cuda:0')\n",
            "tensor(0.3181, device='cuda:0')\n",
            "tensor(3.3206, device='cuda:0')\n",
            "tensor(1.7698, device='cuda:0')\n",
            "tensor(0.1705, device='cuda:0')\n",
            "tensor(1.6664, device='cuda:0')\n",
            "tensor(0.9545, device='cuda:0')\n",
            "tensor(0.1491, device='cuda:0')\n",
            "tensor(2.0161, device='cuda:0')\n",
            "tensor(3.5178, device='cuda:0')\n",
            "tensor(0.0004, device='cuda:0')\n",
            "tensor(2.7983, device='cuda:0')\n",
            "tensor(3.5788, device='cuda:0')\n",
            "tensor(0.2593, device='cuda:0')\n",
            "tensor(0.1703, device='cuda:0')\n",
            "tensor(0.4633, device='cuda:0')\n",
            "tensor(0.9385, device='cuda:0')\n",
            "tensor(0.0421, device='cuda:0')\n",
            "tensor(0.0038, device='cuda:0')\n",
            "tensor(0.0467, device='cuda:0')\n",
            "tensor(0.4878, device='cuda:0')\n",
            "tensor(0.7936, device='cuda:0')\n",
            "tensor(1.9079, device='cuda:0')\n",
            "tensor(7.4553, device='cuda:0')\n",
            "tensor(0.1228, device='cuda:0')\n",
            "tensor(0.2688, device='cuda:0')\n",
            "tensor(0.9660, device='cuda:0')\n",
            "tensor(4.7347, device='cuda:0')\n",
            "tensor(0.9822, device='cuda:0')\n",
            "tensor(1.3528, device='cuda:0')\n",
            "tensor(2.2486, device='cuda:0')\n",
            "tensor(0.0045, device='cuda:0')\n",
            "tensor(0.3899, device='cuda:0')\n",
            "tensor(0.0337, device='cuda:0')\n",
            "tensor(0.0970, device='cuda:0')\n",
            "tensor(0.0924, device='cuda:0')\n",
            "tensor(0.6579, device='cuda:0')\n",
            "tensor(0.5017, device='cuda:0')\n",
            "tensor(0.1566, device='cuda:0')\n",
            "tensor(1.0155, device='cuda:0')\n",
            "tensor(1.2415, device='cuda:0')\n",
            "tensor(6.6156, device='cuda:0')\n",
            "tensor(14.8026, device='cuda:0')\n",
            "tensor(1.8777, device='cuda:0')\n",
            "tensor(0.3432, device='cuda:0')\n",
            "tensor(1.0733, device='cuda:0')\n",
            "tensor(0.4285, device='cuda:0')\n",
            "tensor(5.4732, device='cuda:0')\n",
            "tensor(2.0677, device='cuda:0')\n",
            "tensor(8.4497, device='cuda:0')\n",
            "tensor(1.4292, device='cuda:0')\n",
            "tensor(1.1644, device='cuda:0')\n",
            "tensor(0.6030, device='cuda:0')\n",
            "tensor(1.1608, device='cuda:0')\n",
            "tensor(1.4740, device='cuda:0')\n",
            "tensor(4.7692, device='cuda:0')\n",
            "tensor(0.1183, device='cuda:0')\n",
            "tensor(0.2664, device='cuda:0')\n",
            "tensor(2.8738, device='cuda:0')\n",
            "tensor(1.4588, device='cuda:0')\n",
            "tensor(1.2199, device='cuda:0')\n",
            "tensor(1.5870, device='cuda:0')\n",
            "tensor(0.3337, device='cuda:0')\n",
            "tensor(0.0006, device='cuda:0')\n",
            "tensor(0.0987, device='cuda:0')\n",
            "tensor(4.3935, device='cuda:0')\n",
            "tensor(1.9249, device='cuda:0')\n",
            "tensor(0.0144, device='cuda:0')\n",
            "tensor(1.9648, device='cuda:0')\n",
            "tensor(0.4900, device='cuda:0')\n",
            "tensor(0.0093, device='cuda:0')\n",
            "tensor(7.4753, device='cuda:0')\n",
            "tensor(0.2064, device='cuda:0')\n",
            "tensor(1.8007, device='cuda:0')\n",
            "tensor(2.9288, device='cuda:0')\n",
            "tensor(0.1293, device='cuda:0')\n",
            "tensor(3.3169, device='cuda:0')\n",
            "tensor(0.5576, device='cuda:0')\n",
            "tensor(0.4173, device='cuda:0')\n",
            "tensor(0.1114, device='cuda:0')\n",
            "tensor(1.3150, device='cuda:0')\n",
            "tensor(0.2444, device='cuda:0')\n",
            "tensor(2.2995, device='cuda:0')\n",
            "tensor(0.9366, device='cuda:0')\n",
            "tensor(0.2391, device='cuda:0')\n",
            "tensor(0.0669, device='cuda:0')\n",
            "tensor(1.4613, device='cuda:0')\n",
            "tensor(1.3486, device='cuda:0')\n",
            "tensor(0.0304, device='cuda:0')\n",
            "tensor(0.0093, device='cuda:0')\n",
            "tensor(2.0705, device='cuda:0')\n",
            "tensor(0.1679, device='cuda:0')\n",
            "tensor(3.1831, device='cuda:0')\n",
            "tensor(0.7380, device='cuda:0')\n",
            "tensor(1.8159, device='cuda:0')\n",
            "tensor(3.7087, device='cuda:0')\n",
            "tensor(0.6054, device='cuda:0')\n",
            "tensor(0.2935, device='cuda:0')\n",
            "tensor(4.1782, device='cuda:0')\n",
            "tensor(4.3669, device='cuda:0')\n",
            "tensor(0.9872, device='cuda:0')\n",
            "tensor(0.3005, device='cuda:0')\n",
            "tensor(0.0037, device='cuda:0')\n",
            "tensor(0.0013, device='cuda:0')\n",
            "tensor(0.2936, device='cuda:0')\n",
            "tensor(0.3544, device='cuda:0')\n",
            "tensor(0.0724, device='cuda:0')\n",
            "tensor(0.0008, device='cuda:0')\n",
            "tensor(0.6957, device='cuda:0')\n",
            "tensor(1.2209, device='cuda:0')\n",
            "tensor(0.0070, device='cuda:0')\n",
            "tensor(0.0380, device='cuda:0')\n",
            "tensor(1.2043, device='cuda:0')\n",
            "tensor(0.0971, device='cuda:0')\n",
            "tensor(0.4764, device='cuda:0')\n",
            "tensor(0.9825, device='cuda:0')\n",
            "tensor(0.8765, device='cuda:0')\n",
            "tensor(3.4294, device='cuda:0')\n",
            "tensor(0.3663, device='cuda:0')\n",
            "tensor(1.2272, device='cuda:0')\n",
            "tensor(1.7889, device='cuda:0')\n",
            "tensor(0.0198, device='cuda:0')\n",
            "tensor(0.0365, device='cuda:0')\n",
            "tensor(1.7226, device='cuda:0')\n",
            "tensor(0.0399, device='cuda:0')\n",
            "tensor(0.2170, device='cuda:0')\n",
            "tensor(2.5680, device='cuda:0')\n",
            "tensor(0.3198, device='cuda:0')\n",
            "tensor(0.7462, device='cuda:0')\n",
            "tensor(0.5014, device='cuda:0')\n",
            "tensor(2.7115, device='cuda:0')\n",
            "tensor(4.7331, device='cuda:0')\n",
            "tensor(0.1932, device='cuda:0')\n",
            "tensor(0.2450, device='cuda:0')\n",
            "tensor(3.0538, device='cuda:0')\n",
            "tensor(0.0070, device='cuda:0')\n",
            "tensor(0.0024, device='cuda:0')\n",
            "tensor(1.9748, device='cuda:0')\n",
            "tensor(1.1770, device='cuda:0')\n",
            "tensor(1.3211, device='cuda:0')\n",
            "tensor(0.7678, device='cuda:0')\n",
            "tensor(0.2923, device='cuda:0')\n",
            "tensor(3.2573e-05, device='cuda:0')\n",
            "tensor(0.5666, device='cuda:0')\n",
            "tensor(1.6925, device='cuda:0')\n",
            "tensor(1.3047, device='cuda:0')\n",
            "tensor(0.3826, device='cuda:0')\n",
            "tensor(5.5127, device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0')\n",
            "tensor(2.2694, device='cuda:0')\n",
            "tensor(1.4591, device='cuda:0')\n",
            "tensor(0.0637, device='cuda:0')\n",
            "tensor(1.7100, device='cuda:0')\n",
            "tensor(0.2122, device='cuda:0')\n",
            "tensor(0.8278, device='cuda:0')\n",
            "tensor(3.7542, device='cuda:0')\n",
            "tensor(0.1502, device='cuda:0')\n",
            "tensor(1.6788, device='cuda:0')\n",
            "tensor(1.3914, device='cuda:0')\n",
            "tensor(0.3261, device='cuda:0')\n",
            "tensor(0.0147, device='cuda:0')\n",
            "tensor(0.3910, device='cuda:0')\n",
            "tensor(2.9191, device='cuda:0')\n",
            "tensor(1.1816, device='cuda:0')\n",
            "tensor(5.5946, device='cuda:0')\n",
            "tensor(5.4404, device='cuda:0')\n",
            "tensor(0.5900, device='cuda:0')\n",
            "tensor(1.5654, device='cuda:0')\n",
            "tensor(0.0613, device='cuda:0')\n",
            "tensor(4.6066, device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0')\n",
            "tensor(0.6969, device='cuda:0')\n",
            "tensor(2.1188, device='cuda:0')\n",
            "tensor(0.2574, device='cuda:0')\n",
            "tensor(0.5311, device='cuda:0')\n",
            "tensor(2.6359, device='cuda:0')\n",
            "tensor(0.0614, device='cuda:0')\n",
            "tensor(0.2212, device='cuda:0')\n",
            "tensor(0.6031, device='cuda:0')\n",
            "tensor(0.2183, device='cuda:0')\n",
            "tensor(0.1733, device='cuda:0')\n",
            "tensor(0.9031, device='cuda:0')\n",
            "tensor(0.1501, device='cuda:0')\n",
            "tensor(0.1053, device='cuda:0')\n",
            "tensor(0.2455, device='cuda:0')\n",
            "tensor(0.1671, device='cuda:0')\n",
            "tensor(3.3916, device='cuda:0')\n",
            "tensor(0.1100, device='cuda:0')\n",
            "tensor(0.2784, device='cuda:0')\n",
            "tensor(1.1145, device='cuda:0')\n",
            "tensor(0.0362, device='cuda:0')\n",
            "tensor(1.4355, device='cuda:0')\n",
            "tensor(0.0006, device='cuda:0')\n",
            "tensor(1.3054, device='cuda:0')\n",
            "tensor(0.1137, device='cuda:0')\n",
            "tensor(4.3567, device='cuda:0')\n",
            "tensor(0.5752, device='cuda:0')\n",
            "tensor(0.0951, device='cuda:0')\n",
            "tensor(1.5310, device='cuda:0')\n",
            "tensor(4.5287, device='cuda:0')\n",
            "tensor(0.0074, device='cuda:0')\n",
            "tensor(0.0023, device='cuda:0')\n",
            "tensor(1.2732, device='cuda:0')\n",
            "tensor(0.7941, device='cuda:0')\n",
            "tensor(0.1769, device='cuda:0')\n",
            "tensor(1.9436, device='cuda:0')\n",
            "tensor(0.6700, device='cuda:0')\n",
            "tensor(0.1371, device='cuda:0')\n",
            "tensor(0.0448, device='cuda:0')\n",
            "tensor(0.2023, device='cuda:0')\n",
            "tensor(0.0091, device='cuda:0')\n",
            "tensor(0.2260, device='cuda:0')\n",
            "tensor(0.0141, device='cuda:0')\n",
            "tensor(0.5688, device='cuda:0')\n",
            "tensor(1.2869, device='cuda:0')\n",
            "tensor(0.0745, device='cuda:0')\n",
            "tensor(0.0439, device='cuda:0')\n",
            "tensor(0.0891, device='cuda:0')\n",
            "tensor(3.6584, device='cuda:0')\n",
            "tensor(0.3260, device='cuda:0')\n",
            "tensor(0.7544, device='cuda:0')\n",
            "tensor(0.1559, device='cuda:0')\n",
            "tensor(0.1100, device='cuda:0')\n",
            "tensor(0.0436, device='cuda:0')\n",
            "tensor(0.2772, device='cuda:0')\n",
            "tensor(0.0169, device='cuda:0')\n",
            "tensor(0.1305, device='cuda:0')\n",
            "tensor(0.2183, device='cuda:0')\n",
            "tensor(0.3401, device='cuda:0')\n",
            "tensor(1.4325, device='cuda:0')\n",
            "tensor(0.8941, device='cuda:0')\n",
            "tensor(0.1206, device='cuda:0')\n",
            "tensor(0.6075, device='cuda:0')\n",
            "tensor(1.1129, device='cuda:0')\n",
            "tensor(0.5455, device='cuda:0')\n",
            "tensor(6.7046, device='cuda:0')\n",
            "tensor(0.0171, device='cuda:0')\n",
            "tensor(0.4474, device='cuda:0')\n",
            "tensor(0.5872, device='cuda:0')\n",
            "tensor(0.5192, device='cuda:0')\n",
            "tensor(2.5123, device='cuda:0')\n",
            "tensor(5.4792, device='cuda:0')\n",
            "tensor(3.9102, device='cuda:0')\n",
            "tensor(0.9265, device='cuda:0')\n",
            "tensor(0.1037, device='cuda:0')\n",
            "tensor(2.7875, device='cuda:0')\n",
            "tensor(0.0580, device='cuda:0')\n",
            "tensor(1.3654, device='cuda:0')\n",
            "tensor(0.0123, device='cuda:0')\n",
            "tensor(0.3874, device='cuda:0')\n",
            "tensor(0.0383, device='cuda:0')\n",
            "tensor(0.1386, device='cuda:0')\n",
            "tensor(0.0084, device='cuda:0')\n",
            "tensor(2.0647, device='cuda:0')\n",
            "tensor(1.5743, device='cuda:0')\n",
            "tensor(0.0205, device='cuda:0')\n",
            "tensor(0.6952, device='cuda:0')\n",
            "tensor(0.0776, device='cuda:0')\n",
            "tensor(3.3992, device='cuda:0')\n",
            "tensor(11.6761, device='cuda:0')\n",
            "tensor(10.7613, device='cuda:0')\n",
            "tensor(0.1820, device='cuda:0')\n",
            "tensor(0.5855, device='cuda:0')\n",
            "tensor(0.8863, device='cuda:0')\n",
            "tensor(5.2734, device='cuda:0')\n",
            "tensor(0.2360, device='cuda:0')\n",
            "tensor(0.5078, device='cuda:0')\n",
            "tensor(0.0099, device='cuda:0')\n",
            "tensor(0.5569, device='cuda:0')\n",
            "tensor(8.1206e-05, device='cuda:0')\n",
            "tensor(0.3488, device='cuda:0')\n",
            "tensor(0.0027, device='cuda:0')\n",
            "tensor(0.0197, device='cuda:0')\n",
            "tensor(0.0696, device='cuda:0')\n",
            "tensor(0.0611, device='cuda:0')\n",
            "tensor(0.0161, device='cuda:0')\n",
            "tensor(2.7772, device='cuda:0')\n",
            "tensor(0.1831, device='cuda:0')\n",
            "tensor(0.1597, device='cuda:0')\n",
            "tensor(0.5881, device='cuda:0')\n",
            "tensor(1.2489, device='cuda:0')\n",
            "tensor(0.0397, device='cuda:0')\n",
            "tensor(4.7939, device='cuda:0')\n",
            "tensor(5.1347, device='cuda:0')\n",
            "tensor(8.3701, device='cuda:0')\n",
            "tensor(2.0338, device='cuda:0')\n",
            "tensor(2.5474, device='cuda:0')\n",
            "tensor(0.0543, device='cuda:0')\n",
            "tensor(1.7830, device='cuda:0')\n",
            "tensor(4.6787, device='cuda:0')\n",
            "tensor(2.4748, device='cuda:0')\n",
            "tensor(0.0379, device='cuda:0')\n",
            "tensor(0.3610, device='cuda:0')\n",
            "tensor(1.1642, device='cuda:0')\n",
            "tensor(0.7664, device='cuda:0')\n",
            "tensor(1.4822, device='cuda:0')\n",
            "tensor(0.0021, device='cuda:0')\n",
            "tensor(0.0609, device='cuda:0')\n",
            "tensor(0.0021, device='cuda:0')\n",
            "tensor(2.8054, device='cuda:0')\n",
            "tensor(0.3319, device='cuda:0')\n",
            "tensor(0.1084, device='cuda:0')\n",
            "tensor(0.3101, device='cuda:0')\n",
            "tensor(0.0544, device='cuda:0')\n",
            "tensor(0.7109, device='cuda:0')\n",
            "tensor(0.2417, device='cuda:0')\n",
            "tensor(0.0949, device='cuda:0')\n",
            "tensor(0.1266, device='cuda:0')\n",
            "tensor(0.6595, device='cuda:0')\n",
            "tensor(0.2438, device='cuda:0')\n",
            "tensor(1.0060, device='cuda:0')\n",
            "tensor(0.0120, device='cuda:0')\n",
            "tensor(0.0092, device='cuda:0')\n",
            "tensor(0.8501, device='cuda:0')\n",
            "tensor(0.1963, device='cuda:0')\n",
            "tensor(1.7082, device='cuda:0')\n",
            "tensor(0.8693, device='cuda:0')\n",
            "tensor(0.0121, device='cuda:0')\n",
            "tensor(0.1677, device='cuda:0')\n",
            "tensor(0.9903, device='cuda:0')\n",
            "tensor(0.0975, device='cuda:0')\n",
            "tensor(1.4307, device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0')\n",
            "tensor(0.9244, device='cuda:0')\n",
            "tensor(0.4572, device='cuda:0')\n",
            "tensor(1.0404, device='cuda:0')\n",
            "tensor(0.4055, device='cuda:0')\n",
            "tensor(0.7873, device='cuda:0')\n",
            "tensor(0.2810, device='cuda:0')\n",
            "tensor(0.3174, device='cuda:0')\n",
            "tensor(0.3934, device='cuda:0')\n",
            "tensor(1.5180, device='cuda:0')\n",
            "tensor(0.0602, device='cuda:0')\n",
            "tensor(5.8684e-07, device='cuda:0')\n",
            "tensor(14.3969, device='cuda:0')\n",
            "tensor(0.3925, device='cuda:0')\n",
            "tensor(3.1476, device='cuda:0')\n",
            "tensor(0.4110, device='cuda:0')\n",
            "tensor(1.8862, device='cuda:0')\n",
            "tensor(1.8356, device='cuda:0')\n",
            "tensor(0.0895, device='cuda:0')\n",
            "tensor(0.0191, device='cuda:0')\n",
            "tensor(0.5468, device='cuda:0')\n",
            "tensor(0.1815, device='cuda:0')\n",
            "tensor(0.3238, device='cuda:0')\n",
            "tensor(0.6972, device='cuda:0')\n",
            "tensor(0.4244, device='cuda:0')\n",
            "tensor(0.6653, device='cuda:0')\n",
            "tensor(1.0305, device='cuda:0')\n",
            "tensor(0.0035, device='cuda:0')\n",
            "tensor(3.9578, device='cuda:0')\n",
            "tensor(0.0292, device='cuda:0')\n",
            "tensor(0.8569, device='cuda:0')\n",
            "tensor(0.2603, device='cuda:0')\n",
            "tensor(0.3317, device='cuda:0')\n",
            "tensor(0.0204, device='cuda:0')\n",
            "tensor(1.2507, device='cuda:0')\n",
            "tensor(5.8152, device='cuda:0')\n",
            "tensor(4.5786, device='cuda:0')\n",
            "tensor(0.1205, device='cuda:0')\n",
            "tensor(0.5911, device='cuda:0')\n",
            "tensor(0.1984, device='cuda:0')\n",
            "tensor(3.5398, device='cuda:0')\n",
            "tensor(0.0721, device='cuda:0')\n",
            "tensor(0.0697, device='cuda:0')\n",
            "tensor(0.6218, device='cuda:0')\n",
            "tensor(8.5712, device='cuda:0')\n",
            "tensor(0.0363, device='cuda:0')\n",
            "tensor(0.0324, device='cuda:0')\n",
            "tensor(0.1833, device='cuda:0')\n",
            "tensor(0.0052, device='cuda:0')\n",
            "tensor(1.7163, device='cuda:0')\n",
            "tensor(2.9725e-06, device='cuda:0')\n",
            "tensor(0.4251, device='cuda:0')\n",
            "tensor(2.8061, device='cuda:0')\n",
            "tensor(3.6965, device='cuda:0')\n",
            "tensor(0.0167, device='cuda:0')\n",
            "tensor(0.1485, device='cuda:0')\n",
            "tensor(1.4520, device='cuda:0')\n",
            "tensor(0.1610, device='cuda:0')\n",
            "tensor(0.8609, device='cuda:0')\n",
            "tensor(3.7453, device='cuda:0')\n",
            "tensor(0.1210, device='cuda:0')\n",
            "tensor(1.4673, device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0')\n",
            "tensor(0.1440, device='cuda:0')\n",
            "tensor(0.2466, device='cuda:0')\n",
            "tensor(0.3958, device='cuda:0')\n",
            "tensor(1.5852, device='cuda:0')\n",
            "tensor(1.6124, device='cuda:0')\n",
            "tensor(0.1136, device='cuda:0')\n",
            "tensor(0.4776, device='cuda:0')\n",
            "tensor(0.1317, device='cuda:0')\n",
            "tensor(0.1142, device='cuda:0')\n",
            "tensor(5.2928, device='cuda:0')\n",
            "tensor(0.3038, device='cuda:0')\n",
            "tensor(0.4756, device='cuda:0')\n",
            "tensor(2.4387, device='cuda:0')\n",
            "tensor(1.9590, device='cuda:0')\n",
            "tensor(1.8149, device='cuda:0')\n",
            "tensor(1.3512, device='cuda:0')\n",
            "tensor(2.2429e-06, device='cuda:0')\n",
            "tensor(0.1562, device='cuda:0')\n",
            "tensor(2.9947, device='cuda:0')\n",
            "tensor(0.6468, device='cuda:0')\n",
            "tensor(2.6651, device='cuda:0')\n",
            "tensor(0.2387, device='cuda:0')\n",
            "tensor(3.2049, device='cuda:0')\n",
            "tensor(0.1292, device='cuda:0')\n",
            "tensor(4.0609, device='cuda:0')\n",
            "tensor(0.7157, device='cuda:0')\n",
            "tensor(4.2051, device='cuda:0')\n",
            "tensor(0.0268, device='cuda:0')\n",
            "tensor(3.6005, device='cuda:0')\n",
            "tensor(0.7536, device='cuda:0')\n",
            "tensor(0.0149, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.0010, device='cuda:0')\n",
            "tensor(0.1089, device='cuda:0')\n",
            "tensor(3.2553, device='cuda:0')\n",
            "tensor(0.2055, device='cuda:0')\n",
            "tensor(3.5117, device='cuda:0')\n",
            "tensor(0.9237, device='cuda:0')\n",
            "tensor(0.8281, device='cuda:0')\n",
            "tensor(0.9511, device='cuda:0')\n",
            "tensor(0.2724, device='cuda:0')\n",
            "tensor(1.8264, device='cuda:0')\n",
            "tensor(0.0621, device='cuda:0')\n",
            "tensor(0.7160, device='cuda:0')\n",
            "tensor(0.2588, device='cuda:0')\n",
            "tensor(0.7122, device='cuda:0')\n",
            "tensor(2.7350, device='cuda:0')\n",
            "tensor(0.5161, device='cuda:0')\n",
            "tensor(1.7993, device='cuda:0')\n",
            "tensor(0.1940, device='cuda:0')\n",
            "tensor(1.0229, device='cuda:0')\n",
            "tensor(0.7747, device='cuda:0')\n",
            "tensor(0.8586, device='cuda:0')\n",
            "tensor(0.0435, device='cuda:0')\n",
            "tensor(1.7110, device='cuda:0')\n",
            "tensor(0.4553, device='cuda:0')\n",
            "tensor(0.1456, device='cuda:0')\n",
            "tensor(3.8352, device='cuda:0')\n",
            "tensor(1.6315, device='cuda:0')\n",
            "tensor(1.2855, device='cuda:0')\n",
            "tensor(0.9848, device='cuda:0')\n",
            "tensor(1.4589, device='cuda:0')\n",
            "tensor(0.2720, device='cuda:0')\n",
            "tensor(1.8684, device='cuda:0')\n",
            "tensor(1.4390, device='cuda:0')\n",
            "tensor(0.0721, device='cuda:0')\n",
            "tensor(1.5879, device='cuda:0')\n",
            "tensor(1.6608, device='cuda:0')\n",
            "tensor(0.7377, device='cuda:0')\n",
            "tensor(1.2740, device='cuda:0')\n",
            "tensor(0.1334, device='cuda:0')\n",
            "tensor(3.1044, device='cuda:0')\n",
            "tensor(0.5692, device='cuda:0')\n",
            "tensor(0.4509, device='cuda:0')\n",
            "tensor(0.1929, device='cuda:0')\n",
            "tensor(4.3264, device='cuda:0')\n",
            "tensor(1.6849, device='cuda:0')\n",
            "tensor(0.2089, device='cuda:0')\n",
            "tensor(2.3199, device='cuda:0')\n",
            "tensor(0.1521, device='cuda:0')\n",
            "tensor(11.5120, device='cuda:0')\n",
            "tensor(1.2469, device='cuda:0')\n",
            "tensor(0.9430, device='cuda:0')\n",
            "tensor(0.1663, device='cuda:0')\n",
            "tensor(0.2045, device='cuda:0')\n",
            "tensor(1.6683, device='cuda:0')\n",
            "tensor(5.2370, device='cuda:0')\n",
            "tensor(2.6224, device='cuda:0')\n",
            "tensor(0.0149, device='cuda:0')\n",
            "tensor(0.9292, device='cuda:0')\n",
            "tensor(0.4092, device='cuda:0')\n",
            "tensor(0.8992, device='cuda:0')\n",
            "tensor(0.0632, device='cuda:0')\n",
            "tensor(0.6954, device='cuda:0')\n",
            "tensor(0.3355, device='cuda:0')\n",
            "tensor(2.9721, device='cuda:0')\n",
            "tensor(0.3823, device='cuda:0')\n",
            "tensor(0.9947, device='cuda:0')\n",
            "tensor(0.4538, device='cuda:0')\n",
            "tensor(2.7849, device='cuda:0')\n",
            "tensor(0.3088, device='cuda:0')\n",
            "tensor(0.0894, device='cuda:0')\n",
            "tensor(1.5684, device='cuda:0')\n",
            "tensor(4.4632, device='cuda:0')\n",
            "tensor(0.2897, device='cuda:0')\n",
            "tensor(0.2598, device='cuda:0')\n",
            "tensor(0.1238, device='cuda:0')\n",
            "tensor(1.0248, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.5122, device='cuda:0')\n",
            "tensor(0.0020, device='cuda:0')\n",
            "tensor(1.8341, device='cuda:0')\n",
            "tensor(0.3778, device='cuda:0')\n",
            "tensor(0.3655, device='cuda:0')\n",
            "tensor(0.1685, device='cuda:0')\n",
            "tensor(3.6120, device='cuda:0')\n",
            "tensor(0.0575, device='cuda:0')\n",
            "tensor(2.8328, device='cuda:0')\n",
            "tensor(0.3205, device='cuda:0')\n",
            "tensor(0.0068, device='cuda:0')\n",
            "tensor(3.3781, device='cuda:0')\n",
            "tensor(7.1987, device='cuda:0')\n",
            "tensor(0.4284, device='cuda:0')\n",
            "tensor(0.0167, device='cuda:0')\n",
            "tensor(1.4674, device='cuda:0')\n",
            "tensor(0.6130, device='cuda:0')\n",
            "tensor(0.0940, device='cuda:0')\n",
            "tensor(0.0568, device='cuda:0')\n",
            "tensor(3.4683, device='cuda:0')\n",
            "tensor(0.0870, device='cuda:0')\n",
            "tensor(0.9697, device='cuda:0')\n",
            "tensor(0.3299, device='cuda:0')\n",
            "tensor(2.1074, device='cuda:0')\n",
            "tensor(0.3207, device='cuda:0')\n",
            "tensor(0.0147, device='cuda:0')\n",
            "tensor(1.4085, device='cuda:0')\n",
            "tensor(1.5826, device='cuda:0')\n",
            "tensor(0.0047, device='cuda:0')\n",
            "tensor(2.5069, device='cuda:0')\n",
            "tensor(0.0691, device='cuda:0')\n",
            "tensor(1.1064, device='cuda:0')\n",
            "tensor(0.2263, device='cuda:0')\n",
            "tensor(1.8238, device='cuda:0')\n",
            "tensor(0.0538, device='cuda:0')\n",
            "tensor(0.0173, device='cuda:0')\n",
            "tensor(0.0337, device='cuda:0')\n",
            "tensor(5.8708, device='cuda:0')\n",
            "tensor(0.4396, device='cuda:0')\n",
            "tensor(0.6966, device='cuda:0')\n",
            "tensor(0.1068, device='cuda:0')\n",
            "tensor(0.0011, device='cuda:0')\n",
            "tensor(0.1588, device='cuda:0')\n",
            "tensor(6.2791, device='cuda:0')\n",
            "tensor(3.1270, device='cuda:0')\n",
            "tensor(1.7436, device='cuda:0')\n",
            "tensor(0.0003, device='cuda:0')\n",
            "tensor(6.2179, device='cuda:0')\n",
            "tensor(1.4414, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.2303, device='cuda:0')\n",
            "tensor(0.9920, device='cuda:0')\n",
            "tensor(2.5961, device='cuda:0')\n",
            "tensor(4.4173, device='cuda:0')\n",
            "tensor(3.2719, device='cuda:0')\n",
            "tensor(2.2285, device='cuda:0')\n",
            "tensor(0.9081, device='cuda:0')\n",
            "tensor(0.8735, device='cuda:0')\n",
            "tensor(0.0791, device='cuda:0')\n",
            "tensor(0.0020, device='cuda:0')\n",
            "tensor(2.7323, device='cuda:0')\n",
            "tensor(1.8654, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0')\n",
            "tensor(0.3032, device='cuda:0')\n",
            "tensor(0.1425, device='cuda:0')\n",
            "tensor(0.2613, device='cuda:0')\n",
            "tensor(0.0956, device='cuda:0')\n",
            "tensor(0.3319, device='cuda:0')\n",
            "tensor(0.0392, device='cuda:0')\n",
            "tensor(3.4988, device='cuda:0')\n",
            "tensor(0.2112, device='cuda:0')\n",
            "tensor(0.5481, device='cuda:0')\n",
            "tensor(0.2231, device='cuda:0')\n",
            "tensor(0.0093, device='cuda:0')\n",
            "tensor(0.0826, device='cuda:0')\n",
            "tensor(0.9824, device='cuda:0')\n",
            "tensor(3.9158, device='cuda:0')\n",
            "tensor(0.3031, device='cuda:0')\n",
            "tensor(0.3073, device='cuda:0')\n",
            "tensor(0.0075, device='cuda:0')\n",
            "tensor(0.0381, device='cuda:0')\n",
            "tensor(1.7468, device='cuda:0')\n",
            "tensor(0.5685, device='cuda:0')\n",
            "tensor(0.5098, device='cuda:0')\n",
            "tensor(0.9979, device='cuda:0')\n",
            "tensor(0.3296, device='cuda:0')\n",
            "tensor(5.4638, device='cuda:0')\n",
            "tensor(0.8774, device='cuda:0')\n",
            "tensor(1.8878, device='cuda:0')\n",
            "tensor(1.4937, device='cuda:0')\n",
            "tensor(0.0953, device='cuda:0')\n",
            "tensor(0.5538, device='cuda:0')\n",
            "tensor(0.5825, device='cuda:0')\n",
            "tensor(2.9899, device='cuda:0')\n",
            "tensor(0.1970, device='cuda:0')\n",
            "tensor(1.7101, device='cuda:0')\n",
            "tensor(0.4566, device='cuda:0')\n",
            "tensor(0.0221, device='cuda:0')\n",
            "tensor(5.8533, device='cuda:0')\n",
            "tensor(3.6293, device='cuda:0')\n",
            "tensor(0.0343, device='cuda:0')\n",
            "tensor(0.2361, device='cuda:0')\n",
            "tensor(0.4710, device='cuda:0')\n",
            "tensor(0.0738, device='cuda:0')\n",
            "tensor(0.1616, device='cuda:0')\n",
            "tensor(4.9131, device='cuda:0')\n",
            "tensor(0.0022, device='cuda:0')\n",
            "tensor(2.4091, device='cuda:0')\n",
            "tensor(0.2029, device='cuda:0')\n",
            "tensor(2.7288, device='cuda:0')\n",
            "tensor(0.0826, device='cuda:0')\n",
            "tensor(1.6247, device='cuda:0')\n",
            "tensor(0.0048, device='cuda:0')\n",
            "tensor(0.2896, device='cuda:0')\n",
            "tensor(0.8373, device='cuda:0')\n",
            "tensor(3.1290, device='cuda:0')\n",
            "tensor(0.5131, device='cuda:0')\n",
            "tensor(2.3229, device='cuda:0')\n",
            "tensor(0.7841, device='cuda:0')\n",
            "tensor(1.5136, device='cuda:0')\n",
            "tensor(0.0521, device='cuda:0')\n",
            "tensor(1.2539, device='cuda:0')\n",
            "tensor(0.4838, device='cuda:0')\n",
            "tensor(0.0378, device='cuda:0')\n",
            "tensor(0.2114, device='cuda:0')\n",
            "tensor(0.0797, device='cuda:0')\n",
            "tensor(1.3457, device='cuda:0')\n",
            "tensor(0.2550, device='cuda:0')\n",
            "tensor(0.5363, device='cuda:0')\n",
            "tensor(0.0294, device='cuda:0')\n",
            "tensor(0.0314, device='cuda:0')\n",
            "tensor(0.0181, device='cuda:0')\n",
            "tensor(0.4034, device='cuda:0')\n",
            "tensor(0.3132, device='cuda:0')\n",
            "tensor(4.1430, device='cuda:0')\n",
            "tensor(4.2748, device='cuda:0')\n",
            "tensor(1.3358, device='cuda:0')\n",
            "tensor(0.6595, device='cuda:0')\n",
            "tensor(0.0115, device='cuda:0')\n",
            "tensor(0.8497, device='cuda:0')\n",
            "tensor(2.6574, device='cuda:0')\n",
            "tensor(1.5612, device='cuda:0')\n",
            "tensor(1.5125, device='cuda:0')\n",
            "tensor(3.5381, device='cuda:0')\n",
            "tensor(0.4416, device='cuda:0')\n",
            "tensor(0.2624, device='cuda:0')\n",
            "tensor(0.6332, device='cuda:0')\n",
            "tensor(2.2372, device='cuda:0')\n",
            "tensor(0.0132, device='cuda:0')\n",
            "tensor(0.1031, device='cuda:0')\n",
            "tensor(1.2816, device='cuda:0')\n",
            "tensor(0.8955, device='cuda:0')\n",
            "tensor(1.8287, device='cuda:0')\n",
            "tensor(1.9612, device='cuda:0')\n",
            "tensor(0.3430, device='cuda:0')\n",
            "tensor(0.5103, device='cuda:0')\n",
            "tensor(3.6303, device='cuda:0')\n",
            "tensor(1.0337, device='cuda:0')\n",
            "tensor(0.0124, device='cuda:0')\n",
            "tensor(0.8596, device='cuda:0')\n",
            "tensor(0.0353, device='cuda:0')\n",
            "tensor(1.3563, device='cuda:0')\n",
            "tensor(1.9526, device='cuda:0')\n",
            "tensor(2.0773, device='cuda:0')\n",
            "tensor(1.4596e-05, device='cuda:0')\n",
            "tensor(0.6926, device='cuda:0')\n",
            "tensor(0.4505, device='cuda:0')\n",
            "tensor(1.1637, device='cuda:0')\n",
            "tensor(1.0343, device='cuda:0')\n",
            "tensor(0.6445, device='cuda:0')\n",
            "tensor(0.0201, device='cuda:0')\n",
            "tensor(0.0756, device='cuda:0')\n",
            "tensor(0.3708, device='cuda:0')\n",
            "tensor(0.0297, device='cuda:0')\n",
            "tensor(2.6862, device='cuda:0')\n",
            "tensor(0.1124, device='cuda:0')\n",
            "tensor(1.7411, device='cuda:0')\n",
            "tensor(0.1725, device='cuda:0')\n",
            "tensor(0.0626, device='cuda:0')\n",
            "tensor(0.9005, device='cuda:0')\n",
            "tensor(0.8328, device='cuda:0')\n",
            "tensor(4.6329, device='cuda:0')\n",
            "tensor(0.8976, device='cuda:0')\n",
            "tensor(5.9624, device='cuda:0')\n",
            "tensor(1.1771, device='cuda:0')\n",
            "tensor(0.4849, device='cuda:0')\n",
            "tensor(5.2306, device='cuda:0')\n",
            "tensor(0.0098, device='cuda:0')\n",
            "tensor(0.1761, device='cuda:0')\n",
            "tensor(0.6709, device='cuda:0')\n",
            "tensor(0.5929, device='cuda:0')\n",
            "tensor(4.3154, device='cuda:0')\n",
            "tensor(1.8200, device='cuda:0')\n",
            "tensor(0.6173, device='cuda:0')\n",
            "tensor(4.0762, device='cuda:0')\n",
            "tensor(0.1123, device='cuda:0')\n",
            "tensor(0.6841, device='cuda:0')\n",
            "tensor(0.0212, device='cuda:0')\n",
            "tensor(0.4662, device='cuda:0')\n",
            "tensor(0.0018, device='cuda:0')\n",
            "tensor(0.3974, device='cuda:0')\n",
            "tensor(5.4414, device='cuda:0')\n",
            "tensor(1.9934, device='cuda:0')\n",
            "tensor(0.8820, device='cuda:0')\n",
            "tensor(2.4256, device='cuda:0')\n",
            "tensor(3.6844, device='cuda:0')\n",
            "tensor(1.6836, device='cuda:0')\n",
            "tensor(0.6939, device='cuda:0')\n",
            "tensor(2.9817, device='cuda:0')\n",
            "tensor(0.1674, device='cuda:0')\n",
            "tensor(0.0059, device='cuda:0')\n",
            "tensor(0.8592, device='cuda:0')\n",
            "tensor(0.0904, device='cuda:0')\n",
            "tensor(0.1213, device='cuda:0')\n",
            "tensor(0.0427, device='cuda:0')\n",
            "tensor(1.1777, device='cuda:0')\n",
            "tensor(0.2724, device='cuda:0')\n",
            "tensor(0.7703, device='cuda:0')\n",
            "tensor(0.0998, device='cuda:0')\n",
            "tensor(1.5846, device='cuda:0')\n",
            "tensor(0.0017, device='cuda:0')\n",
            "tensor(0.0948, device='cuda:0')\n",
            "tensor(0.0213, device='cuda:0')\n",
            "tensor(0.0119, device='cuda:0')\n",
            "tensor(1.3274, device='cuda:0')\n",
            "tensor(0.6734, device='cuda:0')\n",
            "tensor(0.0120, device='cuda:0')\n",
            "tensor(0.8714, device='cuda:0')\n",
            "tensor(0.8016, device='cuda:0')\n",
            "tensor(1.5577, device='cuda:0')\n",
            "tensor(0.0139, device='cuda:0')\n",
            "tensor(0.6738, device='cuda:0')\n",
            "tensor(1.8204, device='cuda:0')\n",
            "tensor(0.1501, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assume model is your neural network and loss_fn is your loss function\n",
        "# Forward pass\n",
        "outputs1 = zero_loss_model(X[:2,]).detach()\n",
        "outputs2 = zero_loss_model(X[:2,])\n",
        "loss = criterion(outputs1, outputs2)\n",
        "\n",
        "\n",
        "# Step 1: Compute the gradient of the loss w.r.t. model parameters\n",
        "# This will compute the first-order derivatives\n",
        "first_order_grads = torch.autograd.grad(loss, zero_loss_model.parameters(), create_graph=True)\n",
        "\n",
        "\n",
        "# Flatten the gradients and parameters into vectors\n",
        "flat_grads = torch.cat([g.view(-1) for g in first_order_grads])\n",
        "\n",
        "# Step 2: Define a direction in parameter space (vector v)\n",
        "# You can set v to a random vector or a specific direction\n",
        "# Here, we use a random vector of the same size as the flattened gradient\n",
        "for i in range(2000):\n",
        "  v = torch.randn_like(flat_grads)\n",
        "\n",
        "  # Step 3: Compute the dot product of the gradient and the direction vector\n",
        "  grad_dot_v = torch.sum(flat_grads * v)\n",
        "\n",
        "  # Step 4: Compute the Hessian-vector product (second-order derivative)\n",
        "  hessian_vec = torch.autograd.grad(grad_dot_v, zero_loss_model.parameters(), retain_graph=True)\n",
        "\n",
        "  # Flatten the Hessian-vector product to make it easier to analyze\n",
        "  flat_hessian_vec = v @ torch.cat([hv.view(-1) for hv in hessian_vec])\n",
        "\n",
        "  # The variable `flat_hessian_vec` now contains the second derivative in the direction of `v`\n",
        "  print(flat_hessian_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st-bRrEsMKkL"
      },
      "outputs": [],
      "source": [
        "#@title partial hessian\n",
        "\n",
        "# Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "grads = torch.autograd.grad(loss, zero_loss_model.parameters(), create_graph=True)\n",
        "\n",
        "# Flatten the gradients\n",
        "grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "# Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "Hv = torch.autograd.grad(grads_flat @ v, zero_loss_model.parameters(), retain_graph=True)\n",
        "\n",
        "# Flatten the Hessian-vector product\n",
        "Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "# Step 4: Compute the second-order directional derivative\n",
        "second_order_derivative = (v * Hv_flat).sum()\n",
        "\n",
        "\n",
        "\n",
        "def HessianBatch(model, X, criterion):\n",
        "  hessians = np.array([])\n",
        "  # Compute the loss\n",
        "  outputs = model(X)\n",
        "  loss = criterion(outputs, outputs.detach())\n",
        "\n",
        "  # Compute gradients w.r.t. parameters\n",
        "  grads = torch.autograd.grad(loss, list(model.parameters()), create_graph=True)\n",
        "  flat_grad = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "  # Compute the Hessian matrix\n",
        "  num_params = flat_grad.shape[0]\n",
        "  hessian_matrix = torch.zeros(num_params, num_params)\n",
        "  for idx in range(num_params):\n",
        "    grad2 = torch.autograd.grad(flat_grad[idx], model.parameters(), retain_graph=True)\n",
        "    grad2_flat = torch.cat([g.contiguous().view(-1) for g in grad2])\n",
        "    hessian_matrix[idx] = grad2_flat.detach()\n",
        "\n",
        "    # Print the Hessian matrix\n",
        "    #print('Sample')\n",
        "    #print('gradient', flat_grad)\n",
        "    #print(X_i.data, '->', Y_i.data, outputs.data, loss)\n",
        "\n",
        "  return hessian_matrix\n",
        "\n",
        "\n",
        "\n",
        "def Hessian(model, X, criterion):\n",
        "  hessians = np.array([])\n",
        "  # Compute the loss\n",
        "  outputs = model(X)\n",
        "  loss = criterion(outputs, outputs.detach()).sum(dim=-1)\n",
        "\n",
        "  # Compute gradients w.r.t. parameters\n",
        "  flat_params = list(model.parameters())\n",
        "  numel = sum([p.numel() for p in flat_params])\n",
        "  hessian = torch.zeros((len(X), numel, numel))\n",
        "\n",
        "\n",
        "  for x_i, _ in enumerate(X):\n",
        "    grads = torch.autograd.grad(loss[x_i], flat_params, create_graph=True)\n",
        "    grads = torch.cat([g.contiguous().view(-1) for g in grads])\n",
        "    for g_i, g in enumerate(grads):\n",
        "      hessian[x_i, g_i, :] = torch.concat([i.flatten() for i in (torch.autograd.grad(g, flat_params, create_graph=True))])\n",
        "  return hessian\n",
        "\n",
        "class EigenEstimation(nn.Module):\n",
        "    def __init__(self, n, k, model):\n",
        "        super(EigenEstimation, self).__init__()\n",
        "        # Initialize U as a learnable parameter\n",
        "        self.U = nn.Parameter(torch.randn(n, k))\n",
        "        self.V = nn.Parameter(torch.randn(k, n))\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def forward(self, idxs):\n",
        "        # A is of shape [batch_size, n, n]\n",
        "        # Compute U^T H U for each matrix in the batch\n",
        "        U = self.U  # [n, k]\n",
        "        V = self.V\n",
        "        model = self.model\n",
        "\n",
        "\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "        grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "\n",
        "        H_rs = torch.Tensor([])\n",
        "        UVHUV_rs = torch.Tensor([])\n",
        "        diags = torch.Tensor([])\n",
        "        for i in idxs:\n",
        "          r = torch.zeros(U.shape[0])\n",
        "          r[i] = 0\n",
        "\n",
        "          # True Hessian-vector product\n",
        "          H_r = torch.autograd.grad(grads_flat @ r, model.parameters(), retain_graph=True)\n",
        "          H_r_flat = torch.cat([h.view(-1) for h in H_r])\n",
        "          H_rs.append(H_r_flat)\n",
        "\n",
        "          # Transformed Hessian-vector product\n",
        "          VHU_r = torch.autograd.grad(V @ grads_flat @ U @ r, model.parameters(), retain_graph=True)\n",
        "          VHU_r_flat = torch.cat([h.view(-1) for h in VHU_r])\n",
        "          diags.append(VHU_r_flat[i])\n",
        "\n",
        "          # Reconstructed Hessian-vector product\n",
        "          UVHUV_r = torch.autograd.grad(U @ V @ grads_flat @ U @ V @ r, model.parameters(), retain_graph=True)\n",
        "          UVHUV_r_flat = torch.cat([h.view(-1) for h in UVHUV_r])\n",
        "          UVHUV_rs.append(UVHUV_r_flat)\n",
        "\n",
        "        return torch.stack(H_rs), torch.stack(UVHUV_rs),  diags\n",
        "\n",
        "\n",
        "def total_loss_function(H, H_reconstructed, diag_elements, lambda_penalty=.1):\n",
        "  # Reconstruction Loss\n",
        "  reconstruction_loss = torch.mean((H - H_reconstructed) ** 2)\n",
        "\n",
        "  # Penalty Term: L2 norm squared of diag_elements\n",
        "  penalty_term = torch.mean(torch.mean(abs(diag_elements), dim=1), dim=0)  # Mean over batch\n",
        "  #print(diag_elements, penalty_term)\n",
        "\n",
        "  # Total Loss\n",
        "  total_loss = reconstruction_loss + lambda_penalty * penalty_term\n",
        "\n",
        "  return total_loss, reconstruction_loss, penalty_term\n",
        "\n",
        "\n",
        "\n",
        "def TrainEigenestimation(model, learning_rate, num_epochs, lambda_penalty, k, dataloader, criterion):\n",
        "  # Hyperparameters\n",
        "  n = (sum(p.numel() for p in model.parameters())) # Dimension of the matrices\n",
        "\n",
        "  # Create model\n",
        "  eigen_model = EigenEstimation(n, k)\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(eigen_model.parameters(), lr=learning_rate)\n",
        "  eigen_model.train()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    total_losses = 0\n",
        "    reconstruction_losses = 0\n",
        "    penalty_terms = 0\n",
        "    optimizer.zero_grad()\n",
        "    for X_batch, Y_batch in dataloader:\n",
        "      #print(f'epoch{epoch}')\n",
        "\n",
        "\n",
        "      H = Hessian(model, X_batch+.0*torch.randn_like(X_batch), criterion=criterion(reduction='none'))\n",
        "      # Forward pass\n",
        "      H_reconstructed, diag_elements = eigen_model(H)\n",
        "      #print(X_batch, H_reconstructed)\n",
        "\n",
        "      #print(H, diag_elements, H_reconstructed, diag_elements)\n",
        "      # Compute loss\n",
        "      total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "          H, H_reconstructed, diag_elements, lambda_penalty\n",
        "      )\n",
        "      total_losses = total_losses + total_loss\n",
        "      reconstruction_losses = reconstruction_losses + reconstruction_loss\n",
        "      penalty_terms = penalty_terms + penalty_term\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % round(num_epochs/10) == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total total_losses: {total_losses.item():.6f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_losses.item():.6f}, \"\n",
        "              f\"Penalty Term: {penalty_terms.item():.6f}\")\n",
        "    if reconstruction_losses < .01: break\n",
        "\n",
        "  return eigen_model\n",
        "\n",
        "\n",
        "\n",
        "def ComputeDiagonals(model, eigenmodel, dataloader, criterion):\n",
        "  diag_elements_list = []\n",
        "  for X_batch, Y_batch in dataloader:\n",
        "    H = Hessian(model, X_batch, criterion=nn.MSELoss(reduction='none'))\n",
        "\n",
        "    # Forward pass\n",
        "    H_reconstructed, diag_elements = eigenmodel(H)\n",
        "\n",
        "    diag_elements_list.append(diag_elements)\n",
        "    #print(X_batch)\n",
        "    #print(H, H_reconstructed)\n",
        "    print(X_batch , '->', diag_elements.detach().numpy().round(2))\n",
        "  return torch.Tensor(torch.concat(diag_elements_list))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "eigen_dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)), batch_size=4, shuffle=True)\n",
        "eigenmodel = TrainEigenestimation(zero_loss_model, .01, 400, .1, 3, eigen_dataloader, criterion=nn.MSELoss)\n",
        "\n",
        "\n",
        "eigen_dataloader = DataLoader(TensorDataset(X, Y), batch_size=1, shuffle=True)\n",
        "diags = ComputeDiagonals(zero_loss_model, eigenmodel, eigen_dataloader, criterion=nn.MSELoss)\n",
        "print(eigenmodel.U.data.numpy().round(2))\n",
        "#print(eigenmodel.V.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzxLKGGpJDVr",
        "outputId": "e2774fbc-687d-48b3-f221-b080860d52d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/400], Total total_losses: 199.349396, Reconstruction Loss: 197.576874, Penalty Term: 17.725151\n",
            "Epoch [40/400], Total total_losses: 4.794537, Reconstruction Loss: 4.176722, Penalty Term: 6.178156\n",
            "Epoch [80/400], Total total_losses: 1.898064, Reconstruction Loss: 0.994800, Penalty Term: 9.032642\n",
            "Epoch [120/400], Total total_losses: 1.643376, Reconstruction Loss: 0.714652, Penalty Term: 9.287249\n",
            "Epoch [160/400], Total total_losses: 1.569533, Reconstruction Loss: 0.600098, Penalty Term: 9.694351\n",
            "Epoch [200/400], Total total_losses: 1.336609, Reconstruction Loss: 0.267898, Penalty Term: 10.687112\n",
            "Epoch [240/400], Total total_losses: 1.354987, Reconstruction Loss: 0.320897, Penalty Term: 10.340893\n",
            "Epoch [280/400], Total total_losses: 2.125040, Reconstruction Loss: 1.240399, Penalty Term: 8.846401\n",
            "Epoch [320/400], Total total_losses: 1.066879, Reconstruction Loss: 0.032834, Penalty Term: 10.340454\n",
            "Epoch [360/400], Total total_losses: 2.676514, Reconstruction Loss: 1.906893, Penalty Term: 7.696216\n",
            "Epoch [400/400], Total total_losses: 1.064314, Reconstruction Loss: 0.032964, Penalty Term: 10.313503\n",
            "tensor([[1., 1.]]) -> [[0.01 0.   3.82]]\n",
            "tensor([[0., 1.]]) -> [[0.   1.97 0.  ]]\n",
            "tensor([[1., 0.]]) -> [[1.04 0.   0.83]]\n",
            "tensor([[0., 0.]]) -> [[0. 0. 0.]]\n",
            "[[-0.36  0.   -0.77]\n",
            " [ 0.33  0.   -0.87]\n",
            " [-0.   -0.   -0.  ]\n",
            " [ 0.   -0.66  0.  ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "eigen_dataloader = DataLoader(TensorDataset(X.repeat(16, 1), Y.repeat(16, 1)), batch_size=4, shuffle=True)\n",
        "eigenmodel = TrainEigenestimation(zero_loss_model, .01, 400, .1, 3, eigen_dataloader, criterion=nn.MSELoss)\n",
        "\n",
        "\n",
        "eigen_dataloader = DataLoader(TensorDataset(X, Y), batch_size=1, shuffle=True)\n",
        "diags = ComputeDiagonals(zero_loss_model, eigenmodel, eigen_dataloader, criterion=nn.MSELoss)\n",
        "print(eigenmodel.U.data.numpy().round(2))\n",
        "#print(eigenmodel.V.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nZ2hM2qc-QC",
        "outputId": "f86d9c9c-2e78-41c5-9fd0-751e9ba25a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Second-order directional derivative: 40.147071838378906\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "outputs = zero_loss_model(X)\n",
        "loss = criterion(reduction='none')(outputs, outputs.detach()).sum()\n",
        "\n",
        "\n",
        "# Direction vector in parameter space (same size as model parameters)\n",
        "v = torch.randn_like(torch.cat([p.view(-1) for p in zero_loss_model.parameters()]))\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "grads = torch.autograd.grad(loss, zero_loss_model.parameters(), create_graph=True)\n",
        "\n",
        "# Flatten the gradients\n",
        "grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "# Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "Hv = torch.autograd.grad(grads_flat @ v, zero_loss_model.parameters(), retain_graph=True)\n",
        "\n",
        "# Flatten the Hessian-vector product\n",
        "Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "# Step 4: Compute the second-order directional derivative\n",
        "second_order_derivative = (v * Hv_flat).sum()\n",
        "\n",
        "print(\"Second-order directional derivative:\", second_order_derivative.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeJTqBNwoM3K",
        "outputId": "257e80b8-1866-4e59-ac66-75489f7f1201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(88.9114)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "k = 2\n",
        "V = torch.randn_like(torch.cat([p.view(-1) for p in model.parameters()]).repeat(k,1))\n",
        "\n",
        "model = zero_loss_model\n",
        "outputs = model(X)\n",
        "losses = (criterion(reduction='none')(outputs, outputs.detach()))\n",
        "variances = torch.zeros(len(X))\n",
        "for loss_i, loss in enumerate(losses):\n",
        "  vs = torch.zeros(k)\n",
        "  for v_i,v in enumerate(V):\n",
        "\n",
        "    # Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the gradients\n",
        "    grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "    # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "    Hv = torch.autograd.grad(grads_flat @ v, model.parameters(), retain_graph=True)\n",
        "\n",
        "    # Flatten the Hessian-vector product\n",
        "    Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "    # Step 4: Compute the second-order directional derivative\n",
        "    second_order_derivative = (v * Hv_flat).sum()\n",
        "    vs[v_i] = second_order_derivative.item()\n",
        "  variances[loss_i] = vs.var().item()\n",
        "\n",
        "L = variances.sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vlPvtCAcvfpE",
        "outputId": "124bca19-ea61-4131-f44d-047fdc2c11e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1/10000, L: 4.154786586761475\n",
            "Step 11/10000, L: 3.176546573638916\n",
            "Step 21/10000, L: 3.284395694732666\n",
            "Step 31/10000, L: 3.255119800567627\n",
            "Step 41/10000, L: 2.66479754447937\n",
            "Step 51/10000, L: 2.325073719024658\n",
            "Step 61/10000, L: 2.825889825820923\n",
            "Step 71/10000, L: 2.2542166709899902\n",
            "Step 81/10000, L: 2.385585069656372\n",
            "Step 91/10000, L: 1.9607818126678467\n",
            "Step 101/10000, L: 2.347054958343506\n",
            "Step 111/10000, L: 1.5950849056243896\n",
            "Step 121/10000, L: 1.617498755455017\n",
            "Step 131/10000, L: 1.9519511461257935\n",
            "Step 141/10000, L: 1.7394458055496216\n",
            "Step 151/10000, L: 1.7745057344436646\n",
            "Step 161/10000, L: 1.8405263423919678\n",
            "Step 171/10000, L: 1.1993656158447266\n",
            "Step 181/10000, L: 1.763914704322815\n",
            "Step 191/10000, L: 1.5331813097000122\n",
            "Step 201/10000, L: 0.9299415946006775\n",
            "Step 211/10000, L: 0.8596839904785156\n",
            "Step 221/10000, L: 1.5237809419631958\n",
            "Step 231/10000, L: 1.4458532333374023\n",
            "Step 241/10000, L: 1.637150764465332\n",
            "Step 251/10000, L: 1.6043899059295654\n",
            "Step 261/10000, L: 1.3737986087799072\n",
            "Step 271/10000, L: 1.1368826627731323\n",
            "Step 281/10000, L: 1.310975432395935\n",
            "Step 291/10000, L: 1.0857164859771729\n",
            "Step 301/10000, L: 1.3301583528518677\n",
            "Step 311/10000, L: 1.0838876962661743\n",
            "Step 321/10000, L: 0.9901294708251953\n",
            "Step 331/10000, L: 1.1654378175735474\n",
            "Step 341/10000, L: 0.6424329876899719\n",
            "Step 351/10000, L: 1.4540858268737793\n",
            "Step 361/10000, L: 1.0172507762908936\n",
            "Step 371/10000, L: 0.934016764163971\n",
            "Step 381/10000, L: 0.7224633693695068\n",
            "Step 391/10000, L: 1.153796911239624\n",
            "Step 401/10000, L: 1.0003626346588135\n",
            "Step 411/10000, L: 1.2625657320022583\n",
            "Step 421/10000, L: 0.6191009879112244\n",
            "Step 431/10000, L: 1.0808383226394653\n",
            "Step 441/10000, L: 0.9192487001419067\n",
            "Step 451/10000, L: 1.0658894777297974\n",
            "Step 461/10000, L: 0.5754708051681519\n",
            "Step 471/10000, L: 1.101314663887024\n",
            "Step 481/10000, L: 0.5633745193481445\n",
            "Step 491/10000, L: 0.705563485622406\n",
            "Step 501/10000, L: 0.5999341011047363\n",
            "Step 511/10000, L: 1.1391898393630981\n",
            "Step 521/10000, L: 0.9887555241584778\n",
            "Step 531/10000, L: 0.6581773161888123\n",
            "Step 541/10000, L: 1.1531918048858643\n",
            "Step 551/10000, L: 0.850351095199585\n",
            "Step 561/10000, L: 0.6439458727836609\n",
            "Step 571/10000, L: 1.0648424625396729\n",
            "Step 581/10000, L: 0.598335862159729\n",
            "Step 591/10000, L: 0.5521667003631592\n",
            "Step 601/10000, L: 0.6884517669677734\n",
            "Step 611/10000, L: 1.0438216924667358\n",
            "Step 621/10000, L: 0.5070565342903137\n",
            "Step 631/10000, L: 0.6520330905914307\n",
            "Step 641/10000, L: 0.7185900211334229\n",
            "Step 651/10000, L: 0.6776171922683716\n",
            "Step 661/10000, L: 0.6799108982086182\n",
            "Step 671/10000, L: 0.9265163540840149\n",
            "Step 681/10000, L: 0.8832487463951111\n",
            "Step 691/10000, L: 0.6324357986450195\n",
            "Step 701/10000, L: 0.6563947796821594\n",
            "Step 711/10000, L: 0.38481882214546204\n",
            "Step 721/10000, L: 0.5616271495819092\n",
            "Step 731/10000, L: 0.5515920519828796\n",
            "Step 741/10000, L: 0.24820326268672943\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-b22ba6f046ce>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0;31m# Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m           \u001b[0mHv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_flat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0;31m# Flatten the Hessian-vector product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_TensorOrTensorsOrGradEdge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0mt_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0moverridable_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m         \"\"\"\n\u001b[0;32m-> 2255\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2256\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2286\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m             prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[0;32m-> 2288\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2225\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2227\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Example: Define a simple model\n",
        "model =zero_loss_model  # A simple linear model\n",
        "criterion = torch.nn.MSELoss(reduction='none')  # Mean Squared Error loss, without reduction\n",
        "\n",
        "# Initialize V as a trainable tensor\n",
        "k = 3  # The number of directions (as in the original code)\n",
        "#V = torch.randn_like(torch.cat([p.view(-1) for p in model.parameters()]).repeat(k, 1), requires_grad=True)\n",
        "\n",
        "# Optimizer (you can use Adam or any other optimizer)\n",
        "optimizer = optim.SGD([V], lr=0.001)\n",
        "\n",
        "# Number of optimization steps\n",
        "num_steps = 10000  # Or any number of steps you want to run\n",
        "sparse_p = .2\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16,1).squeeze(0)), batch_size=24, shuffle=True)\n",
        "for step in range(num_steps):\n",
        "\n",
        "    for X_batch in dataloader:\n",
        "      second_order_deriv_mat = []\n",
        "      L = 0\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(X_batch[0])\n",
        "      losses = criterion(outputs, outputs.detach())  # Compute loss with no gradient for outputs.detach()\n",
        "      n = 0\n",
        "      for x_i,x in enumerate(X_batch[0]):\n",
        "        var_component1 = 0\n",
        "\n",
        "        var_component2 = 0\n",
        "\n",
        "        second_order_derivatives = []\n",
        "        for v_i, v in enumerate(V):\n",
        "\n",
        "          # Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "          grads = torch.autograd.grad(losses[x_i], model.parameters(),\n",
        "                                          create_graph=True)\n",
        "\n",
        "          # Flatten the gradients\n",
        "          grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "          # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "          H_r = torch.autograd.grad(grads_flat @ r, model.parameters(), retain_graph=True)\n",
        "\n",
        "          # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "          UVHUV_r = torch.autograd.grad(V @ U @ grads_flat @ V @ U @ r, model.parameters(), retain_graph=True)\n",
        "\n",
        "\n",
        "          # Flatten the Hessian-vector product\n",
        "          Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "          # Step 4: Compute the second-order directional derivative\n",
        "          second_order_derivative = (v * Hv_flat).sum()\n",
        "          #vs[v_i] = second_order_derivative\n",
        "\n",
        "          # Compute the variance for the current loss\n",
        "          #variances[loss_i] = vs.var()  # Removed `.item()` to maintain gradient tracking\n",
        "\n",
        "          # Loss to be maximized (L is the sum of variances)\n",
        "          # Append to a tensor of second_order_derivatives\n",
        "\n",
        "\n",
        "          second_order_derivatives.append(second_order_derivative)  # Append to list instead of in-place modification\n",
        "\n",
        "        second_order_deriv_mat.append(torch.stack(second_order_derivatives))\n",
        "\n",
        "      second_order_derivatives_tensor = torch.stack(second_order_deriv_mat)\n",
        "      row_var = second_order_derivatives_tensor.sum(dim=1)**2\n",
        "      col_var = second_order_derivatives_tensor.var(dim=0).mean()\n",
        "\n",
        "        # Convert the list of second-order derivatives to a tensor\n",
        "        #      var_component1 = var_component1 + (second_order_derivative**2)\n",
        "        #      var_component2 = var_component2 + second_order_derivative\n",
        "        #      n = n + 1\n",
        "        #var = var_component1 - sparse_p *var_component2\n",
        "        #L = L - (second_order_derivative**2 -\n",
        "                       #sparse_p*second_order_derivative) #var_component2 + sparse_p * var_component2\n",
        "      # Backward pass to compute gradients of L w.r.t. V\n",
        "      L =  L - ((second_order_derivatives_tensor.mean(dim=0)**2).mean() +\n",
        "                (second_order_derivatives_tensor.mean(dim=1)**2).mean() -\n",
        "                10 * second_order_derivatives_tensor.mean()\n",
        "                )\n",
        "\n",
        "      L.backward()\n",
        "\n",
        "\n",
        "      # Update V using the optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # Normalize V\n",
        "      V.data = V.data/V.data.norm(dim=1, keepdim=True)\n",
        "      #V/V.norm(dim=1, keepdim=True)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      # Optional: Print the value of L and V at each step\n",
        "      print(f\"Step {step+1}/{num_steps}, L: {L.item()}\")\n",
        "\n",
        "# After optimization, V will contain the direction that maximizes L\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q-6Pft01hQpG",
        "outputId": "86560508-c770-4293-cd3c-bb1a622b2bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1/10000, L: 234.8554229736328\n",
            "Step 11/10000, L: 49.460914611816406\n",
            "Step 21/10000, L: 245.7700653076172\n",
            "Step 31/10000, L: 474.0297546386719\n",
            "Step 41/10000, L: 522.2626342773438\n",
            "Step 51/10000, L: 317.93310546875\n",
            "Step 61/10000, L: 154.40528869628906\n",
            "Step 71/10000, L: 386.526611328125\n",
            "Step 81/10000, L: 418.9598693847656\n",
            "Step 91/10000, L: 507.1475830078125\n",
            "Step 101/10000, L: 185.45094299316406\n",
            "Step 111/10000, L: 171.76205444335938\n",
            "Step 121/10000, L: 261.2942199707031\n",
            "Step 131/10000, L: 447.8503723144531\n",
            "Step 141/10000, L: 83.21048736572266\n",
            "Step 151/10000, L: 163.1501922607422\n",
            "Step 161/10000, L: 76.17061614990234\n",
            "Step 171/10000, L: 164.6453399658203\n",
            "Step 181/10000, L: 207.71388244628906\n",
            "Step 191/10000, L: 67.6426010131836\n",
            "Step 201/10000, L: 252.93740844726562\n",
            "Step 211/10000, L: 218.4727325439453\n",
            "Step 221/10000, L: 275.1949157714844\n",
            "Step 231/10000, L: 62.463809967041016\n",
            "Step 241/10000, L: 376.2247009277344\n",
            "Step 251/10000, L: 68.64906311035156\n",
            "Step 261/10000, L: 345.94384765625\n",
            "Step 271/10000, L: 98.33149719238281\n",
            "Step 281/10000, L: 293.86602783203125\n",
            "Step 291/10000, L: 238.98265075683594\n",
            "Step 301/10000, L: 218.28025817871094\n",
            "Step 311/10000, L: 213.16441345214844\n",
            "Step 321/10000, L: 200.39321899414062\n",
            "Step 331/10000, L: 129.953125\n",
            "Step 341/10000, L: 374.55413818359375\n",
            "Step 351/10000, L: 76.37489318847656\n",
            "Step 361/10000, L: 81.33710479736328\n",
            "Step 371/10000, L: 67.96930694580078\n",
            "Step 381/10000, L: 258.6980285644531\n",
            "Step 391/10000, L: 322.6316223144531\n",
            "Step 401/10000, L: 301.0643005371094\n",
            "Step 411/10000, L: 264.1468811035156\n",
            "Step 421/10000, L: 214.63436889648438\n",
            "Step 431/10000, L: 364.3586120605469\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-cb5451ae97a4>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0;31m# Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0mUVHUV_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mgrads_flat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0;31m# How close is the reconstructed Hessian to its original?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    434\u001b[0m         )\n\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Example: Define a simple model\n",
        "model =zero_loss_model  # A simple linear model\n",
        "criterion = torch.nn.MSELoss(reduction='none')  # Mean Squared Error loss, without reduction\n",
        "\n",
        "# Initialize V as a trainable tensor\n",
        "k = 3  # The number of directions (as in the original code)\n",
        "numel = sum([p.numel() for p in model.parameters()])\n",
        "U = torch.randn(k,numel)\n",
        "V = torch.randn(numel, k)\n",
        "\n",
        "# Optimizer (you can use Adam or any other optimizer)\n",
        "optimizer = optim.Adam([V, U], lr=1e-5)\n",
        "\n",
        "# Number of optimization steps\n",
        "num_steps = 10000  # Or any number of steps you want to run\n",
        "sparse_p = .2\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16,1).squeeze(0)), batch_size=4, shuffle=True)\n",
        "\n",
        "for step in range(num_steps):\n",
        "\n",
        "    for X_batch in dataloader:\n",
        "      second_order_deriv_mat = []\n",
        "      L = 0\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(X_batch[0])\n",
        "      losses = criterion(outputs, outputs.detach())  # Compute loss with no gradient for outputs.detach()\n",
        "      n = 0\n",
        "      for x_i,x in enumerate(X_batch[0]):\n",
        "\n",
        "        MSE = 0\n",
        "        for random_iters in range(10):\n",
        "          # Random vector r\n",
        "          r = torch.randn(numel, requires_grad=True)\n",
        "\n",
        "\n",
        "          # Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "          grads = torch.autograd.grad(losses[x_i], model.parameters(),\n",
        "                                            create_graph=True)\n",
        "\n",
        "          # Flatten the gradients\n",
        "          grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "          # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "          H_r = torch.autograd.grad(grads_flat @ r, model.parameters(), create_graph=True)\n",
        "\n",
        "          # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "          UVHUV_r = torch.autograd.grad(V @ U @ grads_flat @ V @ U @ r, model.parameters(), retain_graph=True)\n",
        "\n",
        "          # How close is the reconstructed Hessian to its original?\n",
        "          MSE = MSE + ((H_r[0] - UVHUV_r[0])**2).mean()\n",
        "\n",
        "        # How many of the reconstructed Hessians diagonals != 0?\n",
        "        diags = torch.tensor(0)\n",
        "        for i in range(len(U)):\n",
        "          # Extract the i-th vectors\n",
        "          u_i = U[i]\n",
        "          v_i = V[:, i]\n",
        "\n",
        "          # Compute H v_i using Hessian-vector product\n",
        "          Hv = torch.autograd.grad(grads_flat @ v_i, model.parameters(), retain_graph=True)[0].flatten()\n",
        "\n",
        "          # Compute the diagonal element M_ii = u_i^T (H v_i)\n",
        "          M_ii = u_i @ Hv\n",
        "\n",
        "          diags = diags + abs(M_ii)  # Store the scalar value\n",
        "\n",
        "        L = L + MSE + (sparse_p * diags/k)\n",
        "\n",
        "      L.backward()\n",
        "\n",
        "\n",
        "      # Update V using the optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # Normalize V\n",
        "      V.data = V.data/V.data.norm(dim=1, keepdim=True)\n",
        "      U.data = U.data/U.data.norm(dim=0, keepdim=True)\n",
        "\n",
        "      #V/V.norm(dim=1, keepdim=True)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      # Optional: Print the value of L and V at each step\n",
        "      print(f\"Step {step+1}/{num_steps}, L: {L.item()}\")\n",
        "\n",
        "# After optimization, V will contain the direction that maximizes L\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2uMDb_3neRc",
        "outputId": "0235d5ae-bcc0-4981-89a9-3c27e7750f77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(U)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wDzPC4km5AP",
        "outputId": "9c06a425-a837-4fbf-99f9-6c876e4d9ae1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.8329,  0.0000],\n",
              "         [ 0.0000,  0.0000]], grad_fn=<TBackward0>),)"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "H_r = torch.autograd.grad(grads_flat @ r, model.parameters(), create_graph=True)\n",
        "H_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypqG1p5gmjvh",
        "outputId": "85d3509a-9fea-4a8c-9916-3e7cba9cd35c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000,  0.0000],\n",
              "         [ 0.0000, -0.6807]]),)"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "H_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ESU5VWl3aL",
        "outputId": "6cb2a5be-32c3-410c-f554-07ec9f51039b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(83070.5078)"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oadJGZ_nilMT",
        "outputId": "f9145d5c-5902-470d-9188-c147287a27d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(104.5706)"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((H_r[0] - UVHUV_r[0])**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-EXYK-TioiM",
        "outputId": "0c45ec27-17da-48f4-dcd4-1012e427f15b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-1.1982,  0.0000],\n",
              "         [ 0.0000,  0.0000]]),)"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "H_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anC_KMPFjmL8",
        "outputId": "711f3037-b444-4792-a66f-b1aee1552180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1638,  0.2624, -0.9505, -0.0293],\n",
            "        [-0.1155,  0.1851,  0.9758,  0.0173],\n",
            "        [-0.4810,  0.7685, -0.4064,  0.1133]], requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "tensor([0., 0.]) -> 0.0\n",
            "tensor([0., 0.]) -> 0.0\n",
            "tensor([0., 0.]) -> 0.0\n",
            "\n",
            "\n",
            "\n",
            "tensor([0., 1.]) -> 0.0017180792056024075\n",
            "tensor([0., 1.]) -> 0.0005997843691147864\n",
            "tensor([0., 1.]) -> 0.025679467245936394\n",
            "\n",
            "\n",
            "\n",
            "tensor([1., 0.]) -> 0.053672440350055695\n",
            "tensor([1., 0.]) -> 0.026683399453759193\n",
            "tensor([1., 0.]) -> 0.46280303597450256\n",
            "\n",
            "\n",
            "\n",
            "tensor([1., 1.]) -> 0.019436951726675034\n",
            "tensor([1., 1.]) -> 0.009673170745372772\n",
            "tensor([1., 1.]) -> 0.1652633547782898\n"
          ]
        }
      ],
      "source": [
        "    print(V)\n",
        "    outputs = model(X)\n",
        "    losses = criterion(outputs, outputs.detach())  # Compute loss with no gradient for outputs.detach()\n",
        "\n",
        "\n",
        "    for loss_i, loss in enumerate(losses):\n",
        "        print('\\n\\n')\n",
        "        for v_i, v in enumerate(V):\n",
        "            # Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "            grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "            # Flatten the gradients\n",
        "            grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "            # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "            Hv = torch.autograd.grad(grads_flat @ v, model.parameters(), retain_graph=True)\n",
        "\n",
        "            # Flatten the Hessian-vector product\n",
        "            Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "            # Step 4: Compute the second-order directional derivative\n",
        "            second_order_derivative = (v * Hv_flat).sum()\n",
        "\n",
        "            print(X[loss_i], '->', second_order_derivative.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OfYGXct9_8Bh",
        "outputId": "c912ef2a-318f-493a-b4b6-5603f7df7884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1/10000, L: 13.602947235107422\n",
            "Step 11/10000, L: 13.019805908203125\n",
            "Step 21/10000, L: 12.306353569030762\n",
            "Step 31/10000, L: 9.395416259765625\n",
            "Step 41/10000, L: 5.546199798583984\n",
            "Step 51/10000, L: 3.6918890476226807\n",
            "Step 61/10000, L: 4.573017120361328\n",
            "Step 71/10000, L: 3.0289721488952637\n",
            "Step 81/10000, L: 4.558367729187012\n",
            "Step 91/10000, L: 6.2711968421936035\n",
            "Step 101/10000, L: 1.8951225280761719\n",
            "Step 111/10000, L: 3.7905519008636475\n",
            "Step 121/10000, L: 2.8424694538116455\n",
            "Step 131/10000, L: 4.736623287200928\n",
            "Step 141/10000, L: 3.9705376625061035\n",
            "Step 151/10000, L: 1.8636683225631714\n",
            "Step 161/10000, L: 5.146601676940918\n",
            "Step 171/10000, L: 2.4394681453704834\n",
            "Step 181/10000, L: 2.8420510292053223\n",
            "Step 191/10000, L: 5.322779655456543\n",
            "Step 201/10000, L: 6.269034385681152\n",
            "Step 211/10000, L: 4.555479526519775\n",
            "Step 221/10000, L: 5.146650791168213\n",
            "Step 231/10000, L: 3.2066707611083984\n",
            "Step 241/10000, L: 3.971620798110962\n",
            "Step 251/10000, L: 4.736026763916016\n",
            "Step 261/10000, L: 4.734887599945068\n",
            "Step 271/10000, L: 3.3892292976379395\n",
            "Step 281/10000, L: 3.205554962158203\n",
            "Step 291/10000, L: 4.201197147369385\n",
            "Step 301/10000, L: 0.9089740514755249\n",
            "Step 311/10000, L: 3.205975294113159\n",
            "Step 321/10000, L: 4.738471508026123\n",
            "Step 331/10000, L: 4.201381206512451\n",
            "Step 341/10000, L: 2.663339614868164\n",
            "Step 351/10000, L: 3.3923768997192383\n",
            "Step 361/10000, L: 2.0751523971557617\n",
            "Step 371/10000, L: 3.97241473197937\n",
            "Step 381/10000, L: 4.557780742645264\n",
            "Step 391/10000, L: 3.205763578414917\n",
            "Step 401/10000, L: 5.323513507843018\n",
            "Step 411/10000, L: 3.7896199226379395\n",
            "Step 421/10000, L: 5.7406325340271\n",
            "Step 431/10000, L: 4.156427383422852\n",
            "Step 441/10000, L: 4.97065544128418\n",
            "Step 451/10000, L: 4.7385640144348145\n",
            "Step 461/10000, L: 3.0808820724487305\n",
            "Step 471/10000, L: 3.0247316360473633\n",
            "Step 481/10000, L: 3.973416328430176\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-4ffce9d6c8d7>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0;31m# Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m           \u001b[0mHv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_flat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0;31m# Flatten the Hessian-vector product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Example: Define a simple model\n",
        "model =zero_loss_model  # A simple linear model\n",
        "criterion = torch.nn.MSELoss(reduction='none')  # Mean Squared Error loss, without reduction\n",
        "\n",
        "# Initialize V as a trainable tensor\n",
        "k = 3  # The number of directions (as in the original code)\n",
        "V = torch.randn_like(torch.cat([p.view(-1) for p in model.parameters()]).repeat(k, 1), requires_grad=True)\n",
        "\n",
        "# Optimizer (you can use Adam or any other optimizer)\n",
        "optimizer = optim.SGD([V], lr=0.001)\n",
        "\n",
        "# Number of optimization steps\n",
        "num_steps = 10000  # Or any number of steps you want to run\n",
        "sparse_p = .01\n",
        "dataloader = DataLoader(TensorDataset(X.repeat(16,1).squeeze(0)), batch_size=24, shuffle=True)\n",
        "for step in range(num_steps):\n",
        "\n",
        "    for X_batch in dataloader:\n",
        "      second_order_deriv_mat = []\n",
        "      L = 0\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(X_batch[0])\n",
        "      losses = criterion(outputs, outputs.detach())  # Compute loss with no gradient for outputs.detach()\n",
        "      n = 0\n",
        "      for x_i,x in enumerate(X_batch[0]):\n",
        "        var_component1 = 0\n",
        "\n",
        "        var_component2 = 0\n",
        "\n",
        "        second_order_derivatives = []\n",
        "        for v_i, v in enumerate(V):\n",
        "\n",
        "          # Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "          grads = torch.autograd.grad(losses[x_i], model.parameters(),\n",
        "                                          create_graph=True)\n",
        "\n",
        "          # Flatten the gradients\n",
        "          grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "          # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "          Hv = torch.autograd.grad(grads_flat @ v, model.parameters(), retain_graph=True)\n",
        "\n",
        "          # Flatten the Hessian-vector product\n",
        "          Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "          # Step 4: Compute the second-order directional derivative\n",
        "          second_order_derivative = (v * Hv_flat).sum()\n",
        "          #vs[v_i] = second_order_derivative\n",
        "\n",
        "          # Compute the variance for the current loss\n",
        "          #variances[loss_i] = vs.var()  # Removed `.item()` to maintain gradient tracking\n",
        "\n",
        "          # Loss to be maximized (L is the sum of variances)\n",
        "          # Append to a tensor of second_order_derivatives\n",
        "\n",
        "          # How close is it to being an eigenvector?\n",
        "          lambda_approx = v@Hv_flat / (v@v)\n",
        "          residual = Hv_flat - (lambda_approx * v)\n",
        "          residual = (residual**2).mean()\n",
        "\n",
        "          #second_order_derivatives.append(second_order_derivative)  # Append to list instead of in-place modification\n",
        "\n",
        "        #second_order_deriv_mat.append(torch.stack(second_order_derivatives))\n",
        "\n",
        "      #second_order_derivatives_tensor = torch.stack(second_order_deriv_mat)\n",
        "      #row_var = second_order_derivatives_tensor.sum(dim=1)**2\n",
        "      #col_var = second_order_derivatives_tensor.var(dim=0).mean()\n",
        "\n",
        "        # Convert the list of second-order derivatives to a tensor\n",
        "        #      var_component1 = var_component1 + (second_order_derivative**2)\n",
        "        #      var_component2 = var_component2 + second_order_derivative\n",
        "        #      n = n + 1\n",
        "        #var = var_component1 - sparse_p *var_component2\n",
        "        #L = L - (second_order_derivative**2 -\n",
        "                       #sparse_p*second_order_derivative) #var_component2 + sparse_p * var_component2\n",
        "      # Backward pass to compute gradients of L w.r.t. V\n",
        "          L =  L + sparse_p*lambda_approx + residual\n",
        "      #L - ((second_order_derivatives_tensor.mean(dim=0)**2).mean() +\n",
        "      #          (second_order_derivatives_tensor.mean(dim=1)**2).mean() -\n",
        "      #          10 * second_order_derivatives_tensor.mean()\n",
        "      #          )\n",
        "\n",
        "      L.backward()\n",
        "\n",
        "\n",
        "      # Update V using the optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # Normalize V\n",
        "      V.data = V.data/V.data.norm(dim=1, keepdim=True)\n",
        "      #V/V.norm(dim=1, keepdim=True)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      # Optional: Print the value of L and V at each step\n",
        "      print(f\"Step {step+1}/{num_steps}, L: {L.item()}\")\n",
        "\n",
        "# After optimization, V will contain the direction that maximizes L\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vovEl2ipfjwK",
        "outputId": "addf39b2-d03f-46ad-b458-774507a784b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-7.5429e-01, -6.5655e-01, -1.1139e-29, -4.2681e-29],\n",
              "        [-7.5429e-01, -6.5655e-01, -1.0423e-28, -7.8978e-29],\n",
              "        [ 7.5429e-01,  6.5655e-01,  1.0486e-29, -7.7798e-30]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfQT4fLecMMA",
        "outputId": "daa6df71-49a1-42fe-a6a2-96ba68886549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-7.5429e-01, -6.5655e-01, -1.1139e-29, -4.2681e-29],\n",
            "        [-7.5429e-01, -6.5655e-01, -1.0423e-28, -7.8978e-29],\n",
            "        [ 7.5429e-01,  6.5655e-01,  1.0486e-29, -7.7798e-30]],\n",
            "       requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "tensor([0., 0.]) -> 0.0\n",
            "tensor([0., 0.]) -> 0.0\n",
            "tensor([0., 0.]) -> 0.0\n",
            "\n",
            "\n",
            "\n",
            "tensor([0., 1.]) -> 0.0\n",
            "tensor([0., 1.]) -> 0.0\n",
            "tensor([0., 1.]) -> 0.0\n",
            "\n",
            "\n",
            "\n",
            "tensor([1., 0.]) -> 1.1378958225250244\n",
            "tensor([1., 0.]) -> 1.1378958225250244\n",
            "tensor([1., 0.]) -> 1.1378958225250244\n",
            "\n",
            "\n",
            "\n",
            "tensor([1., 1.]) -> 3.9808926582336426\n",
            "tensor([1., 1.]) -> 3.9808926582336426\n",
            "tensor([1., 1.]) -> 3.9808926582336426\n"
          ]
        }
      ],
      "source": [
        "    print(V)\n",
        "    outputs = model(X)\n",
        "    losses = criterion(outputs, outputs.detach())  # Compute loss with no gradient for outputs.detach()\n",
        "\n",
        "\n",
        "    for loss_i, loss in enumerate(losses):\n",
        "        print('\\n\\n')\n",
        "        for v_i, v in enumerate(V):\n",
        "            # Step 2: Compute the gradient of the loss w.r.t. the model's parameters\n",
        "            grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "            # Flatten the gradients\n",
        "            grads_flat = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "            # Step 3: Compute the Hessian-vector product (Hessian of the gradient wrt v)\n",
        "            Hv = torch.autograd.grad(grads_flat @ v, model.parameters(), retain_graph=True)\n",
        "\n",
        "            # Flatten the Hessian-vector product\n",
        "            Hv_flat = torch.cat([h.view(-1) for h in Hv])\n",
        "\n",
        "            # Step 4: Compute the second-order directional derivative\n",
        "            second_order_derivative = (v * Hv_flat).sum()\n",
        "\n",
        "            print(X[loss_i], '->', second_order_derivative.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbtz5zGrclss",
        "outputId": "58402432-b56d-4e40-bd5a-7aebff2e4f80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0., grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "residual = Hv_flat - (lambda_approx * v)\n",
        "(residual**2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt5S_7D0c7zY",
        "outputId": "dd83ce6c-f163-4998-f282-3fd037092473"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.5749, 0.5749, 0.0000, 0.0000])"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Hv_flat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSw-MaVOjpaI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FRkHh_ANyi-",
        "outputId": "d1b9abf4-9cfc-4ccc-f487-4ae8dea1da1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/400], Total total_losses: 1132.230591, Reconstruction Loss: 1129.813354, Penalty Term: 24.171822\n",
            "Epoch [40/400], Total total_losses: 51.458878, Reconstruction Loss: 49.460957, Penalty Term: 19.979200\n",
            "Epoch [80/400], Total total_losses: 15.344336, Reconstruction Loss: 13.390650, Penalty Term: 19.536879\n",
            "Epoch [120/400], Total total_losses: 8.605355, Reconstruction Loss: 6.699172, Penalty Term: 19.061840\n",
            "Epoch [160/400], Total total_losses: 6.316202, Reconstruction Loss: 4.441655, Penalty Term: 18.745474\n",
            "Epoch [200/400], Total total_losses: 5.267550, Reconstruction Loss: 3.420193, Penalty Term: 18.473577\n",
            "Epoch [240/400], Total total_losses: 4.685059, Reconstruction Loss: 2.858912, Penalty Term: 18.261467\n",
            "Epoch [280/400], Total total_losses: 4.406294, Reconstruction Loss: 2.619797, Penalty Term: 17.864979\n",
            "Epoch [320/400], Total total_losses: 4.204942, Reconstruction Loss: 2.472462, Penalty Term: 17.324791\n",
            "Epoch [360/400], Total total_losses: 4.022618, Reconstruction Loss: 2.351609, Penalty Term: 16.710087\n",
            "Epoch [400/400], Total total_losses: 3.792461, Reconstruction Loss: 2.202182, Penalty Term: 15.902791\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmo2ZmZNWBHA",
        "outputId": "e99589c6-2a95-4858-c5c0-eb1c3ad49170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 5])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xO18tb3FsxCm"
      },
      "outputs": [],
      "source": [
        "#@title Polytope Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw-o5OouwyRM"
      },
      "source": [
        "#TMS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bxQF2xps2AI"
      },
      "outputs": [],
      "source": [
        "# Data parameters\n",
        "num_features = 5       # Number of unique features\n",
        "feature_sparsity = 0.01    # Proportion of non-zero elements in each feature vector\n",
        "num_data_points = 10000   # Total number of data points\n",
        "\n",
        "# Model parameters\n",
        "hidden_dim = 3           # Dimension of the hidden layer (bottleneck)\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "# Initialize feature vectors\n",
        "feature_vectors = np.random.randn(num_data_points, num_features)\n",
        "feature_vectors = feature_vectors * (np.random.randn(num_data_points, num_features) < feature_sparsity)\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "data_inputs = np.array(feature_vectors)\n",
        "data_labels = np.array(feature_vectors)\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: reduces dimensionality\n",
        "        self.encoder = nn.Linear(num_features, hidden_dim, bias=False)\n",
        "        # Decoder: reconstructs the input\n",
        "        self.decoder = nn.Linear(hidden_dim, num_features, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "def TrainTMS(model, criterion, optimizer, dataloader, n_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        inputs = batch[0]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "    avg_loss = total_loss / num_data_points\n",
        "    if epoch %10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/], Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "gA2MQ787yOWw",
        "outputId": "7038440e-be44-413e-f11f-9d2a214c4873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/], Loss: 0.5094\n",
            "Epoch [11/], Loss: 0.2015\n",
            "Epoch [21/], Loss: 0.2011\n",
            "Epoch [31/], Loss: 0.2006\n",
            "Epoch [41/], Loss: 0.2001\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-399-37f53cda478a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                         batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTrainTMS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-398-504aa0a32409>\u001b[0m in \u001b[0;36mTrainTMS\u001b[0;34m(model, criterion, optimizer, dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \"\"\"\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "autoencoder = Autoencoder(num_features, hidden_dim)\n",
        "dataloader = DataLoader(TensorDataset(torch.Tensor(data_inputs).to('cuda'), torch.Tensor(data_labels).to('cuda')),\n",
        "                        batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "TrainTMS(autoencoder, nn.MSELoss(), optimizer, dataloader, n_epochs=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "PyN_u7BhTSSt",
        "outputId": "f2d15eee-49bb-47ce-a50a-70cc79e2779d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Total Loss: -0.4866, L2 loss: 0.5427, L1: 0.2806\n",
            "Epoch [10/100], Total Loss: -2.1174, L2 loss: 2.2311, L1: 0.5681\n",
            "Epoch [20/100], Total Loss: -1.8850, L2 loss: 1.9898, L1: 0.5242\n",
            "Epoch [30/100], Total Loss: -2.3302, L2 loss: 2.4323, L1: 0.5104\n",
            "Epoch [40/100], Total Loss: -3.3950, L2 loss: 3.4995, L1: 0.5225\n",
            "Epoch [50/100], Total Loss: -2.4625, L2 loss: 2.5647, L1: 0.5113\n",
            "Epoch [60/100], Total Loss: -2.8359, L2 loss: 2.9436, L1: 0.5381\n",
            "Epoch [70/100], Total Loss: -2.9089, L2 loss: 3.0158, L1: 0.5342\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-400-be205f51128f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                         batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTrainPerturbationEstimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-375-76ca1e5091a9>\u001b[0m in \u001b[0;36mTrainPerturbationEstimation\u001b[0;34m(perturb_model, lambda_penalty, dataloader, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Optimizer.step#{self.__class__.__name__}.step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m                 \u001b[0;31m# call optimizer step pre hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 for pre_hook in chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         self.record = torch.ops.profiler._record_function_enter_new(\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "perturb_model = PerturbationEstimation(autoencoder, 5, nn.MSELoss())\n",
        "dataloader = DataLoader(TensorDataset(torch.Tensor(data_inputs).to('cuda'), torch.Tensor(data_labels).to('cuda')),\n",
        "                        batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "\n",
        "TrainPerturbationEstimation(perturb_model, .2, dataloader, learning_rate=.001, num_epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PZv0RbJUTfQ",
        "outputId": "fc149b33-3f9c-4184-d427-666fa1103f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0000, 0.0000, 0.0000, -0.0000, 0.4129]], device='cuda:0') ->\n",
            "tensor([[6.8001e-04, 1.1241e-03, 5.3346e-02, 5.0099e-03, 4.0468e-02],\n",
            "        [1.0880e-02, 1.3337e-02, 4.0107e-03, 3.4694e-04, 1.3878e-05],\n",
            "        [8.8818e-04, 8.8818e-04, 1.0880e-02, 7.1942e-02, 6.4171e-02],\n",
            "        [8.2281e-02, 8.6611e-02, 4.9960e-04, 1.3878e-03, 4.4964e-03],\n",
            "        [2.2204e-04, 0.0000e+00, 6.4171e-02, 7.1942e-02, 4.3521e-02]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[ 0.0000, -0.0362,  1.1187, -2.0127,  1.1681]], device='cuda:0') ->\n",
            "tensor([[ 0.0142,  0.5116,  6.8781,  0.5116,  5.1301],\n",
            "        [ 1.7195,  1.4211,  0.3553,  0.0000,  0.0142],\n",
            "        [ 0.0568,  0.0142,  1.4211,  8.1855,  8.1855],\n",
            "        [12.7898, 12.7898,  0.0568,  0.2274,  0.2274],\n",
            "        [ 0.0142,  0.1279, 10.3597, 10.3597,  4.6043]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "tensor([[-3.0897,  0.2258,  0.0000,  0.0000,  0.0000]], device='cuda:0') ->\n",
            "tensor([[0.0000e+00, 2.2737e-01, 3.6380e+00, 2.2737e-01, 6.8781e+00],\n",
            "        [1.1511e+00, 1.2825e+00, 2.5668e-01, 3.1974e-02, 8.8818e-04],\n",
            "        [1.4211e-02, 1.4211e-02, 1.0267e+00, 8.8818e+00, 7.8479e+00],\n",
            "        [1.5242e+01, 1.5010e+01, 5.6843e-02, 7.1942e-02, 2.5668e-01],\n",
            "        [7.9936e-03, 3.1974e-02, 1.1746e+01, 1.0845e+01, 4.9960e+00]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[-0.2254, -0.0000, -0.0000, -0.4221, -0.9320]], device='cuda:0') ->\n",
            "tensor([[8.8818e-04, 0.0000e+00, 3.1974e-02, 3.5527e-03, 0.0000e+00],\n",
            "        [1.3878e-03, 2.7200e-03, 3.5527e-03, 4.9960e-04, 5.5511e-05],\n",
            "        [8.8818e-04, 0.0000e+00, 7.9936e-03, 2.2204e-02, 7.9936e-03],\n",
            "        [0.0000e+00, 8.8818e-04, 8.8818e-04, 8.8818e-04, 8.8818e-04],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9936e-03, 7.9936e-03, 3.5527e-03]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[-0.0000, -0.1902,  0.0614,  0.6395,  0.0000]], device='cuda:0') ->\n",
            "tensor([[1.0880e-02, 5.5511e-03, 1.7408e-01, 7.9936e-03, 1.7408e-01],\n",
            "        [4.3521e-02, 5.6843e-02, 1.4211e-02, 8.8818e-04, 0.0000e+00],\n",
            "        [2.2204e-04, 1.9984e-03, 3.1974e-02, 2.2737e-01, 2.1338e-01],\n",
            "        [3.5527e-01, 3.5527e-01, 0.0000e+00, 3.5527e-03, 1.4211e-02],\n",
            "        [3.5527e-03, 2.2204e-04, 2.2737e-01, 2.5668e-01, 1.2790e-01]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[-0.0000, 0.7028, -0.0000, 0.2799, -0.0000]], device='cuda:0') ->\n",
            "tensor([[1.3878e-03, 1.9984e-03, 5.6843e-02, 7.9936e-03, 1.4211e-02],\n",
            "        [1.2490e-02, 7.9936e-03, 4.4964e-03, 2.2204e-04, 0.0000e+00],\n",
            "        [2.2204e-04, 1.3878e-05, 1.7000e-02, 6.0452e-02, 3.0656e-02],\n",
            "        [4.0468e-02, 3.4694e-02, 8.8818e-04, 1.3878e-03, 2.7200e-03],\n",
            "        [2.2204e-04, 4.9960e-04, 3.7526e-02, 4.9960e-02, 2.0040e-02]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[ 0.0000, -1.0404,  0.0000, -1.3208, -0.0000]], device='cuda:0') ->\n",
            "tensor([[3.5527e-03, 1.9984e-03, 1.4211e-02, 7.9936e-03, 1.2790e-01],\n",
            "        [1.4211e-02, 3.1974e-02, 3.5527e-03, 3.5527e-03, 0.0000e+00],\n",
            "        [1.9984e-03, 3.5527e-03, 2.2204e-04, 3.7526e-02, 8.0158e-02],\n",
            "        [2.2737e-01, 3.2063e-01, 8.8818e-04, 3.5527e-03, 3.5527e-03],\n",
            "        [1.3878e-03, 4.9960e-04, 1.1746e-01, 8.4432e-02, 5.3346e-02]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[ 0.0000,  0.1265, -0.0000, -0.3556, -1.3330]], device='cuda:0') ->\n",
            "tensor([[3.5527e-03, 1.9984e-03, 1.9984e-01, 1.4211e-02, 2.1338e-01],\n",
            "        [5.6843e-02, 4.3521e-02, 1.0880e-02, 0.0000e+00, 2.2204e-04],\n",
            "        [1.4211e-02, 0.0000e+00, 1.2790e-01, 4.2988e-01, 2.2737e-01],\n",
            "        [3.5527e-01, 2.8777e-01, 0.0000e+00, 1.4211e-02, 1.4211e-02],\n",
            "        [0.0000e+00, 8.8818e-04, 2.5668e-01, 2.8777e-01, 1.7408e-01]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[0.0000, 0.0375, 0.0000, -0.0000, 0.2902]], device='cuda:0') ->\n",
            "tensor([[6.8001e-04, 1.3878e-03, 3.3321e-02, 2.3453e-03, 2.6867e-02],\n",
            "        [8.6736e-03, 7.3413e-03, 2.3453e-03, 1.3878e-05, 1.3878e-05],\n",
            "        [2.2204e-04, 0.0000e+00, 7.9936e-03, 4.3521e-02, 3.7526e-02],\n",
            "        [5.3346e-02, 5.0796e-02, 4.1980e-04, 1.3878e-03, 2.1684e-03],\n",
            "        [2.2204e-04, 2.2204e-04, 4.0468e-02, 4.0468e-02, 2.6867e-02]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "tensor([[ 1.1545, -0.3260,  0.0000,  0.0000, -0.6906]], device='cuda:0') ->\n",
            "tensor([[5.6843e-02, 5.6843e-02, 1.4211e+00, 2.2737e-01, 2.4016e+00],\n",
            "        [5.2230e-01, 4.5969e-01, 9.3314e-02, 1.3878e-03, 4.9960e-04],\n",
            "        [3.5527e-03, 1.4211e-02, 4.2988e-01, 2.9878e+00, 2.4016e+00],\n",
            "        [4.3521e+00, 4.6043e+00, 1.4211e-02, 5.6843e-02, 1.1746e-01],\n",
            "        [7.9936e-03, 3.5527e-03, 3.4142e+00, 3.3593e+00, 1.6043e+00]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for x_, y_ in DataLoader(TensorDataset(torch.Tensor(data_inputs).to('cuda')[:10], torch.Tensor(data_labels).to('cuda')[:10]),\n",
        "                        batch_size=1, shuffle=True, generator=torch.Generator(device='cuda')):\n",
        "    print(x_, '->')\n",
        "    print(perturb_model(x_).transpose(0,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pFcED-cK1mZX",
        "outputId": "cd398602-61cd-4e18-ad8d-535829dd8e40"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-391-d09de1bc5e91>, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-391-d09de1bc5e91>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    batch_size=16, shuffle=True):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(TensorDataset(torch.Tensor(data_inputs[:100]), torch.Tensor(data_labels[:100])),\n",
        "                        batch_size=16, shuffle=True):\n",
        "\n",
        "eigenmodel = TrainEigenestimation(autoencoder, .01, 100, .1, 3, dataloader, criterion=\n",
        "                                  nn.MSELoss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbXlhYea9J4_",
        "outputId": "1c4dc827-95a8-467f-af98-00b1af6e29ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.8225, -0.0000,  0.0000]]) -> [[0.6  1.39 0.56 1.47 1.04]]\n",
            "tensor([[-0.0000, -1.8652,  0.0000]]) -> [[ 4.87 14.41  5.19  5.05  2.58]]\n",
            "tensor([[0.1557, 0.0000, 0.0000]]) -> [[0.02 0.05 0.02 0.05 0.04]]\n",
            "tensor([[-0.0000,  0.4592, -1.5725]]) -> [[ 4.09  0.14  1.52 11.1  17.6 ]]\n",
            "tensor([[0.6461, -0.0000, 0.0000]]) -> [[0.37 0.86 0.35 0.91 0.64]]\n",
            "tensor([[-0.3971, -0.4347, -0.0000]]) -> [[0.27 1.01 0.62 0.56 0.02]]\n",
            "tensor([[-0.1150,  0.0000,  0.1600]]) -> [[0.01 0.05 0.11 0.14 0.1 ]]\n",
            "tensor([[-0.0000, -0.1034,  1.0576]]) -> [[1.29 0.58 1.46 4.94 7.44]]\n",
            "tensor([[0., 0., -0.]]) -> [[0. 0. 0. 0. 0.]]\n",
            "tensor([[ 2.2654, -0.9852,  0.7735]]) -> [[14.14 21.93  1.77 15.19 22.06]]\n",
            "tensor([[-0.0000, 0.2246, -0.0000]]) -> [[0.07 0.21 0.08 0.07 0.04]]\n",
            "tensor([[ 1.1602, -3.3105,  0.0000]]) -> [[19.57 50.4   4.76 20.13  4.29]]\n",
            "tensor([[0.6435, 0.0000, -0.0000]]) -> [[0.37 0.85 0.34 0.9  0.64]]\n",
            "tensor([[0.4825, 0.0000, 0.7821]]) -> [[1.73 1.34 0.07 1.03 5.99]]\n",
            "tensor([[-1.3932,  0.0000,  0.7140]]) -> [[0.29 0.36 5.72 0.93 1.71]]\n",
            "tensor([[0.4987, -0.0000, -0.0000]]) -> [[0.22 0.51 0.21 0.54 0.38]]\n",
            "tensor([[-0.6855,  0.7297, -0.0000]]) -> [[1.55 3.46 0.96 1.96 0.56]]\n",
            "tensor([[-1.0137,  1.0825, -0.1470]]) -> [[4.13 8.21 1.16 4.5  2.46]]\n",
            "tensor([[-0., -0., -0.]]) -> [[0. 0. 0. 0. 0.]]\n",
            "tensor([[ 0.2930,  0.0000, -0.8984]]) -> [[0.18 1.21 2.24 4.09 4.1 ]]\n",
            "[[-0.67  0.27  0.56 -0.62 -0.62]\n",
            " [-0.92  0.12 -0.15  0.19 -0.08]\n",
            " [-0.1  -0.7  -0.22  1.47 -0.16]\n",
            " [-0.34  0.78 -0.02 -1.54  0.41]\n",
            " [ 0.41  0.69  0.15 -0.32 -0.17]\n",
            " [ 1.67  0.07 -0.02 -0.35  1.48]\n",
            " [ 0.51  1.05 -0.13 -0.05 -1.45]\n",
            " [ 2.1   1.24  0.82 -1.22 -2.12]\n",
            " [-0.06 -0.22  0.26  0.6  -0.31]\n",
            " [-0.21 -0.31 -0.24  0.03  0.38]\n",
            " [ 1.13  1.97 -0.8  -0.12 -0.23]\n",
            " [-0.35  0.08  1.73 -1.16 -0.45]]\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(TensorDataset(torch.Tensor(data_inputs[:20]), torch.Tensor(data_labels[:20])),\n",
        "                        batch_size=1, shuffle=True)\n",
        "diags = ComputeDiagonals(autoencoder, eigenmodel, dataloader, criterion=nn.MSELoss)\n",
        "print(eigenmodel.U.data.numpy().round(2))\n",
        "#print(eigenmodel.V.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHTKFtbD30Sk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Dh7-Y2m10Oz",
        "outputId": "d9afd473-74fa-4785-bfa1-1c0a1412ef25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-392-238db4668a6a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mg_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhessian_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mhessian\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhessian_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    434\u001b[0m         )\n\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "  flat_params = list(autoencoder.parameters())\n",
        "  numel = sum([p.numel() for p in flat_params])\n",
        "  hessian = torch.zeros((10000), numel, numel)\n",
        "  outputs = autoencoder(torch.Tensor(data_inputs))\n",
        "  loss = nn.MSELoss(reduction='none')(outputs, outputs.detach()).sum(dim=1)\n",
        "  for x_i, _ in enumerate(torch.Tensor(data_inputs)):\n",
        "    grads = torch.autograd.grad(loss[x_i], flat_params, create_graph=True)\n",
        "    grads = torch.cat([g.contiguous().view(-1) for g in grads])\n",
        "    print(grads.shape)\n",
        "    for g_i, g in enumerate(grads):\n",
        "      hessian_val = torch.concat([i.flatten() for i in (torch.autograd.grad(g, flat_params, create_graph=True))])\n",
        "      hessian[x_i, g_i, :] = hessian_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK3Zxrf14fXV",
        "outputId": "6a32409c-4d59-4d53-b8ee-1fb41490165b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10000, 30, 30])"
            ]
          },
          "execution_count": 382,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hessian.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "gDAx6O0B1I4V",
        "outputId": "0fd3bed2-8136-4a9c-b2a5-ecd69554a7d2"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-307-4baf9cd9e311>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meigenmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainEigenestimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-257-84dedc9c6a8e>\u001b[0m in \u001b[0;36mTrainEigenestimation\u001b[0;34m(model, learning_rate, num_epochs, lambda_penalty, k, dataloader, criterion)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m       \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m       \u001b[0;31m# Forward passf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mH_reconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigen_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-257-84dedc9c6a8e>\u001b[0m in \u001b[0;36mHessian\u001b[0;34m(model, X, criterion)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mg_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mgrad_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     grad_outputs_ = _make_grads(\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_grads_batched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    152\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ],
      "source": [
        "\n",
        "def Hessian(model, X, criterion):\n",
        "  hessians = np.array([])\n",
        "  # Compute the loss\n",
        "  outputs = model(X)\n",
        "  loss = criterion(outputs, outputs.detach())\n",
        "\n",
        "  # Compute gradients w.r.t. parameters\n",
        "  flat_params = list(model.parameters())\n",
        "  numel = sum([p.numel() for p in flat_params])\n",
        "  hessian = torch.zeros((len(X), numel, numel))\n",
        "\n",
        "\n",
        "  for x_i, _ in enumerate(X):\n",
        "    grads = torch.autograd.grad(loss[x_i], flat_params, create_graph=True)\n",
        "    grads = torch.cat([g.contiguous().view(-1) for g in grads])\n",
        "    for g_i, g in enumerate(grads):\n",
        "      hessian_val = torch.autograd.grad(g, flat_params, create_graph=True)[0].flatten()\n",
        "      hessian[x_i, g_i, :] = torch.autograd.grad(g, flat_params, create_graph=True)[0].flatten()\n",
        "  return hessian\n",
        "\n",
        "\n",
        "eigenmodel = TrainEigenestimation(autoencoder, .001, 400, .1, 4, dataloader, criterion=nn.MSELoss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MfzYfEx5sy68"
      },
      "outputs": [],
      "source": [
        "#@title Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUC8dfkxA6qP",
        "outputId": "2d194e5d-6b2c-4248-b91e-7895f1d2e9da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.]]) -> tensor([[-0.0090]]) tensor([0.]) tensor(8.1381e-05, grad_fn=<MseLossBackward0>)\n",
            "gradient tensor([-0.0050,  0.0000,  0.0000, -0.0024])\n",
            "tensor([[0., 1.]]) -> tensor([[0.9930]]) tensor([1.0022]) tensor(8.4146e-05, grad_fn=<MseLossBackward0>)\n",
            "gradient tensor([0.0000, 0.0000, 0.0000, 0.0183])\n",
            "tensor([[1., 0.]]) -> tensor([[1.0029]]) tensor([0.9967]) tensor(3.8546e-05, grad_fn=<MseLossBackward0>)\n",
            "gradient tensor([-0.0124, -0.0000,  0.0000,  0.0000])\n",
            "tensor([[1., 1.]]) -> tensor([[0.0034]]) tensor([0.]) tensor(1.1358e-05, grad_fn=<MseLossBackward0>)\n",
            "gradient tensor([0., 0., 0., 0.])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "for i in range(Y.shape[0]):\n",
        "    # Make sure X has requires_grad=True if you want to compute its gradient\n",
        "    X_i = X[[i], :].clone().detach().requires_grad_(True)  # Clone to avoid modifying X\n",
        "    Y_i = Y[[i], :].clone().detach()  # Clone to avoid modifying X\n",
        "    Y_i = Y_i + torch.randn_like(Y_i) * .01\n",
        "    # Forward pass: compute output\n",
        "    outputs = zero_loss_model(X_i)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, Y_i)\n",
        "\n",
        "    # Backpropagation: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    print(X_i.data, '->', Y_i.data, outputs.data, loss)\n",
        "    # Access gradients of model parameters (weights, biases)\n",
        "    #print(torch.autograd.grad(flat_grads[idx], zero_loss_model.parameters(), retain_graph=True))\n",
        "    print('gradient', torch.concat([param.grad for _,param in zero_loss_model.named_parameters()]).flatten())\n",
        "    # Zero the gradients after each iteration to avoid accumulation\n",
        "    zero_loss_model.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAs_HUDCqjka",
        "outputId": "7d1789d8-1da3-4fca-85ec-aad3a06d9585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample\n",
            "gradient tensor([0., 0., 0., 0.], grad_fn=<CatBackward0>)\n",
            "tensor([[0., 0.]]) -> tensor([[-0.1257]]) tensor([0.]) tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
            "Hessian matrix for sample 0:\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "Sample\n",
            "gradient tensor([0.0000, 0.0000, 0.0000, 0.1677], grad_fn=<CatBackward0>)\n",
            "tensor([[0., 1.]]) -> tensor([[0.9183]]) tensor([1.0022]) tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
            "Hessian matrix for sample 1:\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 2.]])\n",
            "Sample\n",
            "gradient tensor([0.0748, 0.0000, 0.0000, 0.0000], grad_fn=<CatBackward0>)\n",
            "tensor([[1., 0.]]) -> tensor([[0.9593]]) tensor([0.9967]) tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "Hessian matrix for sample 2:\n",
            "tensor([[2., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "Sample\n",
            "gradient tensor([0., 0., 0., 0.], grad_fn=<CatBackward0>)\n",
            "tensor([[1., 1.]]) -> tensor([[-0.0103]]) tensor([0.]) tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
            "Hessian matrix for sample 3:\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "for i in range(Y.shape[0]):\n",
        "    # Prepare input and target data\n",
        "    X_i = X[[i], :].clone().detach().requires_grad_(False)\n",
        "    Y_i = Y[[i], :].clone().detach()\n",
        "    Y_i = Y_i + torch.randn_like(Y_i) * .1\n",
        "    zero_loss_model.zero_grad()\n",
        "    # Compute the loss\n",
        "    outputs = zero_loss_model(X_i)\n",
        "    loss = criterion(outputs, Y_i)\n",
        "\n",
        "    # Compute gradients w.r.t. parameters\n",
        "    grads = torch.autograd.grad(loss, zero_loss_model.parameters(), create_graph=True)\n",
        "    flat_grads = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "    # Compute the Hessian matrix\n",
        "    num_params = flat_grads.shape[0]\n",
        "    hessian_matrix = torch.zeros(num_params, num_params)\n",
        "    for idx in range(num_params):\n",
        "        grad2 = torch.autograd.grad(flat_grads[idx], zero_loss_model.parameters(), retain_graph=True)\n",
        "        grad2_flat = torch.cat([g.contiguous().view(-1) for g in grad2])\n",
        "        hessian_matrix[idx] = grad2_flat.detach()\n",
        "\n",
        "    # Print the Hessian matrix\n",
        "    print('Sample')\n",
        "    print('gradient', flat_grads)\n",
        "    print(X_i.data, '->', Y_i.data, outputs.data, loss)\n",
        "\n",
        "\n",
        "    print(f\"Hessian matrix for sample {i}:\")\n",
        "    print(hessian_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCS1b7qUsdgW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SysXU1-EdQB5"
      },
      "source": [
        "# Eigenestimation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTl6zKbE9ZyY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LIJwtm5FmOE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zozubkoCFmLy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "188Re0rJHpaG",
        "outputId": "9ad856c5-05fc-4a89-f1db-9c528bc78eb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0.]])"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B80ItL5sEwPJ",
        "outputId": "916e468d-0707-459c-9373-e3b7a95c0878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch0\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]]) tensor([[0., 0.]], grad_fn=<DiagonalBackward0>) tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]], grad_fn=<UnsafeViewBackward0>) tensor([[0., 0.]], grad_fn=<DiagonalBackward0>)\n",
            "Epoch [1/2], Total Loss: 0.000000, Reconstruction Loss: 0.000000, Penalty Term: 0.000000\n",
            "epoch1\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]]) tensor([[0., 0.]], grad_fn=<DiagonalBackward0>) tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]], grad_fn=<UnsafeViewBackward0>) tensor([[0., 0.]], grad_fn=<DiagonalBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EigenEstimation()"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IudwqWOdYQR"
      },
      "source": [
        "# Polytope Toy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coUIRAO5daNC",
        "outputId": "48140acb-3829-49a5-f8be-6ae8473c6f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 13.4891\n",
            "Epoch 2/100, Loss: 1.7475\n",
            "Epoch 3/100, Loss: 1.3540\n",
            "Epoch 4/100, Loss: 1.1823\n",
            "Epoch 5/100, Loss: 1.0321\n",
            "Epoch 6/100, Loss: 0.9037\n",
            "Epoch 7/100, Loss: 0.7883\n",
            "Epoch 8/100, Loss: 0.6941\n",
            "Epoch 9/100, Loss: 0.6105\n",
            "Epoch 10/100, Loss: 0.5438\n",
            "Epoch 11/100, Loss: 0.4914\n",
            "Epoch 12/100, Loss: 0.4435\n",
            "Epoch 13/100, Loss: 0.4054\n",
            "Epoch 14/100, Loss: 0.3700\n",
            "Epoch 15/100, Loss: 0.3422\n",
            "Epoch 16/100, Loss: 0.3228\n",
            "Epoch 17/100, Loss: 0.3079\n",
            "Epoch 18/100, Loss: 0.2892\n",
            "Epoch 19/100, Loss: 0.2814\n",
            "Epoch 20/100, Loss: 0.2753\n",
            "Epoch 21/100, Loss: 0.2604\n",
            "Epoch 22/100, Loss: 0.2559\n",
            "Epoch 23/100, Loss: 0.2486\n",
            "Epoch 24/100, Loss: 0.2395\n",
            "Epoch 25/100, Loss: 0.2392\n",
            "Epoch 26/100, Loss: 0.2329\n",
            "Epoch 27/100, Loss: 0.2230\n",
            "Epoch 28/100, Loss: 0.2163\n",
            "Epoch 29/100, Loss: 0.2127\n",
            "Epoch 30/100, Loss: 0.2083\n",
            "Epoch 31/100, Loss: 0.2034\n",
            "Epoch 32/100, Loss: 0.1977\n",
            "Epoch 33/100, Loss: 0.1954\n",
            "Epoch 34/100, Loss: 0.1927\n",
            "Epoch 35/100, Loss: 0.1904\n",
            "Epoch 36/100, Loss: 0.1879\n",
            "Epoch 37/100, Loss: 0.1830\n",
            "Epoch 38/100, Loss: 0.1826\n",
            "Epoch 39/100, Loss: 0.1773\n",
            "Epoch 40/100, Loss: 0.1755\n",
            "Epoch 41/100, Loss: 0.1705\n",
            "Epoch 42/100, Loss: 0.1678\n",
            "Epoch 43/100, Loss: 0.1682\n",
            "Epoch 44/100, Loss: 0.1675\n",
            "Epoch 45/100, Loss: 0.1600\n",
            "Epoch 46/100, Loss: 0.1615\n",
            "Epoch 47/100, Loss: 0.1575\n",
            "Epoch 48/100, Loss: 0.1577\n",
            "Epoch 49/100, Loss: 0.1548\n",
            "Epoch 50/100, Loss: 0.1523\n",
            "Epoch 51/100, Loss: 0.1490\n",
            "Epoch 52/100, Loss: 0.1513\n",
            "Epoch 53/100, Loss: 0.1461\n",
            "Epoch 54/100, Loss: 0.1438\n",
            "Epoch 55/100, Loss: 0.1435\n",
            "Epoch 56/100, Loss: 0.1418\n",
            "Epoch 57/100, Loss: 0.1444\n",
            "Epoch 58/100, Loss: 0.1399\n",
            "Epoch 59/100, Loss: 0.1392\n",
            "Epoch 60/100, Loss: 0.1383\n",
            "Epoch 61/100, Loss: 0.1357\n",
            "Epoch 62/100, Loss: 0.1378\n",
            "Epoch 63/100, Loss: 0.1347\n",
            "Epoch 64/100, Loss: 0.1340\n",
            "Epoch 65/100, Loss: 0.1343\n",
            "Epoch 66/100, Loss: 0.1310\n",
            "Epoch 67/100, Loss: 0.1322\n",
            "Epoch 68/100, Loss: 0.1300\n",
            "Epoch 69/100, Loss: 0.1287\n",
            "Epoch 70/100, Loss: 0.1278\n",
            "Epoch 71/100, Loss: 0.1301\n",
            "Epoch 72/100, Loss: 0.1266\n",
            "Epoch 73/100, Loss: 0.1277\n",
            "Epoch 74/100, Loss: 0.1245\n",
            "Epoch 75/100, Loss: 0.1267\n",
            "Epoch 76/100, Loss: 0.1243\n",
            "Epoch 77/100, Loss: 0.1226\n",
            "Epoch 78/100, Loss: 0.1232\n",
            "Epoch 79/100, Loss: 0.1219\n",
            "Epoch 80/100, Loss: 0.1212\n",
            "Epoch 81/100, Loss: 0.1195\n",
            "Epoch 82/100, Loss: 0.1194\n",
            "Epoch 83/100, Loss: 0.1186\n",
            "Epoch 84/100, Loss: 0.1186\n",
            "Epoch 85/100, Loss: 0.1168\n",
            "Epoch 86/100, Loss: 0.1203\n",
            "Epoch 87/100, Loss: 0.1175\n",
            "Epoch 88/100, Loss: 0.1178\n",
            "Epoch 89/100, Loss: 0.1186\n",
            "Epoch 90/100, Loss: 0.1166\n",
            "Epoch 91/100, Loss: 0.1144\n",
            "Epoch 92/100, Loss: 0.1153\n",
            "Epoch 93/100, Loss: 0.1137\n",
            "Epoch 94/100, Loss: 0.1126\n",
            "Epoch 95/100, Loss: 0.1130\n",
            "Epoch 96/100, Loss: 0.1130\n",
            "Epoch 97/100, Loss: 0.1129\n",
            "Epoch 98/100, Loss: 0.1100\n",
            "Epoch 99/100, Loss: 0.1094\n",
            "Epoch 100/100, Loss: 0.1104\n",
            "Test Loss: 0.1720\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Step 1: Define the function f(x)\n",
        "\n",
        "# Parameters\n",
        "n = 10   # Dimensionality of input vector x\n",
        "K = 20   # Number of linear functions (facets of the polytope)\n",
        "N = 10000  # Number of samples\n",
        "\n",
        "# Generate random coefficients for the linear functions\n",
        "np.random.seed(0)\n",
        "a = np.random.randn(K, n)  # Shape: (K, n)\n",
        "b = np.random.randn(K)     # Shape: (K,)\n",
        "\n",
        "def f(x):\n",
        "    # x: numpy array of shape (n,)\n",
        "    # Compute f(x) = max_{i} (a_i^T x + b_i)\n",
        "    return np.max(np.dot(a, x) + b)\n",
        "\n",
        "# Step 2: Generate input-output pairs\n",
        "\n",
        "# Generate random input vectors X\n",
        "X = np.random.randn(N, n)  # Shape: (N, n)\n",
        "\n",
        "# Compute corresponding outputs Y\n",
        "Y = np.array([f(xi) for xi in X])  # Shape: (N,)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X).float()\n",
        "Y_tensor = torch.from_numpy(Y).float().unsqueeze(1)  # Shape: (N, 1)\n",
        "\n",
        "# Step 3: Define the neural network model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_size = h\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = n\n",
        "hidden_sizes = [64, 64]  # You can adjust the hidden layer sizes\n",
        "output_size = 1\n",
        "\n",
        "polytope_model = Net(input_size, hidden_sizes, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(polytope_model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch_X, batch_Y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = polytope_model(batch_X)\n",
        "        loss = criterion(outputs, batch_Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "    total_loss /= len(dataset)\n",
        "    #print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Testing the model (optional)\n",
        "# Generate new test data\n",
        "X_test = np.random.randn(1000, n)\n",
        "Y_test = np.array([f(xi) for xi in X_test])\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).float()\n",
        "Y_test_tensor = torch.from_numpy(Y_test).float().unsqueeze(1)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "polytope_model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred = polytope_model(X_test_tensor)\n",
        "    test_loss = criterion(Y_pred, Y_test_tensor)\n",
        "    #print(f\"Test Loss: {test_loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "3CdrF07zeSrr",
        "outputId": "de9bec93-5d3c-48ca-d2b8-e8a9fffac902"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'generator' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-658f53f2d919>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Compute gradients of outputs w.r.t inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolytope_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: [batch_size, n]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Compute Hessians for each sample in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "A_tensor = torch.Tensor(hessians)\n",
        "n = A_tensor.shape[1]  # Dimension of the matrices\n",
        "k = 2  # You can set k < n or k > n depending on your needs\n",
        "learning_rate = 1e-1\n",
        "num_epochs = 1000\n",
        "lambda_penalty = 0.1  # Adjust as needed\n",
        "\n",
        "# Create model\n",
        "model = EigenEstimation(n, k)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Make a dataloader from X_train and Y_train\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for X_batch, _ in dataloader:\n",
        "        X_batch.requires_grad = True  # Enable gradients w.r.t inputs\n",
        "\n",
        "        # Compute outputs from polytope_model\n",
        "        outputs = polytope_model(X_batch)\n",
        "        outputs = outputs.squeeze()  # Ensure outputs have correct shape\n",
        "\n",
        "        # Compute gradients of outputs w.r.t inputs\n",
        "        grads = torch.autograd.grad((outputs-outputs.detach())**2, polytope_model.parameters()[i], torch.ones_like(outputs), create_graph=True)[0]  # Shape: [batch_size, n]\n",
        "\n",
        "        # Compute Hessians for each sample in the batch\n",
        "        batch_hessians = []\n",
        "        for i in range(X_batch.size(0)):\n",
        "            grad_i = grads[i]  # Shape: [n]\n",
        "            hessian_i = []\n",
        "            for j in range(n):\n",
        "                grad2 = torch.autograd.grad(grad_i[j], polytope_model.parameters()[i], retain_graph=True)[0]  # Shape: [n]\n",
        "                hessian_i.append(grad2.unsqueeze(0))  # Shape: [1, n]\n",
        "            hessian_i = torch.cat(hessian_i, dim=0)  # Shape: [n, n]\n",
        "            batch_hessians.append(hessian_i.detach())  # Detach to prevent further gradients\n",
        "        batch_hessians = torch.stack(batch_hessians)  # Shape: [batch_size, n, n]\n",
        "\n",
        "        # Forward pass through EigenEstimation model\n",
        "        A_reconstructed, diag_elements = model(batch_hessians)\n",
        "\n",
        "        # Compute loss\n",
        "        total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "            batch_hessians, A_reconstructed, diag_elements, lambda_penalty\n",
        "        )\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total Loss: {total_loss.item():.6f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_loss.item():.6f}, \"\n",
        "              f\"Penalty Term: {penalty_term.item():.6f}\")\n",
        "\n",
        "# After training, get the reconstructed A and diag elements\n",
        "# You can use a batch from the dataloader for demonstration\n",
        "X_batch, _ = next(iter(dataloader))\n",
        "X_batch.requires_grad = True\n",
        "outputs = polytope_model(X_batch)\n",
        "outputs = outputs.squeeze()\n",
        "grads = torch.autograd.grad(outputs, X_batch, torch.ones_like(outputs), create_graph=True)[0]\n",
        "\n",
        "batch_hessians = []\n",
        "for i in range(X_batch.size(0)):\n",
        "    grad_i = grads[i]\n",
        "    hessian_i = []\n",
        "    for j in range(n):\n",
        "        grad2 = torch.autograd.grad(grad_i[j], X_batch[i], retain_graph=True)[0]\n",
        "        hessian_i.append(grad2.unsqueeze(0))\n",
        "    hessian_i = torch.cat(hessian_i, dim=0)\n",
        "    batch_hessians.append(hessian_i.detach())\n",
        "batch_hessians = torch.stack(batch_hessians)\n",
        "\n",
        "A_reconstructed, diags = model(batch_hessians)\n",
        "print(diags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXT89UrcdpSB"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "A_tensor = torch.Tensor(hessians)\n",
        "n = A_tensor.shape[1]  # Dimension of the matrices\n",
        "k = 2  # You can set k < n or k > n depending on your needs\n",
        "learning_rate = 1e-1\n",
        "num_epochs = 1000\n",
        "lambda_penalty = 0.1  # Adjust as needed\n",
        "\n",
        "# Create model\n",
        "model = MatrixReconstructionModel(n, k)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    A_reconstructed, diag_elements = model(A_tensor)\n",
        "\n",
        "    # Compute loss\n",
        "    total_loss, reconstruction_loss, penalty_term = total_loss_function(\n",
        "        A_tensor, A_reconstructed, diag_elements, lambda_penalty\n",
        "    )\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Total Loss: {total_loss.item():.6f}, \"\n",
        "              f\"Reconstruction Loss: {reconstruction_loss.item():.6f}, \"\n",
        "              f\"Penalty Term: {penalty_term.item():.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "A_reconstructed, diags = model(A_tensor)\n",
        "print(diags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "y6gotnMwWBLa",
        "outputId": "f479e2e3-49bc-4d05-d6fd-8b2768d7fcfc"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'datasets' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-33a8d517fc55>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'datasets' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, datasets\n",
        "\n",
        "from torch.datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'distilgpt2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "\n",
        "# Preprocess the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text'],\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 48  # Adjust based on your computational resources\n",
        "data_loader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: {\n",
        "        'input_ids': torch.stack([torch.tensor(e['input_ids']) for e in x]),\n",
        "        'attention_mask': torch.stack([torch.tensor(e['attention_mask']) for e in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_zmK9OWcDwC"
      },
      "outputs": [],
      "source": [
        "class MatrixReconstructionModel(nn.Module):\n",
        "    def __init__(self, n, k):\n",
        "        super(MatrixReconstructionModel, self).__init__()\n",
        "        self.B = nn.Parameter(torch.randn(n, k))\n",
        "\n",
        "    def forward(self, A):\n",
        "        B = self.B  # [n, k]\n",
        "        B_t = B.transpose(0, 1)  # [k, n]\n",
        "\n",
        "        # Compute M = B^T A B\n",
        "        M = torch.matmul(B_t.unsqueeze(0), torch.matmul(A, B.unsqueeze(0)))  # [batch_size, k, k]\n",
        "\n",
        "        # Extract the diagonal elements of M\n",
        "        diag_elements = torch.diagonal(M, dim1=-2, dim2=-1)  # [batch_size, k]\n",
        "\n",
        "        # Create diagonal matrices from the diagonal elements\n",
        "        D = torch.zeros_like(M)\n",
        "        indices = torch.arange(M.size(-1))\n",
        "        D[:, indices, indices] = diag_elements\n",
        "\n",
        "        # Reconstruct A\n",
        "        A_reconstructed = torch.matmul(B.unsqueeze(0), torch.matmul(D, B_t.unsqueeze(0)))  # [batch_size, n, n]\n",
        "\n",
        "        return A_reconstructed, diag_elements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDJeHVoc6w54",
        "outputId": "d79a55c6-572a-4512-f1f4-8376b3281418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients of inputs: tensor([[ 3.7544, -3.8698]])\n",
            "Gradients of fc1.weight: tensor([[  0.0000,   0.0000],\n",
            "        [ -9.2799, -18.5598]])\n",
            "Gradients of fc2.weight: tensor([[ 0.0000, -6.8964]])\n"
          ]
        }
      ],
      "source": [
        "# Define a loss function (e.g., Mean Squared Error)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define an example input and target\n",
        "inputs = torch.tensor([[1.0, 2.0]], requires_grad=True)  # Input vector\n",
        "target = torch.tensor([[3.0]])  # Target value\n",
        "\n",
        "# Perform a forward pass (get model prediction)\n",
        "output = model(inputs)\n",
        "\n",
        "# Compute the loss\n",
        "loss = loss_fn(output, target)\n",
        "\n",
        "# Perform backpropagation to compute the gradient of the loss\n",
        "loss.backward()\n",
        "\n",
        "# Access the gradients\n",
        "print(f\"Gradients of inputs: {inputs.grad}\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Gradients of {name}: {param.grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "2dLUEg158bt4",
        "outputId": "69c9d06d-a23c-4f49-b310-7444d0d47ce0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e461b622-7741-45a9-938f-1de8349fd5a1\" class=\"plotly-graph-div\" style=\"height:800px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e461b622-7741-45a9-938f-1de8349fd5a1\")) {                    Plotly.newPlot(                        \"e461b622-7741-45a9-938f-1de8349fd5a1\",                        [{\"x\":[[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0],[-10.0,-9.797979797979798,-9.595959595959595,-9.393939393939394,-9.191919191919192,-8.98989898989899,-8.787878787878787,-8.585858585858587,-8.383838383838384,-8.181818181818182,-7.979797979797979,-7.777777777777778,-7.575757575757576,-7.373737373737374,-7.171717171717171,-6.96969696969697,-6.767676767676768,-6.565656565656566,-6.363636363636363,-6.161616161616162,-5.959595959595959,-5.757575757575758,-5.555555555555555,-5.353535353535354,-5.151515151515151,-4.94949494949495,-4.747474747474747,-4.545454545454546,-4.343434343434343,-4.141414141414142,-3.9393939393939394,-3.737373737373738,-3.5353535353535355,-3.333333333333333,-3.1313131313131315,-2.929292929292929,-2.7272727272727275,-2.525252525252525,-2.3232323232323235,-2.121212121212121,-1.9191919191919187,-1.717171717171718,-1.5151515151515156,-1.3131313131313131,-1.1111111111111107,-0.9090909090909101,-0.7070707070707076,-0.5050505050505052,-0.30303030303030276,-0.10101010101010033,0.10101010101010033,0.30303030303030276,0.5050505050505052,0.7070707070707076,0.9090909090909083,1.1111111111111107,1.3131313131313131,1.5151515151515156,1.7171717171717162,1.9191919191919187,2.121212121212121,2.3232323232323235,2.525252525252524,2.7272727272727266,2.929292929292929,3.1313131313131315,3.333333333333334,3.5353535353535346,3.737373737373737,3.9393939393939394,4.141414141414142,4.3434343434343425,4.545454545454545,4.747474747474747,4.94949494949495,5.1515151515151505,5.353535353535353,5.555555555555555,5.757575757575758,5.9595959595959584,6.161616161616163,6.363636363636363,6.565656565656564,6.767676767676768,6.969696969696969,7.171717171717173,7.373737373737374,7.575757575757574,7.777777777777779,7.979797979797979,8.18181818181818,8.383838383838384,8.585858585858585,8.787878787878789,8.98989898989899,9.19191919191919,9.393939393939394,9.595959595959595,9.7979797979798,10.0]],\"y\":[[-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0,-10.0],[-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798,-9.797979797979798],[-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595,-9.595959595959595],[-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394,-9.393939393939394],[-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192,-9.191919191919192],[-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899,-8.98989898989899],[-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787,-8.787878787878787],[-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587,-8.585858585858587],[-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384,-8.383838383838384],[-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182,-8.181818181818182],[-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979,-7.979797979797979],[-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778,-7.777777777777778],[-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576,-7.575757575757576],[-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374,-7.373737373737374],[-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171,-7.171717171717171],[-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697,-6.96969696969697],[-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768,-6.767676767676768],[-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566,-6.565656565656566],[-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363,-6.363636363636363],[-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162,-6.161616161616162],[-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959,-5.959595959595959],[-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758,-5.757575757575758],[-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555,-5.555555555555555],[-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354,-5.353535353535354],[-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151,-5.151515151515151],[-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495,-4.94949494949495],[-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747,-4.747474747474747],[-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546,-4.545454545454546],[-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343,-4.343434343434343],[-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142,-4.141414141414142],[-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394,-3.9393939393939394],[-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738,-3.737373737373738],[-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355,-3.5353535353535355],[-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333,-3.333333333333333],[-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315,-3.1313131313131315],[-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929,-2.929292929292929],[-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275,-2.7272727272727275],[-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525,-2.525252525252525],[-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235,-2.3232323232323235],[-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121,-2.121212121212121],[-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187,-1.9191919191919187],[-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718,-1.717171717171718],[-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156,-1.5151515151515156],[-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131,-1.3131313131313131],[-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107,-1.1111111111111107],[-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101,-0.9090909090909101],[-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076,-0.7070707070707076],[-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052,-0.5050505050505052],[-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276,-0.30303030303030276],[-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033,-0.10101010101010033],[0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033,0.10101010101010033],[0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276,0.30303030303030276],[0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052,0.5050505050505052],[0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076,0.7070707070707076],[0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083,0.9090909090909083],[1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107,1.1111111111111107],[1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131,1.3131313131313131],[1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156,1.5151515151515156],[1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162,1.7171717171717162],[1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187,1.9191919191919187],[2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121,2.121212121212121],[2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235,2.3232323232323235],[2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524,2.525252525252524],[2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266,2.7272727272727266],[2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929,2.929292929292929],[3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315,3.1313131313131315],[3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334,3.333333333333334],[3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346,3.5353535353535346],[3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737,3.737373737373737],[3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394,3.9393939393939394],[4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142,4.141414141414142],[4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425,4.3434343434343425],[4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545,4.545454545454545],[4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747,4.747474747474747],[4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495,4.94949494949495],[5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505,5.1515151515151505],[5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353,5.353535353535353],[5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555,5.555555555555555],[5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758,5.757575757575758],[5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584,5.9595959595959584],[6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163,6.161616161616163],[6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363,6.363636363636363],[6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564,6.565656565656564],[6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768,6.767676767676768],[6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969,6.969696969696969],[7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173,7.171717171717173],[7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374,7.373737373737374],[7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574,7.575757575757574],[7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779,7.777777777777779],[7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979,7.979797979797979],[8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818,8.18181818181818],[8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384,8.383838383838384],[8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585,8.585858585858585],[8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789,8.787878787878789],[8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899,8.98989898989899],[9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919,9.19191919191919],[9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394,9.393939393939394],[9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595,9.595959595959595],[9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798,9.7979797979798],[10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0]],\"z\":[[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]],\"type\":\"surface\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"zaxis\":{\"title\":{\"text\":\"Loss\"},\"range\":[-1,20]},\"xaxis\":{\"title\":{\"text\":\"Alpha (Direction 1)\"}},\"yaxis\":{\"title\":{\"text\":\"Beta (Direction 2)\"}}},\"title\":{\"text\":\"Loss Landscape Visualization\"},\"autosize\":false,\"width\":800,\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e461b622-7741-45a9-938f-1de8349fd5a1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# Function to compute the loss for given parameters\n",
        "def compute_loss(model, X, Y):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, Y)\n",
        "    return loss.item()\n",
        "\n",
        "# Flatten all model parameters into a single vector\n",
        "def get_flat_params(model):\n",
        "    return torch.cat([param.view(-1) for param in model.parameters()])\n",
        "\n",
        "# Set the model's parameters from a flattened vector\n",
        "def set_flat_params(model, flat_params):\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        param_length = param.numel()\n",
        "        param.data = flat_params[idx:idx + param_length].view_as(param).data\n",
        "        idx += param_length\n",
        "\n",
        "# Get the current weights of the model\n",
        "initial_params = get_flat_params(model)\n",
        "\n",
        "# Generate two random direction vectors in the weight space\n",
        "direction1 = torch.randn_like(initial_params)\n",
        "direction2 = torch.randn_like(initial_params)\n",
        "\n",
        "# Normalize the directions to ensure consistent step sizes\n",
        "direction1 /= direction1.norm()\n",
        "direction2 /= direction2.norm()\n",
        "\n",
        "# Generate a grid of values by perturbing the weights along the two directions\n",
        "steps = 100\n",
        "alpha_range = np.linspace(-10,10, steps)\n",
        "beta_range = np.linspace(-10, 10, steps)\n",
        "loss_surface = np.zeros((steps, steps))\n",
        "\n",
        "# Compute the loss for each combination of alpha and beta\n",
        "for i, alpha in enumerate(alpha_range):\n",
        "    for j, beta in enumerate(beta_range):\n",
        "        # Perturb the weights along the two directions\n",
        "        perturbed_params = initial_params + alpha * direction1 + beta * direction2\n",
        "        set_flat_params(model, perturbed_params)\n",
        "        loss_surface[i, j] = compute_loss(model, X, Y)\n",
        "\n",
        "# Create an interactive 3D plot using Plotly\n",
        "alpha_grid, beta_grid = np.meshgrid(alpha_range, beta_range)\n",
        "fig = go.Figure(data=[go.Surface(z=loss_surface, x=alpha_grid, y=beta_grid)])\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    title=\"Loss Landscape Visualization\",\n",
        "    scene = dict(\n",
        "        xaxis_title=\"Alpha (Direction 1)\",\n",
        "        yaxis_title=\"Beta (Direction 2)\",\n",
        "        zaxis_title=\"Loss\",\n",
        "        zaxis=dict(range=[-1, 20])\n",
        "\n",
        "        ),\n",
        "    autosize=False,\n",
        "    width=800,\n",
        "    height=800,\n",
        ")\n",
        "\n",
        "# Add an x limit\n",
        "#fig.update_layout(\n",
        "#    xaxis=dict(range=[-30, 30]),\n",
        "#)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcLbB93K9mHF",
        "outputId": "8a6987cc-631d-43cc-e79c-249c30f8611d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[  5.2627,   3.5271],\n",
            "        [  4.2118,   4.3789],\n",
            "        [  7.4239, -12.7311]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-10.4186,  -5.4016,   6.2350], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ -9.6496,  15.8645, -14.6110]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([2.5999], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Print model params\n",
        "for param in model.parameters(): print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmZj435KcbFr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "SdKBUJndIsBy",
        "outputId": "9b14c2af-5e8b-4519-9b38-7953f814fcf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.7211213111877441\n",
            "Epoch 10, Loss: 0.707047700881958\n",
            "Epoch 20, Loss: 0.7000093460083008\n",
            "Epoch 30, Loss: 0.6965289115905762\n",
            "Epoch 40, Loss: 0.6948180198669434\n",
            "Epoch 50, Loss: 0.693979024887085\n",
            "Epoch 60, Loss: 0.6935676336288452\n",
            "Epoch 70, Loss: 0.6933650970458984\n",
            "Epoch 80, Loss: 0.6932646036148071\n",
            "Epoch 90, Loss: 0.693213701248169\n",
            "Parameter containing:\n",
            "tensor([[-0.1312, -0.1574],\n",
            "        [-0.5697,  0.0834]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0364, -0.1856], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.1684,  0.6222]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1806], requires_grad=True)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the toy neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)  # Input: 2 features (XOR inputs), 2 hidden units\n",
        "        self.fc2 = nn.Linear(2, 1)  # Output: 1 scalar output\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# XOR dataset\n",
        "#X = torch.rand_like(100,2)\n",
        "#Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleNet()\n",
        "# Initialize model parameters to random values\n",
        "#for param in model.parameters():\n",
        "#    param.data = -.5+torch.rand_like(param)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=.1)\n",
        "\n",
        "dataloader = DataLoader(TensorDataset(X, Y), batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "  for x_batch, y_batch in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X)\n",
        "    # Calculate loss without modifying y_batch\n",
        "    loss = criterion(outputs, Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if epoch % 10 == 0:\n",
        "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Observe if it converges to a local minimum\n",
        "Y_pred = model(X)\n",
        "\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=Y_pred.flatten().float().detach().numpy())\n",
        "\n",
        "for param in model.parameters(): print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beDmkNw7fJWi",
        "outputId": "d21c759f-bb4b-4ad9-f091-06b61145e5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.6940107345581055\n",
            "Epoch 1000, Loss: 0.6926224827766418\n",
            "Epoch 2000, Loss: 0.6910852193832397\n",
            "Epoch 3000, Loss: 0.6782428026199341\n",
            "Epoch 4000, Loss: 0.4693261981010437\n",
            "Epoch 5000, Loss: 0.1397436559200287\n",
            "Epoch 6000, Loss: 0.06500802934169769\n",
            "Epoch 7000, Loss: 0.040623798966407776\n",
            "Epoch 8000, Loss: 0.029159536585211754\n",
            "Epoch 9000, Loss: 0.022612782195210457\n",
            "\n",
            "Predictions after training:\n",
            "tensor([[0.0179],\n",
            "        [0.9761],\n",
            "        [0.9839],\n",
            "        [0.0150]])\n",
            "\n",
            "Binary Predictions (Rounded):\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define the neural network\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)  # Input layer -> Hidden layer (2 neurons)\n",
        "        self.fc2 = nn.Linear(2, 1)  # Hidden layer -> Output layer (1 neuron)\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.fc1(x))  # Apply sigmoid after first layer\n",
        "        x = self.sigmoid(self.fc2(x))  # Apply sigmoid after second layer\n",
        "        return x\n",
        "\n",
        "# XOR dataset (inputs and expected outputs)\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)  # XOR inputs\n",
        "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)  # XOR outputs\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = XORNet()\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n",
        "\n",
        "# DataLoader for batch processing\n",
        "dataloader = DataLoader(TensorDataset(X, Y), batch_size=4, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10000):  # Train for 10,000 epochs\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(x_batch)  # Forward pass\n",
        "        loss = criterion(outputs, y_batch)  # Calculate loss\n",
        "        loss.backward()  # Backward pass (compute gradients)\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Test the trained model\n",
        "with torch.no_grad():\n",
        "    predictions = model(X)\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    print(predictions)\n",
        "\n",
        "    # Round the predictions to 0 or 1 for binary classification\n",
        "    binary_predictions = torch.round(predictions)\n",
        "    print(\"\\nBinary Predictions (Rounded):\")\n",
        "    print(binary_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "U38NZltgAo8T",
        "outputId": "da252ba9-4b51-4fc8-d1c3-9091af9c0568"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACGL0lEQVR4nO3dd3hUVfoH8O/MJDPpvZMQeiBAIIRipKosRawoChawN5BVlEV2FduuIKx9se2q6Iqg7s+KigJKUXoJJUBogYRUUid9MjPn98fkXjIQIGVm7pTv53nmeci9d+49N8Nk3jnnPe9RCSEEiIiIiDyYWukGEBERESmNARERERF5PAZERERE5PEYEBEREZHHY0BEREREHo8BEREREXk8BkRERETk8RgQERERkcdjQEREREQejwERUQeZzWb069cP//jHPxx+7cbGRiQkJODtt992+LVtaf369VCpVFi/fn27z7F9+3ZotVqcOnXKdg27iNWrVyMgIABnzpxxyPWIyL4YEJHLu+OOO+Dj44MjR46ct2/RokVQqVRYtWqV3a6/YsUK5ObmYtasWVbbGxoaMG/ePMTFxcHX1xfDhg3DmjVrWnXOgoICPPXUU7jiiisQGBh4wWDB29sbc+bMwT/+8Q/U19fb4nZc1t/+9jdMmzYNiYmJDrnehAkT0KNHDyxcuNAh17OFl156Cd98843SzbC5zz//HHfccQd69uwJlUqFMWPGXPDYjrwvASAvLw+33HILQkJCEBQUhOuvvx4nTpywwV2Q4gSRiysqKhKhoaHiiiuusNp+4sQJ4evrK2666Sa7Xn/AgAHigQceOG/71KlThZeXl3jyySfFe++9J9LT04WXl5fYtGnTJc/522+/CQCiZ8+eIj09XQAQv/32W4vHlpeXC61WKz744IOO3opiTCaTqKurEyaTqV3P37NnjwAgNm/ebOOWXdzbb78t/Pz8hF6vd+h128vf31/MmDFD6WbY3OjRo0VAQIC44oorRGhoqBg9evQFj+3I+7Kqqkr07NlTREVFiZdfflm8+uqrIiEhQcTHx4uSkhIb3hEpgQERuYX3339fABDLli2Tt02YMEEEBQWJ06dP2+26u3fvFgDE2rVrrbZv27ZNABBLliyRt9XV1Ynu3buL9PT0S55Xr9eL0tJSIYQQX3755UUDIiGEuOaaa8TIkSPbdxNuYPbs2aJz587CbDY79LpFRUVCo9G4TDDaloCourravo2xoZycHDmY7tu37wUDoo6+L19++WUBQGzfvl3edujQIaHRaMT8+fM7dhOkOAZE5BbMZrMYPny4iIiIECUlJWLFihUCgHjzzTftet0FCxYIrVYrDAaD1fa5c+cKjUYjKisrrba/9NJLAoDIyclp9TVaExC98cYbQqVSyUFUW0yfPl2Eh4efdw9CCPGnP/1J9OrVq83nPNeKFSvEoEGDREBAgAgMDBT9+vUTr7/+urxf6hFrfo+jR48Wffv2FZmZmWLMmDHC19dXxMXFiZdffvm883fu3FncddddVtsef/xxERYWZhUkzZo1SwAQb7zxhrytsLBQABBvv/22EMLy+9DpdOLgwYNW5xs3bpwICQkReXl5VttTU1PFdddd1/ZfygXceOONIjU11WrbNddcIwCIb7/9Vt62detWAUD8+OOPrTovgPMeUnD07LPPCgAiMzNTTJs2TYSEhIiBAwcKISyvQ0sBxowZM0RiYqLVNpPJJF577TWRnJwsdDqdiIqKEg888IAoKytr/S+ggy4WEHX0fTlkyBAxZMiQ87aPGzdOdO/evd1tJufAHCJyCyqVCu+99x4qKyvx8MMP4/HHH8fgwYMxc+ZMu1538+bN6NevH7y9va2279mzB7169UJQUJDV9qFDhwIAMjIybNqOtLQ0CCGwefPmNj/3zjvvRGlpKX7++Wer7YWFhfj1119xxx13yNsqKytRUlJyyUd1dbX8nDVr1mDatGkIDQ3Fyy+/jEWLFmHMmDH4448/Ltm28vJyTJgwAQMGDMArr7yC3r17Y968efjpp5/kY/Ly8pCTk4NBgwZZPXfkyJEoKytDZmamvG3Tpk1Qq9XYtGmT1TYAGDVqFADgjTfeQGRkJGbMmAGTyQQAeO+99/DLL7/grbfeQlxcnNV10tLS2vV7v5CRI0di79690Ov1AAAhBP74448W261WqzF8+PBWnfe///0vdDodRo4cif/+97/473//iwcffNDqmClTpqC2thYvvfQS7r///ja3/cEHH8TcuXMxfPhwvPHGG7j77ruxfPlyjB8/Ho2NjfJx1dXVrfp/VFlZ2eY2XExH3pdmsxn79u3D4MGDz9s3dOhQHD9+HFVVVTZtLzmY0hEZkS3Nnz9fABAajUbs2rWrzc83m80XzQepqKiw+jk+Pr7FHKW+ffuKK6+88rztmZmZAoB49913W92m1vQQ5efnCwAt9p5cislkEvHx8eLWW2+12v7qq68KlUolTpw4IW8bPXp0iz0N5z6aD8v8+c9/FkFBQcJoNF6wDRfqIQIgPvnkE3lbQ0ODiImJsfqdr127VgAQ33//vdU5i4uLrXp+KioqhFqtFlOmTBHR0dHycbNnzz6vJ+nnn38WAMTf//53ceLECREQECBuuOGGFtsu9S4UFRVd8P7aYseOHVY9P/v27RMAxJQpU8SwYcPk46677rrzepIu5UJDZlIP0bRp087b19oeok2bNgkAYvny5VbHrV69+rztM2bMaNX/o4vlAl3IxXqIOvK+PHPmjAAgXnjhhfP2LV26VAAQhw8fbnN7yXl42TfcInKsiIgIAEBcXBz69evX6ucVFRVhwYIFWLlyJfR6PUJCQjBhwgRcd911GDx4MAwGA7766iusWbMGGzdulJ9XWlqK0NDQ885XV1cHnU533nYfHx95vy1JbSgpKWnzc9VqNW6//Xa8+eabqKqqQmBgIABg+fLluPzyy9G1a1f52FdeeQXl5eWXPGfzXpSQkBDU1NRgzZo1mDBhQpvaFhAQYNVDpdVqMXToUKtZPaWlpQBw3usQGRmJ3r17Y+PGjXj44Yfxxx9/QKPRYO7cufjyyy9x9OhR9OzZE5s2bcKIESOgUqnk544bNw4PPvggXnjhBfzvf/+Dj48P3nvvvRbb2Px3HxUV1ab7a0lqaioCAgKwceNGTJw4EZs2bUJ8fDymT5+OyZMno7a2Fr6+vvj999+tfje28NBDD7X7uV9++SWCg4Pxpz/9yer/YVpaGgICAvDbb7/htttuAwD85S9/aVXbW3pvdURH3pfSPke+r8mxGBCR28jNzcWzzz6Lfv364cCBA1i8eDGefvrpVj33qaeewvHjx/Haa68hMjISe/fuxXfffYfbb78dQggAQPfu3fHKK6+c91xpf3O+vr5oaGg4b7s0Nd7X17ctt3ZJUhuaf6i3xfTp0/Hyyy/j66+/xvTp05GVlYVdu3bh3XfftTouLS2tzed+5JFH8MUXX2DixIno1KkTxo0bh1tuuaVVwVF8fPx59xQaGop9+/add2xLr8PIkSPx448/ArAMMQ0ePBiDBw9GWFgYNm3ahOjoaOzdu1f+oG7un//8J7799ltkZGTgs88+u2Cw09Hf/bk0Gg3S09Pl4bFNmzZh5MiRGDFiBEwmE7Zu3Yro6GiUlZVh5MiRNrmmpHnw21ZHjx5FZWXlBX9PxcXF8r+Tk5ORnJzcpvOXlZXBYDDIP/v6+iI4OLhN5+jI+1La58j3NTkWAyJyG1IdoJ9++kmuzXPbbbehW7dul3zu3Llzrf5AX3vttXj66adRXFyMo0ePIjg4GH379j3vQy88PLzFHpPY2Fjk5eWdt72goAAAzstD6SipDVIPWVslJycjLS0Nn376KaZPn45PP/0UWq0Wt9xyi9Vx534oXUjzD6uoqChkZGTg559/xk8//YSffvoJH330EaZPn46PP/74oufRaDQtbm8e/ISHhwNAi6/DiBEj8O9//xsnTpyQAwuVSoURI0Zg06ZNiIuLg9lsbjGw2LNnj/whvn//fkybNq3FtnT0d9+SESNGyLWlNm3ahL/97W8ICQlBv3795EAOgM0DopY+0FUqVYvBppRfJTGbzYiKisLy5ctbPHdkZKT878rKylb1pmi1WoSFhQEAJk+ejA0bNsj7ZsyYgWXLll3yHM115H0ZFhYGnU4nH9vW55PzY0BEbuHrr7/Gd999h9deew3x8fF4/fXX8fPPP2PmzJlWCbgXcqFvq1FRURcdBunduzeys7PP2z5w4ED89ttv0Ov1Vgmc27Ztk/fbktSGPn36tPsc06dPx5w5c1BQUIDPPvsMkyZNOm/I4twPpQs598NKq9Xi2muvxbXXXguz2YxHHnkE7733Hp555hn06NGj3W0GLK8BgBZfBylgWLNmDXbs2IGnnnoKgCWB+p133kFcXBz8/f3P6/mqqanB3XffjeTkZFx++eVYvHgxbrzxRgwZMuS8a2RnZyMiIsLqA7+jRo4cCYPBgBUrViAvL0++j1GjRskBUa9eveTAqLXa04sVGhraYuHBcyuCd+/eHWvXrsXw4cMv2VPy5z//+ZLBMACMHj1aLkh67nBte4KPjrwv1Wo1+vfvj507d563b9u2bejWrZs83EwuSsH8JSKb0Ov1Ij4+XqSmplol7r7xxhsCgPjiiy/sdu1nnnlGeHt7i/r6eqvt0pTo5vVO6uvrRY8ePawSY1ujLdPuO1Icrri4WHh5eYkpU6YIAOL//u//zjtm586dYs2aNZd8ZGZmys9pqU1SEuqBAweEEBefdn+ulqZ7JyQkiDvvvLPF++rUqZNISkoSKpVKnv4t1aPp1auXuOqqq857zsyZM4W3t7fYtWuXqK6uFt27dxd9+vQ573UWwjLt/tprr7Xalp+fLw4dOtRiKYPWqKmpEd7e3iIpKckq4fvzzz8X/v7+olOnTuLee+9t83mjo6PF9ddff952Kan6zJkz5+178sknhU6nE8XFxfK2jIwMoVarrV6H9evXCwAt1uNpbGwU5eXl8s+ZmZmt+n+0c+fONt/jxZKq2/K+PHXqlDh06JDVtkWLFgkAYseOHfK2w4cPC41GI+bNm9fmtpJzYUBELm/27NlCrVZbFUsTQgij0SgGDRok4uLi7FZJeOfOnQKA+Pnnn8/bN2XKFOHl5SXmzp0r3nvvPXH55ZcLLy8vsWHDBqvjpA+jcwOeF198Ubz44oti6tSpAoC455575G3nuuaaa8SIESPO2442ztSR6t2EhIS0+OHfHjfccIMYNWqUeO6558R//vMf8cwzz8h1bqRieh0NiGbNmiU6derUYmFG6ffXv39/eVtjY6Pw9/cXAMRzzz1ndfy6deuESqWy2r5x40ahVqvF3LlzrY6VCjP+5z//Oa+NAER2dra8LTs7+7wZeBdz2WWXCQBWwVZBQYE8A6t5EdLWuvrqq4W/v7945ZVXxIoVK8TWrVuFEBcPiA4ePCjUarVITU0V//rXv8SCBQtEVFSU6N+//3mvw4MPPigAiIkTJ4rXXntN/Otf/xJ//vOfRVxcnPjyyy/b3N7W2rBhg/zeiIqKEl26dJF/Pvf91tr3pTTLsTm9Xi+6d+8uoqKixOLFi8Vrr70mEhISRFxcnFXASK6JARG5tJ07dwqNRiNmzZrV4v7t27cLtVotZs+ebbc2pKSktPhtva6uTjz55JMiJiZG6HQ6MWTIELF69erzjnviiSeESqU679uo9MHX0qO5iooKodVqz/tQrqqqEgDE1KlTW30vX3zxhQDQ4lIk7fW///1PjBs3TkRFRQmtVis6d+4sHnzwQVFQUCAf09GASKoY3tLyC1Jv1MMPP2y1fezYsQKAWLdunbxNr9eLxMREMWjQINHY2Gh1/OOPPy7UarXYsmWLvO2dd95pcemOlgKi/fv3CwDiqaeeOv+X1IK5c+e2WEqhR48eAoA4fvx4q87T3OHDh8WoUaOEr69vi4UZWwqIhBDi008/Fd26dRNarVYMHDhQ/Pzzzy2+DkJYqsanpaUJX19fERgYKPr37y/+8pe/iPz8/Da3t7Wk9rf0ePbZZ62Obe37sqWASAghcnNzxc033yyCgoJEQECAuOaaa8TRo0ftdWvkQAyIiDrok08+EYGBgVZDAm0xZMgQcfPNN7f7+q+99pqIjY0VtbW1Vtt/+OEHoVKpxL59+1p9rm+++UYAEBs3bmx3e5Ry5ZVXijvuuMOh1xw4cKB47LHHWnXs0qVLhb+/vygsLLRzq4ioPVRCtDB9gIhazWw2IyUlBdOmTcPf/va3Nj1Xr9cjMjISGRkZ7UqIbmxsRPfu3fHUU0/hkUcesdo3d+5c5OXl4bPPPmv1+a655hocOnQIx44ds9k0ckfZtm0bRo4ciaNHjzpkxfvVq1fj5ptvxokTJ1pVf2jKlCno2bMnXnrpJbu3jYjajgEREWHlypXYt28fFi5ciDfeeAOzZ89WuknUBoWFhRfd356aPUSehgEREUGlUiEgIAC33nor3n33XXh5sSKHK7lUb157avYQeRr+1SOiFgvvketYs2bNRfezYCDRpbGHiIiIiDyeWukGEBERESmNQ2atYDabkZ+fj8DAQJebeUNEROSphBCoqqpCXFwc1OqL9wExIGqF/Px8JCQkKN0MIiIiaofc3FzEx8df9BgGRK0gLdiXm5trtSAgEREROS+9Xo+EhIRWLbzLgKgVpGGyoKAgBkREREQupjXpLoomVS9cuBBDhgxBYGAgoqKicMMNNyArK8vqmPr6esycORPh4eEICAjATTfdhKKiIqtjcnJyMGnSJPj5+SEqKgpz586F0Wi0Omb9+vUYNGgQdDodevTowZocREREJFM0INqwYQNmzpyJrVu3Ys2aNWhsbMS4ceNQU1MjH/P444/j+++/x5dffokNGzYgPz8fkydPlvebTCZMmjQJBoMBmzdvxscff4xly5ZhwYIF8jHZ2dmYNGkSrrjiCmRkZOCxxx7Dfffdh59//tmh90tERETOyanqEJ05cwZRUVHYsGEDRo0ahcrKSkRGRuKzzz7DzTffDAA4fPgw+vTpgy1btuCyyy7DTz/9hGuuuQb5+fmIjo4GALz77ruYN28ezpw5A61Wi3nz5uGHH37AgQMH5GtNnToVFRUVWL169SXbpdfrERwcjMrKSg6ZERERuYi2fH47VR2iyspKAEBYWBgAYNeuXWhsbMTYsWPlY3r37o3OnTtjy5YtAIAtW7agf//+cjAEAOPHj4der0dmZqZ8TPNzSMdI5zhXQ0MD9Hq91YOIiIjcl9MERGazGY899hiGDx+Ofv36AbAsWKjVahESEmJ1bHR0tLyYYWFhoVUwJO2X9l3sGL1ej7q6uvPasnDhQgQHB8sPTrknIiJyb04TEM2cORMHDhzAypUrlW4K5s+fj8rKSvmRm5urdJOIiIjIjpxi2v2sWbOwatUqbNy40apwUkxMDAwGAyoqKqx6iYqKihATEyMfs337dqvzSbPQmh9z7sy0oqIiBAUFwdfX97z26HQ66HQ6m9wbEREROT9Fe4iEEJg1axa+/vpr/Prrr+jatavV/rS0NHh7e2PdunXytqysLOTk5CA9PR0AkJ6ejv3796O4uFg+Zs2aNQgKCkJycrJ8TPNzSMdI5yAiIiLPpugss0ceeQSfffYZvv32WyQlJcnbg4OD5Z6bhx9+GD/++COWLVuGoKAgPProowCAzZs3A7BMux84cCDi4uKwePFiFBYW4s4778R9992Hl156CYBl2n2/fv0wc+ZM3HPPPfj1118xe/Zs/PDDDxg/fvwl28lZZkRERK6nLZ/figZEF6oc+dFHH+Guu+4CYCnM+MQTT2DFihVoaGjA+PHj8fbbb8vDYQBw6tQpPPzww1i/fj38/f0xY8YMLFq0CF5eZ0cE169fj8cffxwHDx5EfHw8nnnmGfkal8KAiIiIyPW4TEDkKhgQERERuR6XrUNEREREpAQGRERE5DKqG4yXPoioHZxi2j0REdGl/OvXo/jnL0fQOyYQ1w6Iw7Upcegc7qd0s8hNMIeoFZhDRESkrOySGox/bSMMJrPV9gEJIXhwVDdc3T9WoZaRM2MOERERuZUXVx2EwWTGyJ4RePmm/hjeIxxqFbA3twKPLN+NQwVcc5I6hgERERE5tV8PF+HXw8Xw1qjw3HV9ceuQzlh+32XY+tercEVSJADg1TVHFG4luToGRERE5LQajCa88P1BAMA9w7uie2SAvC8q0Ad/m5QMtQpYc7AIGbkVCrWS3AEDIiIiclof/n4SJ0trERmow6wre5y3v0dUAG5MtayB+covWY5uHrkRBkREROSUCivr8davRwEA8yf2RqCPd4vHPTa2J7w1Kmw6WoKtJ0od2URyIwyIiIjIKS366RBqDSYM6hyCGwZ2uuBxCWF+uHVIAgDgnz9ngZOnqT0YEBERkdM5WVKDbzLyoVIBL1zfD2p1y2tfSh69sid0XmrsPFWO9UfOOKiV5E4YEBERkdP543gJAGBolzD06xR8yeOjg3wwPT0RgCWXiL1E1FYMiIiIyOlsPVEGAEjvHt7q5zw8pgf8tRocyNPj58xCezWN3BQDIiIicipCCDk5+rJurQ+Iwvy1uGt4FwDAZ9tz7dE0cmMMiIiIyKmcKKnBmaoGaL3UGJgQ0qbnTh5kmYK/+VgJKmsb7dA6clcMiIiIyKlIvUODOofAx1vTpud2jwxAr+gAGM0C6w4X2aN55KYYEBERkVOR8ofaMlzW3IS+MQCAnw4wj4hajwERERE5jfbmDzU3oV8sAGDjkTOoaTDarG3k3hgQERGR0+hI/pCkT2wgEsP90GA0Y30WaxJR6zAgIiIip9GR/CGJSqVqNmxWYLO2kXtjQERERE6jo/lDkgn9LAHRb4eLUd9o6nC7yP0xICIiIqdgi/whyYD4EMQG+6DGYMLvR0ts0TxycwyIiIjIKdgif0iiVqswvmnYbDWrVlMrMCAiIiKnYIv8oeakYbM1B4vQaDJ3+Hzk3hgQERGRU7BV/pBkSJcwhPtrUVnXKAdbRBfCgIiIiBRny/whiUatwri+0QCA1SzSSJfAgIiIiBRny/yh5qQijT9nFsFkFjY7L7kfBkRERKQ4W+cPSdK7hSNQ54WS6gYcyKu02XnJ/TAgIiIixW2zcf6QROulxrBuYZZrZDOPiC6MARERESnuUIEeADDAhsNlkmFdLUGWFHQRtYQBERERKarRZEZ2SQ0AoFd0oM3PL/UQbT9ZxjwiuiAGREREpKhTpTUwmgX8tRrEBfvY/PzJsUEI1Hmhqt4o90QRnUvRgGjjxo249tprERcXB5VKhW+++cZqv0qlavGxZMkS+ZguXbqct3/RokVW59m3bx9GjhwJHx8fJCQkYPHixY64PSIiaoWjRdUAgB5RAVCpVDY/v5dGjcFdQgGA9YjoghQNiGpqajBgwAAsXbq0xf0FBQVWjw8//BAqlQo33XST1XEvvPCC1XGPPvqovE+v12PcuHFITEzErl27sGTJEjz33HN4//337XpvRETUOkeLpYDI9sNlEilZeyvziOgCvJS8+MSJEzFx4sQL7o+JibH6+dtvv8UVV1yBbt26WW0PDAw871jJ8uXLYTAY8OGHH0Kr1aJv377IyMjAq6++igceeKDjN0FERB0iBUQ9owPsdo1hTQHRjpNlMJsF1Grb90SRa3OZHKKioiL88MMPuPfee8/bt2jRIoSHhyM1NRVLliyB0WiU923ZsgWjRo2CVquVt40fPx5ZWVkoLy93SNuJiOjCjhZVAQB6RtkvIOoXFwR/rQaVdY04XFhlt+uQ61K0h6gtPv74YwQGBmLy5MlW22fPno1BgwYhLCwMmzdvxvz581FQUIBXX30VAFBYWIiuXbtaPSc6OlreFxoaet61Ghoa0NDQIP+s1zMJj4jIHowmM040zTDracchM0seURg2HDmDrSdKkRwXZLdrkWtymYDoww8/xO233w4fH+sZCHPmzJH/nZKSAq1WiwcffBALFy6ETqdr17UWLlyI559/vkPtJSKiS8str4PBaIaPtxqdQn3teq1h3SwB0bbsUtwzouuln0AexSWGzDZt2oSsrCzcd999lzx22LBhMBqNOHnyJABLHlJRUZHVMdLPF8o7mj9/PiorK+VHbm5ux26AiIhaJA2XdY8MgMbOeT1SYvW2bEseEVFzLhEQffDBB0hLS8OAAQMueWxGRgbUajWioqIAAOnp6di4cSMaGxvlY9asWYOkpKQWh8sAQKfTISgoyOpBRES2JydU2zF/SNK/UzD8tBpU1DbiSDHziMiaogFRdXU1MjIykJGRAQDIzs5GRkYGcnJy5GP0ej2+/PLLFnuHtmzZgtdffx179+7FiRMnsHz5cjz++OO444475GDntttug1arxb333ovMzEx8/vnneOONN6yG2oiISBnH5Blm9ssfknhr1EhLtHw2cBkPOpeiAdHOnTuRmpqK1NRUAJZ8oNTUVCxYsEA+ZuXKlRBCYNq0aec9X6fTYeXKlRg9ejT69u2Lf/zjH3j88cetagwFBwfjl19+QXZ2NtLS0vDEE09gwYIFnHJPROQEjjb11PRwQA8R0LweEQs0kjWVEIIDqZeg1+sRHByMyspKDp8REdmI2SyQ/Oxq1Dea8esTo9Et0v5B0c6TZbj53S0I89di19Nj7VIZm5xHWz6/XSKHiIiI3E9eRR3qG83QatToHObnkGumxIfAx1uNshqDnL9EBDAgIiIihUjDZd0i/eGlcczHkdareR4Rh83oLAZERESkiOaLujrSZV2b8oiymVhNZzEgIiIiRZydcm//GWbNST1E+05XOPS65NwYEBERkSIcsahrS/p2CgYA5JbVobzG4NBrk/NiQERERA4nhMAxByzq2pJgX290jfAHAOzLq3Totcl5MSAiIiKHK6isR43BBC+1Conh/g6/fv+mXqL9HDajJgyIiIjI4aQK1V0i/KH1cvxHUUq8JSDad5o9RGTBgIiIiBzOkWuYtSQlPgQAsJ9DZtSEARERETncsWJl8ockfeOCoFZZhu6Kq+oVaQM5FwZERETkcHINIgcs6toSf52XXP9oP4fNCAyIiIjIwYQQig+ZAUD/TiEAmEdEFgyIiIjIoc5UN6CyrhFqFeTp70o4m1hdoVgbyHkwICIiIoc61jRc1jnMDz7eGsXa0b8pINqfVwkhhGLtIOfAgIiIiBzqVFktAMuUeyUlxwbBS61CSbUBBZVMrPZ0DIiIiMihcpoCos5hfoq2w8dbg15NSd0cNiMGRERE5FA5pc4REAEs0EhnMSAiIiKHcpYeIoAFGuksBkRERORQp0prAECRNczO1byHiInVno0BEREROUxlbSP09UYAQEKYr8KtAXpFB0KrUaOyrlHuuSLPxICIiIgcRgo6IgJ08NN6KdwaQOulRp9YKbGaw2aejAERERE5zKkyabhM+fwhCfOICGBAREREDuRMCdUSqUDj3twKZRtCimJAREREDuNMU+4lUmL1gbxKmM1MrPZUDIiIiMhhnLGHqEdkAHy9NagxmHCipFrp5pBCGBAREZHDyAGRE+UQeWnOJlYfLKhSuDWkFAZERETkEAajGfkVdQCARCfqIQKApJggAMDhAr3CLSGlMCAiIiKHyK+og1kAPt5qRAbqlG6OFamHKKuQPUSeigERERE5RPP8IZVKpXBrrCU1LfJ6mAGRx2JAREREDnHKCROqJb2bhszyKuqgr29UuDWkBAZERETkELlNAVGCEwZEwX7eiA32AQAcYS+RR2JAREREDiEv6uqEAREAJMVw2MyTKRoQbdy4Eddeey3i4uKgUqnwzTffWO2/6667oFKprB4TJkywOqasrAy33347goKCEBISgnvvvRfV1dZ1JPbt24eRI0fCx8cHCQkJWLx4sb1vjYiIzpFTZplh5kxT7ps7GxBxppknUjQgqqmpwYABA7B06dILHjNhwgQUFBTIjxUrVljtv/3225GZmYk1a9Zg1apV2LhxIx544AF5v16vx7hx45CYmIhdu3ZhyZIleO655/D+++/b7b6IiMiaEEIeMusc5q9wa1rWpymPiDPNPJOiSw1PnDgREydOvOgxOp0OMTExLe47dOgQVq9ejR07dmDw4MEAgLfeegtXX301/vnPfyIuLg7Lly+HwWDAhx9+CK1Wi759+yIjIwOvvvqqVeBERET2U1ZjQHWDEQAQH+qrcGta1nzITAjhdDPhyL6cPodo/fr1iIqKQlJSEh5++GGUlpbK+7Zs2YKQkBA5GAKAsWPHQq1WY9u2bfIxo0aNglarlY8ZP348srKyUF5e7rgbISLyYNKU+5ggH/h4axRuTcu6RwbAS61CVb0R+ZX1SjeHHMypA6IJEybgk08+wbp16/Dyyy9jw4YNmDhxIkwmEwCgsLAQUVFRVs/x8vJCWFgYCgsL5WOio6OtjpF+lo45V0NDA/R6vdWDiIjazxmX7DiX1kuN7pEBAIAs5hF5HEWHzC5l6tSp8r/79++PlJQUdO/eHevXr8dVV11lt+suXLgQzz//vN3OT0TkaZxxlfuWJMUEIquoCocKqnBl7+hLP4HchlP3EJ2rW7duiIiIwLFjxwAAMTExKC4utjrGaDSirKxMzjuKiYlBUVGR1THSzxfKTZo/fz4qKyvlR25urq1vhYjIo0g9RM465V4i5RExsdrzuFRAdPr0aZSWliI2NhYAkJ6ejoqKCuzatUs+5tdff4XZbMawYcPkYzZu3IjGxrOVR9esWYOkpCSEhoa2eB2dToegoCCrBxERtd8pFxgyA7immSdTNCCqrq5GRkYGMjIyAADZ2dnIyMhATk4OqqurMXfuXGzduhUnT57EunXrcP3116NHjx4YP348AKBPnz6YMGEC7r//fmzfvh1//PEHZs2ahalTpyIuLg4AcNttt0Gr1eLee+9FZmYmPv/8c7zxxhuYM2eOUrdNRORxnLlKdXPSqvfHz1TDYDQr3BpyJEUDop07dyI1NRWpqakAgDlz5iA1NRULFiyARqPBvn37cN1116FXr1649957kZaWhk2bNkGnO7tK8vLly9G7d29cddVVuPrqqzFixAirGkPBwcH45ZdfkJ2djbS0NDzxxBNYsGABp9wTETlIfaMJhXrLrC1nHzKLC/ZBoI8XjGaB42eqL/0EchuKJlWPGTMGQogL7v/5558veY6wsDB89tlnFz0mJSUFmzZtanP7iIio406X10EIwF+rQZi/9tJPUJBKpULvmEDsOFmOrMIq9IllyoSncKkcIiIicj1yhepwf5codiglVh/i1HuPwoCIiIjsSlrUtXOYc1aoPldvLuHhkRgQERGRXcmLujp5/pCkN6feeyQGREREZFc5ZU09ROHOuajruXo1BUQFlfWorG28xNHkLhgQERGRXUlFGROcdFHXcwX5eKNTiKWth5lH5DEYEBERkd0IIZDbNGSW6CI9RMDZYbPDHDbzGAyIiIjIbkqqDahrNEGlAuJCfJRuTqslMSDyOAyIiIjIbnLLLcNlMUE+0HlpFG5N6/WOlWaaccjMUzAgIiIiu3GVJTvO1Ss6AABwtLj6ogWEyX0wICIiIruRA6JQ1wqIukb4Q60CquqNKK5qULo55AAMiIiIyG6khOoEFynKKNF5adClKQn8aBHXNPMEDIiIiMhupBwiVynK2FyPKGnYjInVnoABERER2U2Oi+YQAUDPZnlE5P4YEBERkV0YTWYUVNYDcM0eop5Rlqn3xxgQeQQGREREZBcFlfUwmQW0XmpEBuiUbk6bSUNmDIg8AwMiIiKyC2m4LD7UF2q1SuHWtF33yACoVEBZjQGl1Zxp5u4YEBERkV1IU+5dcbgMAHy1GsQ3rb/GPCL3x4CIiIjsQpph5mo1iJqT8ogYELk/BkRERGQXOS5ag6i5nlIeURGn3rs7BkRERGQXrj5kBjSvRcQeInfHgIiIiOwiV06qdt2AqGc0p957CgZERERkczUNRpTWGAC4ZlFGidRDVFzVgMraRoVbQ/bEgIiIiGzudLklfyjY1xvBvt4Kt6b9AnReiAv2AQAcO8M8InfGgIiIZCazgBBC6WaQGzi7ZIfrJlRLukt5RFzk1a0xICIiAEBFrQGjl/yG9IW/4tOtp2AwmpVuErkwKX/IlafcSzj13jMwICIiAMCnW0/hdHkdCvX1ePqbA7jylfX4cmcujCYGRtR2rrzK/bm4yKtnYEBERGgwmrBs8ykAwA0D4xAZqMPp8jrM/d8+jHt9I06W1CjcQnI18gwzdwiIWIvIIzAgIiJ8uycfJdUNiAnywZIpA7Bx7hWYP7E3Qv28ceJMDV5be0TpJpKLyW0qyugOPUTSTLP8ynpUNxgVbg3ZCwMiIg9nNgu8v+kEAOCeEV3grVHDV6vBg6O746O7hwIAfs4s5AcBtZoQotmyHa6fVB3ip0VkoA4AcJzDZm6LARGRh9tw5AyOFVcjQOeFqUM7W+0bEB+MbhH+qG804+cDhQq1kFxNaY0BtQYTVCqgkxsERMDZYTPmEbkvBkREHu79jZbeoWlDExDkY10vRqVS4YbUTgCAbzLyHN42ck1S/lBMkA90XhqFW2MbZ5fwYB6Ru2JAROTBDuRVYsuJUmjUKtw1vGuLx9ww0BIQ/XGsBEX6ekc2z6140my93KaijO4w5V5yNrGaPUTuigERkQf7d1Pu0DUpsegU0vLQRudwPwxODIVZAN/vzXdk89yC0WTGs98eQPKCn/FLpmcMO56dYeYew2UA0IO1iNyeogHRxo0bce211yIuLg4qlQrffPONvK+xsRHz5s1D//794e/vj7i4OEyfPh35+dZ/kLt06QKVSmX1WLRokdUx+/btw8iRI+Hj44OEhAQsXrzYEbdH5NTyKuqwal8BAOD+kd0ueqw0bPb1Hg6btYW+vhF3L9uBj7ecgsFkxsdbTirdJIdwh1XuzyXVIsotr0V9o0nh1pA9KBoQ1dTUYMCAAVi6dOl5+2pra7F7924888wz2L17N7766itkZWXhuuuuO+/YF154AQUFBfLj0Ucflffp9XqMGzcOiYmJ2LVrF5YsWYLnnnsO77//vl3vjcjZfbL5JExmgcu7h6Nfp+CLHjupfyy8NSpk5utxhLVYWiW3rBY3v7MZm46WwMfb8qd2y/FSFFe5/7Dj2Rlm7hMQhftrEernDSGA42fYS+SOvJS8+MSJEzFx4sQW9wUHB2PNmjVW2/71r39h6NChyMnJQefOZ2fDBAYGIiYmpsXzLF++HAaDAR9++CG0Wi369u2LjIwMvPrqq3jggQdsdzNELmbj0RIAwLRzZpa1JNRfizFJUVhzsAhf78nDvAm97d08l7Ynpxz3f7ITJdUGRAXq8MGMIXjm2wPIyK3AT/sLMePyLko30a7OrmPmPgGRSqVCz6hAbD9ZhqNF1egbd/EvEeR6XCqHqLKyEiqVCiEhIVbbFy1ahPDwcKSmpmLJkiUwGs/WS9myZQtGjRoFrVYrbxs/fjyysrJQXl7e4nUaGhqg1+utHkTupKbBiKxCy//roV3DWvWcG5uGzb7dkwezmQvAXkhFrQHTP9iOkmoD+sQG4dtZw9E/PhjXDogD4P55WEaTGfkVll4wdxoyA4AeTcNmx5hH5JZcJiCqr6/HvHnzMG3aNAQFBcnbZ8+ejZUrV+K3337Dgw8+iJdeegl/+ctf5P2FhYWIjo62Opf0c2FhywmOCxcuRHBwsPxISEiwwx0RKWd/XiXMAogL9kF0kE+rnnNl7ygE6ryQX1mP7SfL7NxC17XrVDmqGoxICPPFlw+lIzbYklg8qX8sVCpg56ly5FXUKdxK+ymorIfJLKD1UiOqqZihu+jJqfduzSUCosbGRtxyyy0QQuCdd96x2jdnzhyMGTMGKSkpeOihh/DKK6/grbfeQkNDQ7uvN3/+fFRWVsqP3Nzcjt4CkVPZk1MBABjYOaTVz/Hx1uDq/rEAgG+YXH1Be09XAgCGdglHgO5sVkJMsA+GdrH0xv2wz317ieQZZiG+UKtVCrfGtrjqvXtz+oBICoZOnTqFNWvWWPUOtWTYsGEwGo04efIkACAmJgZFRUVWx0g/XyjvSKfTISgoyOpB5E725FiGi1MTQtv0vBsHWYbNfthfgAYjZ9q0ZP/pCgDAgITzc0ykYTNpdp87ynGjRV3PJc00O1Vay///bsipAyIpGDp69CjWrl2L8PDwSz4nIyMDarUaUVFRAID09HRs3LgRjY2N8jFr1qxBUlISQkPb9mFA5A6EENiTWwEASG1DDxEADO0ShogALarqjcho6mWis4QQ2NfUQ9S/hZl7E/vFQKNWYd/pSpwsqXF08xwiu9RyX90i/BVuie1FBeoQ6OMFk1ngZEmt0s0hG1M0IKqurkZGRgYyMjIAANnZ2cjIyEBOTg4aGxtx8803Y+fOnVi+fDlMJhMKCwtRWFgIg8EAwJIw/frrr2Pv3r04ceIEli9fjscffxx33HGHHOzcdttt0Gq1uPfee5GZmYnPP/8cb7zxBubMmaPUbRMpKq+iDmeqGuClVl1yuv251GoV0rtHAAA2Hy+1R/NcWl5FHUprDPBSq9An9vye5fAAHYb3sPz+VrnpsFn2GUtA1NUNAyLLTDPmEbkrRQOinTt3IjU1FampqQAs+UCpqalYsGAB8vLy8N133+H06dMYOHAgYmNj5cfmzZsBWIa2Vq5cidGjR6Nv3774xz/+gccff9yqxlBwcDB++eUXZGdnIy0tDU888QQWLFjAKffksaT8oeS4IPh4t32dqcu7W3pqtzAgOs/+pt6hpJjAC/5ur02x5GF9v9c9h82yS9w3IALO5hEd4RIebkfROkRjxoyBEBeevnuxfQAwaNAgbN269ZLXSUlJwaZNm9rcPiJ3JAVEqQkh7Xq+FBDtyS1HrcEIP62if0acipRQnRIfcsFjxvWNwd++PoCsoipkFVYhKSbQQa2zP5NZ4FSpZSjJbQMieeo9e4jcjVPnEJH7uVSQS/a3J7cpobpz+3LoOof5oVOILxpNAjtOtlzLy1Ptz6sAAKTEX3goMtjXG6OTIgG437BZfkUdDCYztBo14i6wNp6rk1e9Zw+R22FARA5x4kw1pry7GakvrsGCbw8gM79S6SZ5pAajCZl5loKMbU2olqhUKrmXaPPxEls1zeWZzWcTqi8WEAFnZ5t9tzffrb4kSMNlieF+0LjZlHtJz2hLj152SQ0aTWaFW0O2xICI7MpsFvh480lc/eYm7DhZjoraRnyy5RQmvfk7rnlrE/679RTqDJy+6igH8/UwmMwI89d2qIrw5T2YR3SuU2W1qKo3QuelRq/oiw+Dje0TBW+NCqdKa92qSKO75w8BlmKm/loNjGaBU6XuOVPQUzEgIrvJr6jDnR9uw7PfZaK+0YwRPSLw7h2DMCnFslDogTw9nvnmAGZ8tB0mLgXhEBnSdPuEEKhU7f8Gf3nTTLP9eZWorG28xNGeYV9T/aHkuCB4ay7+p9VP64UeTcm5mfnuszSQHBBFum9ApFKp0KMp4OWwmXthQER2kZFbgfGvbcQfx0rh463GC9f3xSf3DMWEfrFYetsgbPvrWDxzTTL8tRpszy7Dh79nK91kjyBXqG5nQrUkOsgH3SP9IQSwNZu9RADODpe1spRB3zjLtPyDbhQQnShx3xpEzZ2des+AyJ0wICKbMxjNmPvlXlQ1GDEgIQQ/zh6J6eldrMr4h/lrce+Irnj6mmQAwJJfsrhgogN0NKG6OamXiMNmFlIP0cVmmDUnBUTu1UNkeQ93jQhQuCX2xYDIPTEgIpt7f+NxHC2uRkSAFh/fPQTdIi/8x3HqkASM6hUJg9GMJ77cCyOTFO3mTFUDcsvqoFIBKS0sK9FWUmL1H8eYWG0yCxxoSla/VEK1pG+c5biDbjLBoMFoQl65JR+qS4T7LdvRnDT1/mgRp967EwZEZFPZJTV489djAIBnrklGiJ/2oserVCq8fFN/BPp4YW9uBd7fdMIRzfRIUv5Qz6gABPl4d/h8l3ULh0pl+ZZcXFXf4fO5smPF1ahrNMFfq7noF4Dm+sRa8lDyK+tRXmOwZ/McIresFmYBBOi8EBngXqvcn0sqzniipIZf4twIAyKyGSEE/vb1fhiMZozsGYHrmqYWX0pssC+evbYvAOD1NUeRVchvXfbQ3gVdLyTUX4vkpuUpPH3YTBou69spuNXTzQN9vNEl3NKT4g7DZieaLdnRkYR9V9ApxBc+3moYjGbklrvPLEFPx4CIbObrPXnYfLwUOi81/n5Dvzb9UbxpUCdc1TsKBpMZc77IYH0PO5ArVLez/lBL5HpExzw9ILIMew1o5XCZRBo2c4e6XJ4w5V6iVquaFWjkFzh3wYCIbKKsxoC//3AIAPDnsT2RGN62P4oqlQoLJ/dHsK83MvP1+DbDvSr4Ks1kFtjb1Ithi4RqyeVNC5VuPuHZeUT78i69ZEdLkt0osdqTAiLg7LAZE6vdBwMisomFPx5CWY0BSdGBuH9kt3adIyrIB/eP7AoAWLk9x5bN83hHi6tQazAhQOclf7O1hSFdwuClViG3rA65ZbU2O68rMRjNOJTftoRqydmAyPV7iE54WEAkvY84O9Z9MCCiDjtWXI0vd50GALw0uf8li9JdzJTBCdCoVdh5qpxd0TZ0uMDyu+wTG2jTJRUCdF4Y0FTTyFOX8ThSVAWDyYxgX+82V/+Wpt6fKKlBrcFoj+Y5zEkPC4jOTr3n3yl3wYCIOuw/TTPDxvaJRlpix4ZjooN8cGXvKADAyh25HW4bWUh/tHteYkmJ9hjelEf0u4fmEe2V6w8FtzmZOCrQB5GBOggBHCpw3Q/W6gYjiqsaAABdPCUganovHSuuhpmV9t0CAyLqkOKqeny1Ow8A8ODo9g2VnWva0AQAwP/tPo36Rq5zZgtHmpYY6GXD4TLJyF6Wlds3HT3jkUuw7G/lgq4XcrZitesOm0m9QxEBWgT7drykgytICPWF1kuN+kazW61H58kYEFGHfLz5JAwmM1I7h2BwB3uHJKN7RSE22AcVtY34ObPQJuf0dFKegz16iFITQhDo44WK2kZ5+rknkX63vWOC2vV8OSAqcN3Eak/LHwIAL41aXqKEw2bugQERtVtNgxGfbrUkPz84qpvNao9o1CpMGWzpJVq5ncNmHVXfaJJX5ZYq7NqSl0aNkT0ts802HDlj8/M7u9xySzJ5W/OHJGen3rtuQJR9xvMCIuDsFwwu8uoeGBBRu32+IxeVdY3oEu6HPyXH2PTctwyOh0oFbDlRKnfHU/ucOFMDswCCfb3tVkF4dNOwmacFRPWNJhTpLbkzCe0OiCw9RIcLq1y2/pa0hpmn5A9JpCHoIwyI3AIDImqXRpMZHzStUH//qG42nbkEAPGhfhjV0/Ihy+TqjpETqqMC7FZBeFRTQLQ3t8ItlqFordNNVYr9tRqE+rUvdyYh1A+BOi8YjGYcP+OaH6zZpZZeMndf5f5cUo/rMQ6ZuQUGRNQuP+4vQF5FHcL9tbhpULxdrjFtaGcAwP92nXbZb87OQOrOt0f+kCQ22BdJ0YEwC2CTBy32Kg2XJYT5tTvYVKtV6CPVI8pzvWEzIQSyz3jGKvfn6tGsOCNnmrk+BkTUZkIIvLfBMtV+xuVd4OOtsct1ruoThYgAHUqqG7DuUJFdruEJmvcQ2dOYpKZhsyzPGTY7XXY2IOoIaU04V8wjKqsxQF9vhEoFJIa79yr35+oS7gedlxq1BhNOeWhhUnfCgIja7I9jpThYoIevtwZ3XpZot+t4a9SYMtjS+7SCydXtJvUQ9bJjDxFgnUfkKd+WpYU9E0I7Fgj0deGK1dKSHXHBvnb7cuSsvDRq9I6xvK8OufAsQbLwUroB5Ho+/MOSO3TL4HiE+mvteq2b0+Lxzvrj2Hy8BNUNRgTo+F+2LRqMJpy04wyz5tK6hMJPq0FJdQMOFerl2VPuLFfuIfLt0Hmk39XBAj2EEC61WrwnTrlvrk9sEPaersTBfD2u7h+rdHNs5mC+Hv/5/QSK9PUor2lEea0B5bUGRAbq8MiYHpiSFg+vDqxK4Izc627I7k6V1uC3rGIAluEye+seGYDEcD80mgS2HPfMSsgdIc0wC/TxQlSgfWaYSXReGlzeVLV6vYcMm+VIAVEHe4h6RgdAq1Gjqt6I3DLXKvLnaYu6nqtP03Cnu/QQCSHwyZaTuOHtP/DV7jx5RKCgsh71jWbkltVh/lf78afXNuL7vflu1RvMgIja5NOtpyCEZVZRt0jHJFCeHYopdsj13Im0Enev6ECH9DqMTrIsu+Ip0+9zbZRD5K1Ro1eM5f3kasNmnraG2bncKSCqqDXgwf/uwoJvM2EwmnFl7yi8futALLt7CL6dORzrnxyDZ65JRpi/FtklNXh0xR5c89bvyMitULrpNsGAiFqtzmDCFzsti7jOSLdf7tC5pGTd9VlnIIT7fBtxhGNFjkmoloxuKpWw+1Q59PWNDrmmUirrGqGvtyzIGh/asSEzAOgb65oFGuUeokjPDIh6x1pyiPIr61FR67olJ3adKsOkN3/HLweL4K1RYcE1yfhgxmDckNoJY5KiMCAhBF0i/HHviK7Y+JcrMOdPvRCo88LBAj1u+/dWt1jcmQERtdp3e/NQWdeIhDBfjGnqCXCEy7qFQ6tR43R5nZyvQK1zxAFT7pvrHO6HbhH+MJoFNrv5Yq9S71C4vxb+NshtS5YLNLpOQNRoMsvvSU+rQSQJ8vGWc8hcdfmVzPxKTPv3NuRV1KFLuB++eng47hnR9YK9ygE6L8y+qic2/uUKjOwZgVqDCXd/tAPrs1y7F58BEbWKEAIfbz4FALjzskSbF2K8GD+tF4Z2DQPgWVO6bcFRU+6bG+UhQ5yny20zXCaRZisdLnSdIn/HiqthMJoRqPPqcB6VK+sTIw2buc5rJ6luMGLWZ3tgMJoxsmcEVs0eif6tXKg41F+Lf08fjLF9otBgNOP+T3biFxdef5IBEbXKrlPlOFigh4+3Grc0rTPmSJ66NERHWGaYWT607T3DrLnm9YjceYhTSn62XUBk+VA9XV6HKhcZbjyQZ8l3So4LgtqBX5KcjavmEQkh8Lev9yO7pAaxwT54c2pqm2fy+nhr8Pbtabi6fwwaTQKPLN+N7/fm26nF9sWASGEGoxmVdc7/x+/jLZbeoRsGdkKIn32n2rdE+pDdeqIU9Y0mh1/fFZ0sqYXJLBCo80JMkI/DrntZt3DovNTIr6zH/jzXShBui7MzzDqePwQAwX7e8ut0pMg1ehqkfKd+ndy/xMLFSMOdrhYQfbEzF99m5EOjVuGtaantLqOi9VLjzampuDG1E4xmgT+v3INfD7teMV0GRAoqrKzHtH9vxazPdsPkxFMXi/X1+Gl/AQDgTgcmUzfXIyoAccE+aDCasfWEe+em2Io0XNYj2n5rmLXEx1uDCf0si/1+2LTenTvKtfGQGXA2QddVhl6kHqJ+nYIUbomypErjR4uqXWaZoazCKjz7XSYA4IlxvTC4S1iHzuelUeOfUwbgpkHxMAvg0c/2uFyAyIBIQZV1jTiYr8emoyV4Y+0RpZtzQZ9tz4HRLDCkS6hixfZUKhVGJ3HYrC2khOpeUY5JqG7uvhHdAACr9hWgoNK16uq0Vq6NahA1l9SUR5TlAnlEJrOQk4j7eUARzouJD/W1LNBrco0FemsNRsz8bDfqG80Y1SsSD43qbpPzatQqLLqpPy7vHo4agwn3LtuBYn29Tc7tCIoGRBs3bsS1116LuLg4qFQqfPPNN1b7hRBYsGABYmNj4evri7Fjx+Lo0aNWx5SVleH2229HUFAQQkJCcO+996K62vo/5L59+zBy5Ej4+PggISEBixcvtvettUpSTCAWTu4PAHjz12NO2cVoMJrx2bYcAMD09C6KtoV5RG0jrcDtyPwhSf/4YAzrGgajWWDZHycdfn17E0LIK913tEp1c2cTq53/m3V2SQ1qDSb4eKsdVpPMWalUqma9e87/2i1enYVjxdWICtTh1VsG2DT/y1ujxju3p6FbpD/yK+tx/yc7UWdwjTQHRQOimpoaDBgwAEuXLm1x/+LFi/Hmm2/i3XffxbZt2+Dv74/x48ejvv5sxHn77bcjMzMTa9aswapVq7Bx40Y88MAD8n69Xo9x48YhMTERu3btwpIlS/Dcc8/h/ffft/v9tcYNqZ3k9cAe/3yv/K3TWXybkYfiqgZEBeowvm+Mom25vEcEvNQqnDhT43S/J2ck9RD1cOAMs+buH2npJfpsew6qG4yKtMFezlQ1oMFohloFxIXYMiCSpt5XOX1CulRAMjk2yKGzTp1VcqxrzDQ7caYan2615IS+cssARATYvoJ9sJ83PpwxBCF+3th7uhJzvshwiYrWigZEEydOxN///nfceOON5+0TQuD111/H008/jeuvvx4pKSn45JNPkJ+fL/ckHTp0CKtXr8Z//vMfDBs2DCNGjMBbb72FlStXIj/fkuW+fPlyGAwGfPjhh+jbty+mTp2K2bNn49VXX3XkrV7U09f0wYCEEFTWNeLh5bucJmnYbBZ4b6NlVft7R3SF1kvZEdYgH28MSgwFAKxnL9FFGYxmuYKwvRd1vZAre0ehW4Q/quqN+GKHey3OKyVUxwb7wtuG6zl1jwyAl1qFqnojCiqde6iBCdXWpJlmB528sOaSn7NgNAtc2TsKI5sKqdpDlwh/vHdHGrw1Kvx0oBD//CXLbteyFafNIcrOzkZhYSHGjh0rbwsODsawYcOwZcsWAMCWLVsQEhKCwYMHy8eMHTsWarUa27Ztk48ZNWoUtNqz2fPjx49HVlYWysvLW7x2Q0MD9Hq91cOedF4avH37IIT6eeNAnh7Pf59p1+u11rrDxThWXI1AnRemDeusdHMANBs2Yz2iizpZWgOjWSBA54XYYMfNMGtOrVbhnhFdAVgWBDa6SLJpa5xNqLZd7xBgma3TvWn4ydmHzeSEag/PH5I0n3rvrL17u06V4acDhVCrgKcm9rb79YZ1C8eiySkAgLfXH8eXO537i5HTBkSFhZbiTtHR0Vbbo6Oj5X2FhYWIirKumOzl5YWwsDCrY1o6R/NrnGvhwoUIDg6WHwkJ9q+70ynEF29MTYVKBazYnov/23Xa7te8lHc3HAcA3H5ZIoJ8vBVujYUUEG0+XgKD0X0+YG3taLPhMiVXTr9pUDxC/bxxurwOvxx0vhy59pJrENmhGGGSCxRoFELIAVFfD59hJkmKCYRaBZTWGHCmqkHp5pxHCIGXfjwMALhlcILDeo5vSovHrCt6AAD++vV+p54l7LQBkZLmz5+PyspK+ZGb65iodlSvSDx2VS8AwLPfZcqVcJWw42QZdp0qh1ajxj3DuyjWjnP1jQtCZKAOtQYTdp4sU7o5TkuJCtUt8dVq5By5f286oWhbbMlWi7q2RA6InDgX5XR5HfT1Rmg1avRUYBajM/Lx1sjJ5ZlOmFj9c2YRdp0qh4+3Go//qZdDrz3nT70wqX8sGk0CD326S17/ztk4bUAUE2NJ4C0qsv5WWVRUJO+LiYlBcbH18gBGoxFlZWVWx7R0jubXOJdOp0NQUJDVw1FmXdkDaYmhqG4w4skv9yqWiPZeU+/Q5EGdEOXAon6XolKpMLJHBABgazYDoguReoiUyh9q7o70RGg1auzJqcCuU+7xmtlryAwA+sQ6/9R7qXeoV0yA4rmFzsRZK1Y3msx4ebWld+j+kd0Q7eC/6Wq1Cq/cMgADE0JQUduIe5btcMqFcG36P/n06dNWM7w6omvXroiJicG6devkbXq9Htu2bUN6ejoAID09HRUVFdi1a5d8zK+//gqz2Yxhw4bJx2zcuBGNjWerQa9ZswZJSUkIDQ21SVttSaNW4dVbBsBPq8HWE2X48A/HF7Y7UlSFtYeKoVIBD4zq5vDrX4qUWL0np+UcMLKsMQUoN8OsuahAH9yQGgcA+NvXB5BX4fp1iew7ZGb5UD1+ptpph4UP5DN/qCV9nLSw5srtOcguqUG4vxYPjrZNzaG28vHW4N/TB6NTiC+yS2rw4H93Od3/b5sGRKWlpfjggw9afXx1dTUyMjKQkZEBwJJInZGRgZycHKhUKjz22GP4+9//ju+++w779+/H9OnTERcXhxtuuAEA0KdPH0yYMAH3338/tm/fjj/++AOzZs3C1KlTERdn+QN82223QavV4t5770VmZiY+//xzvPHGG5gzZ44tb92mEsP98fSkZADA4p+zHF7G/70NlqGN8ckxTllfJLVzCAAgI7fCJaZyOprJLOQuaWcIiABg5hU9EOavxeHCKlz/r9+x3YV79xpNZrnYZGc7DJnFBfsg0McLRrNw2iJ/B/IsPSB9OcPMijP2ENU0GPH6Wkv9vsfG9mzzWmW2FBmowwd3DUaAzgvbsssw67PdTlXZW9G+zp07dyI1NRWpqakAgDlz5iA1NRULFiwAAPzlL3/Bo48+igceeABDhgxBdXU1Vq9eDR+fs919y5cvR+/evXHVVVfh6quvxogRI6xqDAUHB+OXX35BdnY20tLS8MQTT2DBggU268myl2lDE3Bl7ygYjGY8tjLDYZF0fkUdvs3IAwA8ONr5eocAICk6EL7eGlTVG532A0NJp8trYTCZofVS27RGTkckhvvj25nD0Sc2CCXVBtz2761yLRRXk19RB7MAdF5qRAbavoaLSqWSCzQ647BZ84TqfnFMqG6ub1NAdOJMtdOUT1mxPQelNQZ0CffD1KHKzxbuHROEd+9Ig9ZLjV8OFuGxlRlOMwNV0YBozJgxEEKc91i2bBkAyx+GF154AYWFhaivr8fatWvRq5d1MlhYWBg+++wzVFVVobKyEh9++CECAqy/FaekpGDTpk2or6/H6dOnMW/ePEfdYrupVJYS6KF+3jhYoMcb6xyztMd7G47DaBYY1jUMqZ2db0gRsKyZkxJv+Wa6J6dC2cY4oRNnLL1D3SL8napgXkKYH/7v4XRMSomF0Szw9DcH8MQXe7HhyBmU1ThfPsGFSMNl8aG+dpvBJyVWH3LCqfdF+gaU1higUavkHhGyiAzUIdxfC7NwjmC2wWiSJzM8MqaHTWtmdcSInhF47440aDVq/LC/AHO+2OsU63k6x2+HWhQV6IN/3GhZ2uOd9cex+XiJXa+XmV+J/zZ9a599VU+7XqujpDyi3cwjOo/Ua9bdCYc7/bRe+Ne0VPxlQhJUKuD/dp/GjA+3Y9CLazB80a948L87sT6r+NInUpA9FnU9l5RH5AwfqueSeod6RAbAx1ujcGuci0p1Nkg86ATDZt/syUORvgExQT64vimPz1lc0TsKS28fBC+1Ct/tzcfcL5UPito0mDh58uSL7q+oqOhIW6gFV/ePxZS0eHy56zRmr8jAj38egahA288QMJsFnvnmAMwCmJQSi+FNM7mcVWpCCAD2ELVECoi6Rfor3JKWqVQqPDKmBwbEh2DljlwcyKtEdkkN8irqkFdhqVe04Jpk3D28q9JNbZE9FnU9Vx8nnnovJVSz/lDL+nUKxu/HSrAnpxzTFByiMpmFnA9638iu0Hk5X/D6p+RovDUtFbNW7MFXe/LgpVFh0eQUm66t1hZtCoiCgy+eQBccHIzp06d3qEF0vheu74d9pyuRVVSF2Sv2YPl9l9l8KOSLnbnYnVMBf60GzzQldDuzgU2J1UeKq1BV34hAJykc6QyONw2ZOWMPUXPDe0TIgbe+vhGZeXp8vec0vth5Gs9/fxCFlfWYN6G3Yn8cLyTXDou6nqtXU0BUqK9HRa0BIX7aSzzDceQlOzjDrEVDu4bi3Q3AjpPK9l7/klmIEyU1CPb1dorcoQuZ2D8Wr5sF/rxyD9ZnnUFxVQNiFKqu36aA6KOPPrJXO+gifLUavH3HIFz31u/YeqIMr605gifHJ9ns/GU1BixqqlHx+J96KfafsS2iAn0QH+qL0+V12He60ul7tBzphJP3ELUkyMcb6d3DcVm3MHSJ8Mfi1Vl4b+MJFFTWY8mUFKf6dpvjgB6iIB9vdArxRV5FHQ4XVuGybuF2u1ZbZUoJ1Zxh1qK0xDCoVEB2SQ2Kq+rt0qN/KUIIvNNUS25GeqKiM8ta49oBcVCrVEiOC1L084c5RC6ie2QAXppsySf612/H8JsN8ywW/XQIFbWN6B0TiLsu72Kz89rboKak792nmEckqaxtREm1JUHZGUsmXIo0nPbqLQPk3IK7PtyBWoNR6abJTtuxSnVzzjjTrLS6AflNi84mc4ZZi4J9vdG7KQdsR7Yyf5s2Hy/FvtOV8PFWY4aL/E2flBKLrhHKfoljQORCrh/YCXdcZun6nPN5BvJtUOBu58kyfLHTsm7aP27sBy8nmYXQGlI9oj25FYq2w5kcL7H0DsUE+Tj9t8KLmTwoHh/dPQT+Wg22nCjFx5udY4p+TYMRpU0z4uwdEDnjmmbScFm3CH+X/v9lb0O7WL6s7VBoeaG31x8DAEwd0hnhAbYvDeGuXOfTjwAAT09KRr9OQSivbcSdH2xDYdO3tfaobzTh6W8OAABuHZyAtMQwWzXTIaSyAHtyyp12dWlHO17sesNlFzKyZyQWXGvJZ/tyZ65TvManm/KHgny8EOxr37y13k2zlZxp1fu9TV8+2Dt0cUO6Wv6WblOgAOne3Ar8cawUGrUK9410zokJzooBkYvx8dbgndvTEBfsg+NnajDlvc3yrJe2qDUYcf8nO3G4sAqhft54amJvO7TWvpJjg6D1UqO8thEnS5VbCNeZnChxjYTq1pqUEgdfbw1OlNRglxMMjUoLLsfbMX9IIg2ZHSmscpqK7OuPnAEApHd3npwmZzS0iyUgOlyoR2Vd4yWOtq33Nlpyh64fEOeQ/6fuhAGRC0oI88MXD6UjMdwPuWV1mPLuFnntqtaoqm/EjA+3Y9PREvhpNXj79jSE+jvPLJbW0nqp0b+TVKBR+Q9LZ+BOPUQAEKDzwqSUWADAl01Du0qS1mHrFGr/CuBdI/yh1ahRYzDJidxKKqsxyHW/ruwdpXBrnFtUkA+6hPtBCMfmOJ4qrcHqA4UAoNiaZa6MAZGLig/1wxcPpqNnVAAK9fW49b0tOJh/6a71iloD7vjPNuw4WY5AHy/8995hLv1tj/WIrLlbDxEA3DI4AQCwal++4snVeU1DZp0csCSKt0aNPk1DUxlOkCe3PqsYQlh6ZmODnWNJGGc2pIvjh80+/D0bZgGM7hUp56BR6zEgcmHRQT74/MF09I0LQmmNAVPe3Yznvsu8YG9Rblktpr6/FXtPVyLUzxsr7r8MaYnOuTxHa8l5RLnsIWo0mXGqtCkgcpJFXW1hSJdQdAn3Q43BhB/3FyraltMVZ5ftcIRBTRMHnKEi+7rDlpmtV/Vh71BrDG3KI3JUYnVFrUGeIHP/SOdch9LZcZqAiwvz1+Kz+y/D/R/vxPaTZVi2+SSWbT6J9G7huG1YZ9Q3mrA9uwzbssvkbvfIQB0+vXeYW3yDGJQYAgA4VFCFWoMRflrP/S+dW1aLRpOAj7casUHOX0uqtVQqFaYMTsCSn7Pwxc5c3JwWr1hbHNlDBABpiaH46I+TiudPNZrM2JhlyR/icFnrSAHRvtMVqG802X2Zk+XbclDXaELvmEAM7+G6vf5KYg+RGwj29cbKBy7DJ/cMxZ+So6FWAVtOlOLRFXsw93/78OWu08gpq4VaBQxODMUXD6a7RTAEALHBvogJ8oHJLLD/dKXSzVHU2UVdA5yuunNHTR7UCWoVsD27DCebhgWVkO/AHCLgbK2tw4VVqGlQbrhwx8kyVDUYEe6vxYD4EMXa4Uo6h/khKlCHRpOw+5B+g9GEZZtPAgAeGNXNbosOuzsGRG5CrVZhVK9I/Hv6YGyadyUevbIHukb4I7VzCB4a3R0f3T0EGc+Ow/8evlzx4le2xnpEFvKirm40XCaJDfbFyJ6RAID/7VImubrBaEJxVQMAx/UQxYX4IjbYEvDvPV3hkGu25NdDluGyK3pHuV2wbS8qlcphw2bfZeTjTFUDooN0uCbFuRZxdSUMiNxQpxBfPDEuCb89OQZfPzIcT03sjSuSohDkput9SQGRp1eslhd1dbOAVyIlV/9v12lFVsUuqLDU/PLxViPMgbMyB8n1tiocds1z/SrlD3G4rE0cERAJIfCfTdkAgLuHd4XWix/r7cXfHLk8+QPDw3uIpCEzd+whAoCxyVEI8fNGob4evx8rcfj1pSn3cSG+Dh2SGNQ08UGpPKITZ6pxoqQG3hoVRvTkmoFtIc0023WqHEaT2S7X2Hi0BFlFVfDXajDNiRdxdQUMiMjl9Y0LhloFnKlqQHFV+yt3uzp37yHSeWlww8BOAIAvduY6/PpyDSIHDZdJpJmguxWqyC71Dg3rGo5AN+1ltpek6EAE+Xih1mCSlz2xtf9sOgEAuGVIgt2rp7s7BkTk8ny1GnRpCgIOFzjPuk+OVFZjQHmtpSKuuxRlbIk0w2ztwSIYjPb5xn0h0gwzR025lyTHBkHnpUZFbaNcZ8qRpICIs8vaTq1Wyb1E9hg2O5ivx6ajJVCrgHuGc5mOjmJARG6hT4zzrfvkSCeaeoc6hfi6demBvnFBCPHzRoPRjEMFjn2tleoh0nqpkRJvqcju6GEzfX0jtjcVFmT9ofax57pmb/16FIBliRt7LzbsCRgQkVuQ1n065KE9RPJwmRv3DgGWmTtSzpijgwOphyjOwQER0Dyx2rH3vOlICYxmge6R/kgMd+//W/Yi9RDtPFlm08kAWYVV+KlpmY5Hr+xhs/N6MgZE5Bb6NK0M7uheA2chJ1S70ZIdF6JU9WaleogA5RKr1x0uAgBc1Sfaodd1J/07BSPY1xvltY3YdPSMzc4r9Q5d3T8GvaLdo66c0hgQkVvoHWv5g3D8TLXDc0ucgaf0EAHKTEM3mwUKKh1blLE56Z6PFlc7bPV0g9GM9axO3WFaLzVuTLVMBvh8h20mAxwtqsIP+wsAAI9e2dMm5yQGROQmOoX4ItDHC40mIQcHnsSTeogGJIRArbL02BTpHTOr8Ex1AxpNAhq1CjEKLIsSGahD5zDL6umOWuj16z2nUVZjQGSgzuXXPFTarUMsNbTWHipCSXVDh8/3r9+OQQhgfN9ouXecOo4BEbkFlUrlsYnVBqMZp5rWqfOEgMhf54WkptfaUcU4TzflD8UE+cBLo8yfTXn6vQPu2WgyY+lvxwEAD47qBm+F7tld9IkNwoD4YDSaBL7endehcx0/U43v9+YDYO+QrfF/ObmNPrGemVidU1YDk1nAX6tBdJBO6eY4hKPziJTMH5I48p6/zchHTlktwv21uH1Yot2v5wluaeol+nxnbofqSS399RjMAhjbJwr9OgXbqnkEBkTkRnp7aGL18abhsq6R/h6zqOPZYoUVDrne2Rlmjh8uk0iJ1Rk5FXZdusRkFvjXb8cAAPeP6gZfrX1XafcU1w2Ig6+3BseKq9sd1J4sqcE3GZYeptlXsXfI1hgQkduQpt4fLvSsHqJjxZacqR4eMFwmkZKM95+uRIPRZPfrOXqV+5YkRQfCT6tBVYMRR4vt93981b58ZJfUINTPG3dext4hWwn08cbV/WMBtD+5+s1fj8IsgCuSIpESH2LD1hHAgIjcSFJMIFRNS3jYInHRVUgBUU8PmnqbGO6HMH8tDCaz3ZZEaO7skJlyxe+8NGoMTAgBAOw+VWGXa5jNAm/9aukdundEV/jr3LfIpxKmDrUMm63aV4DqBmObnrvhyBl81ZR/9OexvWzeNmJARG7ET+uFLuGet4SH1FvgCQnVEkuBxhAAjkkylobMlOwhAs4OFdpr9fSfDhTiWHE1gny8MP3yLna5hicbnBiKbpH+qDWYsKopMbo1Kmsb8Zf/7QUA3HV5FzkwJttiQERu5WxitWfkEZnNAseLLTlEPaM9JyACgFQH1SMSQjhFUjUAXN7dstr8moNFqGljD8OlWHqHLMX+7hnRFUFcyNXmVCoVbh1s6SVa2YZhs2e/O4AifQO6Rfhj3oTe9mqex2NARG6ld9N07EMeMvU+v7IOdY0meGtUSPSwtYykPCJ7z7rS1xvl4Q0lk6oBYFjXMHQJ90N1gxGr9rW+h6E1fjlYiMOFVQjQeeHuy7lQqL1MHhQPL7UKGbkVyGpFvuNP+wvwTUY+1Crgn7cMYJK7HTEgIrciJ1Z7yJDZ0ab8oa4R/orVx1HKgIRgaNQqFFTWy0nP9iANl4X5axVfOFetVmHq0M4AgM+226bqMQAU6+vx9DeZACxDMsF+7B2yl8hAnbxQ7jPfHEBV/YUrj5+pasBfv94PAHh4THf5SwDZh9P/Be3SpQtUKtV5j5kzZwIAxowZc96+hx56yOocOTk5mDRpEvz8/BAVFYW5c+fCaLRtdzM5B6lq67HiajSa3H8Jj+PSDLMozxouAyw5Y9IQqT17iZxluExyc1o8vDUq7M2tQGZ+ZYfPZzSZ8eiKPSipbkDvmEDMvIILhdrbo1f2RIDOC9tPluG2f29DaQuTQIQQmP/VfpTXNqJPbBD+fBUTqe3N6QOiHTt2oKCgQH6sWbMGADBlyhT5mPvvv9/qmMWLF8v7TCYTJk2aBIPBgM2bN+Pjjz/GsmXLsGDBAoffC9lffKgvAnVeMJjM8nIW7uxokRQQec4Ms+bkYTM7zboCgLxySxVwZwmIIgJ0GJccAwBYaYNeotfWHsG27DL4azVYevsgDsk4QL9OwVj5wGUI89dif14lbnlvi9zLKYTAr4eLcMt7W7D2UBG8NSq8essAaL2c/uPa5Tn9bzgyMhIxMTHyY9WqVejevTtGjx4tH+Pn52d1TFDQ2bVdfvnlFxw8eBCffvopBg4ciIkTJ+LFF1/E0qVLYTAYlLglsiOVSiUv9OoJS3gcO+O5PUSAY/KI8pygBtG5pjUNm32zJw+1hvb3dv+WVSwv0bHophSPmqmotH6dgvHFg+mIC/bB8TM1mPLuFvx3y0lMfGMT7lm2EztOlkOrUeO56/pyvTIHcfqAqDmDwYBPP/0U99xzj1VF3uXLlyMiIgL9+vXD/PnzUVtbK+/bsmUL+vfvj+joaHnb+PHjodfrkZmZ6dD2k2NIidUH3XymmRACR4ssuVI9PTwgysyvRH2jfQo05ldYFpCNc5IeIgC4vHs4Oof5oarBiFX7Ctp1jryKOjz+eQYAYHp6Iq4dEGfDFlJr9IgKwJcPX45uEf7Iq6jDM99m4nBhFfy1Gjw4qhs2zbuCS6c4kEtV3frmm29QUVGBu+66S9522223ITExEXFxcdi3bx/mzZuHrKwsfPXVVwCAwsJCq2AIgPxzYWFhi9dpaGhAQ8PZMV293r0/WN2N9G3K3ROrz1Q3QF9vhFplSar2RAlhvogI0KKk2oDM/EqkJYbZ/BqnnSyHCJCSqxOweHUWVmzPwS1NU7lbq85gwszlu1FR24iU+GD8bVIfO7WULqVTiC++eCgdD3yyE7nldbjr8i64Y1giE9sV4FIB0QcffICJEyciLu7sN5kHHnhA/nf//v0RGxuLq666CsePH0f37t3bdZ2FCxfi+eef73B7SRm9PaQW0bGm/KHOYX7w8fbMvA9LgcZQ/HKwCLtPVdglIJJmmcU70ZAZAExJS8CrvxzBnpwKHCrQt3pYpaq+Efcu24mM3AoE+Xhh6W2DoPPyzP8/ziIiQIf/e/hyAPCY9QidkcsMmZ06dQpr167Ffffdd9Hjhg0bBgA4dsxSfj4mJgZFRUVWx0g/x8TEtHiO+fPno7KyUn7k5tpueivZX1K0ZQmP4qqGFmdvuAtPzx+SDGiq2nvABjOuzlXfaJKXgXGmHiLAMn17XF9Lb/fK7Tmtek5FrQF3fLAd20+WIVDnhY/uHoIED6tf5aykWdKkHJcJiD766CNERUVh0qRJFz0uIyMDABAba1lELz09Hfv370dxcbF8zJo1axAUFITk5OQWz6HT6RAUFGT1INfhr/OSixS2pvCZq/L0GWaS5KaekYN2WNNMmvnjp9UgxAmHMKTk6q/25KGy9sL1bACgpLoB0/69DXtzKxDi543P7r/MLj1qRK7KJQIis9mMjz76CDNmzICX19lRvuPHj+PFF1/Erl27cPLkSXz33XeYPn06Ro0ahZSUFADAuHHjkJycjDvvvBN79+7Fzz//jKeffhozZ86ETqdT6pbIzjwhsfqYB9cgaq5vnOW1Pn6m2uaJ1VJCdacQX6f89j68ewQSw/1QVW/E1W9uwrYTpS0el1tWi6nvb8WhAj0iAnT4/IF09I8PdnBriZybSwREa9euRU5ODu655x6r7VqtFmvXrsW4cePQu3dvPPHEE7jpppvw/fffy8doNBqsWrUKGo0G6enpuOOOOzB9+nS88MILjr4NcqCkporVUi+KOzrKgAiAZegoIkALswAO27hHMK/CMmPVmWaYNadWq7D0tkFIDPdDXkUdpv57K15efRgGoxlCCOw6VY5Zn+3GmH+ux7HiasQG++CLBy+T3x9EdJZLJFWPGzcOQojztickJGDDhg2XfH5iYiJ+/PFHezSNnJQUJEh5Nu6motYg57Z4ekCkUqmQHBeMjUfO4GC+3qYrgTvLKvcX069TMH6YPRIvfJ+JL3aexjvrj2PT0TPQqFTYe/psXlV6t3AsvjmFOUNEF+ASARFRW0kF5o4VV0MI4ZTDHR0hDZfFBvsgQMe3cXJsEDYeOWOTpSyac8Yp9y0J0Hlh8c0DcEVSFJ76aj8O5FmGirVealw/IA53De+CvnEcIiO6GP4lJbfULdIfKhVQWdeI0hoDIgLcK1+M+UPWkuPskzMmJVU725T7C5nYPxapnUPxxroj6BTii2lDOyPczf7vE9kLAyJySz7eGiSE+iGnrBbHiqsZELk5KbH6cEEVTGYBjdo2PYKnSi05RPGhrjPMFBPsg4WTU5RuBpHLcYmkaqL2kPOIit0vj0hKqO7p4VPuJV3C/eHrrUFdownZJbZZ1Le6wYiCSssss+6RnlkJnMiTMCAityV9iB13w8Rq9hBZ06jPLuprq2Gz7DOWwCoiQIsQP61NzklEzosBEbktd+0hqmkwyiuwe+qiri2Rhs1slVgtBdLduAI8kUdgQERuSwqIjrtZQHSiqeci3F+LUH/2XEiSYy2zqGxVsVoKiLozICLyCAyIyG1JH2T5lfWoaTAq3BrbOVpsKT7I4TJr8kyzfH2LdcvaSgo8mT9E5BkYEJHbCvHTIiLA0oMifbi5A+YPtax3TCDUKqC0xoDiqo4v6iv3EPH3TOQRGBCRW5MLNJ5xn0Vez84w4wd1cz7eGvn17uiwmckscKJptloPDpkReQQGROTWurthYvXxYq5yfyG2SqzOK6+DwWiG1kvttOuYEZFtMSAityZ9uz9e7B5DZvWNJpwstdxLr2j2XJzLVhWr5RlmEf42K/JIRM6NARG5NXdb5PVoUTXMAgjz1yIy0L2qb9uCNNMss4NDZpxhRuR5GBCRW5MCopMlNWg0mRVuTccdKrR80CdFB7rdgrW2IPUQnSqtRVV9Y7vPczYg4gwzIk/BgIjcWmywD/y0GhjNQl6XypVlFVqSw5NimD/UkjB/LWKDfQAAhwvbn0gvDbFyhhmR52BARG5NpVLJwx7usITH4aYeoj6xDIguJDm2KbE6r/2J1RwyI/I8DIjI7bnTEh5ne4iCFG6J8+rbwcTqiloDSmsMAICuERwyI/IUDIjI7cmLvLp4QHSmqgEl1QaoVJxhdjHJ8tT79gVEx5uKeMYF+8Bf52WzdhGRc2NARG7PXWaaSb1DiWF+8NPyg/pC+sZZZpodKaqCwdj2RHou6krkmRgQkdtrvsirLda4UoqUP9Sbw2UXFR/qi1A/bzSaBA60o0AjZ5gReSYGROT2EsP94aVWocZgQqG+XunmtNthzjBrFZVKhcFdwgAAO0+Wtfn5nGFG5JkYEJHb89ao0TncD4BrJ1ZLQ2acYXZpQ7qEAgC2Z5e3+bknOMOMyCMxICKPIC3h4aoBkckscKSIM8xaa4jUQ3SqDGZz64dJDUYzTpVZ6lUxICLyLAyIyCPIeUQumlh9srQGDUYzfL016Bzmp3RznF7fuGD4eKtRUdvYptc8p6wGJrOAv1aD6CAujULkSRgQkUfo7uI9RIcLLL1DvaIDuNhoK2i91EhNaBo2a0Me0bFm+UNcGoXIszAgIo9wtjija656nyWtYcaE6lYb0lVKrG59HtGJEuYPEXkqBkTkEaQZQyXVDaisbf+in0o51JRQzSn3rXc2sbr1PUTSDLNurFBN5HEYEJFHCNB5yTkhx0tcb9gsSw6I2EPUWqmdQ6FRq5BXUYf8irpWPUeuQcQp90QehwEReYxuEZYPuewzrjVsVtNgRE7TzCcOmbVegM5LXuh1RyvyiIQQXNSVyIMxICKP0a2p8vAJF+shymqabh8ZqEN4AGc+tYU8/b4VeURnqhtQVW+EWgUkhnMmH5GnYUBEHkNam+qEi/UQcbis/YZ2teQRtaaHSMofSgjzg4+3xq7tIiLnw4CIPIbcQ+RiAdHhAmkNMwZEbZWWaOkhyiqqumQy/dYTpQCApGj+nok8kVMHRM899xxUKpXVo3fv3vL++vp6zJw5E+Hh4QgICMBNN92EoqIiq3Pk5ORg0qRJ8PPzQ1RUFObOnQuj0ejoWyEnIM0cyi61FN9zFYc5w6zdIgN16BbhDyGAXTkX7iUSQuCbjDwAwNX9Yx3VPCJyIk4dEAFA3759UVBQID9+//13ed/jjz+O77//Hl9++SU2bNiA/Px8TJ48Wd5vMpkwadIkGAwGbN68GR9//DGWLVuGBQsWKHErpLD4UD9oNWoYjOZWzzpSmhCCi7p20OBWrGu2O6cCp0pr4afVYFzfaEc1jYiciNMHRF5eXoiJiZEfERERAIDKykp88MEHePXVV3HllVciLS0NH330ETZv3oytW7cCAH755RccPHgQn376KQYOHIiJEyfixRdfxNKlS2EwGJS8LVKARq2Sk2VdZQmPIn0DKusaoVGr5OKS1DZnE6sv3EP09Z7TAIAJfWPgp/VySLuIyLk4fUB09OhRxMXFoVu3brj99tuRk5MDANi1axcaGxsxduxY+djevXujc+fO2LJlCwBgy5Yt6N+/P6Kjz37jGz9+PPR6PTIzMx17I+QUpDyi7BLXyCM61FShumuEPxN920kKiPadrkR9o+m8/QajGav2FQAAbhzUyaFtIyLn4dQB0bBhw7Bs2TKsXr0a77zzDrKzszFy5EhUVVWhsLAQWq0WISEhVs+Jjo5GYWEhAKCwsNAqGJL2S/supKGhAXq93upB7sHVZppJa5hxuKz9EsP9EBmog8Fkxr7TleftX59VjIraRkQF6nB59wgFWkhEzsCp+4YnTpwo/zslJQXDhg1DYmIivvjiC/j6+trtugsXLsTzzz9vt/OTcqTEalepRZSZb/kA7xvHhOr2UqlUGNolDD/sL8Dm4yUY2rTGmURKpr5+YBwXziXyYE7dQ3SukJAQ9OrVC8eOHUNMTAwMBgMqKiqsjikqKkJMTAwAICYm5rxZZ9LP0jEtmT9/PiorK+VHbm6ubW+EFONqPUSZ+ZbeyX5xwQq3xLWN6mXp+Xl3w3EczD/b41tZ14i1h4oBADekcriMyJO5VEBUXV2N48ePIzY2FmlpafD29sa6devk/VlZWcjJyUF6ejoAID09Hfv370dxcbF8zJo1axAUFITk5OQLXken0yEoKMjqQe5B6iEqqKxHrcG5yy9U1TfKuU7sIeqYm9MSMLpXJOobzXjw050or7FMqvhpfwEMRjOSogPlZT6IyDM5dUD05JNPYsOGDTh58iQ2b96MG2+8ERqNBtOmTUNwcDDuvfdezJkzB7/99ht27dqFu+++G+np6bjssssAAOPGjUNycjLuvPNO7N27Fz///DOefvppzJw5Ezodl0DwRKH+WoT6eQNw/l6iQ035Q7HBPlyyo4M0ahXenJqKzmF+yC2rw+yVe2AyC3y1xzJcdkNqJ6hUHC4j8mROHRCdPn0a06ZNQ1JSEm655RaEh4dj69atiIyMBAC89tpruOaaa3DTTTdh1KhRiImJwVdffSU/X6PRYNWqVdBoNEhPT8cdd9yB6dOn44UXXlDqlsgJSMNmzj7T7ECelD/E4TJbCPbzxvvT0+DrrcGmoyWY++VebM8ug0plyR8iIs/m1EnVK1euvOh+Hx8fLF26FEuXLr3gMYmJifjxxx9t3TRyYd0i/LHrVLnT9xAdaEqo7teJQzm20jsmCEumpGDWZ3vk3qHLuoYjLsR+kzSIyDU4dQ8RkT3IidVOPtNMSv5lD5FtXZMSh4dGd5d/vpHJ1EQEJ+8hIrIHV1jktb7RhKPFloCNPUS2N3d8Egor65BTVotJKVy7jIgYEJEH6i4HRNUQQjhlMu3hwiqYzALh/lrEBPko3Ry3o1Gr8PrUVKWbQUROhENm5HESwvygVgE1BhOKqxqUbk6LpIKMyXFBThmwERG5GwZE5HF0XhokhDn3Iq8H8poKMnZi/hARkSMwICKPJBVodNap91IPEStUExE5BgMi8kjOvIRHo8ksL+rKhGoiIsdgQEQeqVuzxGpnc6y4GgaTGYE6LySE+indHCIij8CAiDxStwipFpHz9RBJFaqT44Kg5urrREQOwYCIPJLUQ5RbVosGo0nh1liTV7hnQjURkcMwICKPFBWog79WA7MAckprlW6OFSmhmivcExE5DgMi8kgqlUpOrD7uRInVZrNgDxERkQIYEJHHkobNnGnqfXZpDWoNJvh4q+XSAEREZH8MiMhjSYnVzlScUeod6hMbBC8N355ERI7Cv7jksXpEOWFAlMf8ISIiJTAgIo8lBUTHiiyLvDqDA6xQTUSkCAZE5LG6RPhBo1ahqsGIIr3yi7wKwYRqIiKlMCAij6Xz0iCxaZHXY8XKD5vlltWhorYR3hoVekYHKN0cIiKPwoCIPJo0bHa0uErhlgAZpysAAMmxQdB5aZRtDBGRh2FARB5NziNygh6ifbkVAIABCSGKtoOIyBMxICKPJg1NHXWCgGhvUw/RgPgQRdtBROSJGBCRR+sRGQgAOK5wQGQ0mbG/acr9gAQmVBMRORoDIvJo3aMs1aBLawwoqzEo1o4jRdWobzQjQOclF4wkIiLHYUBEHs1P64VOIb4AlM0j2tc0XJYSHwy1WqVYO4iIPBUDIvJ4Z/OIlJtptlcOiEIUawMRkSdjQEQer0fTqvdHi5TrIcrIteQPDWT+EBGRIhgQkceTeoiUWtOszmDCkSJL7xSn3BMRKYMBEXm8HlGWmWZK9RBl5lfCZBaICtQhJshHkTYQEXk6BkTk8aTijIX6elTVNzr8+hlNBRlT4kOgUjGhmohICQyIyOMF+3ojKlAHQJmZZntPM3+IiEhpDIiIoOwSHtKUe+YPEREphwEREYCeCgVE5TUGnCqtBQCkdApx6LWJiOgsBkREUK6HSKo/1DXCH8F+3g69NhERneXUAdHChQsxZMgQBAYGIioqCjfccAOysrKsjhkzZgxUKpXV46GHHrI6JicnB5MmTYKfnx+ioqIwd+5cGI1GR94KOTl5ppmjA6Km+kMD4pk/RESkJC+lG3AxGzZswMyZMzFkyBAYjUb89a9/xbhx43Dw4EH4+/vLx91///144YUX5J/9/Pzkf5tMJkyaNAkxMTHYvHkzCgoKMH36dHh7e+Oll15y6P2Q85J6iHLLa1HfaIKPt8Yh12X+EBGRc3DqgGj16tVWPy9btgxRUVHYtWsXRo0aJW/38/NDTExMi+f45ZdfcPDgQaxduxbR0dEYOHAgXnzxRcybNw/PPfcctFqtXe+BXENEgBYhft6oqG3E8TPV6Btn/x4bIQSX7CAichJOPWR2rspKy/BCWFiY1fbly5cjIiIC/fr1w/z581FbWyvv27JlC/r374/o6Gh52/jx46HX65GZmdnidRoaGqDX660e5N5UKpW8hIej8ojyKupQUm2Al1qFvnFBDrkmERG1zKl7iJozm8147LHHMHz4cPTr10/efttttyExMRFxcXHYt28f5s2bh6ysLHz11VcAgMLCQqtgCID8c2FhYYvXWrhwIZ5//nk73Qk5q57RAdh5qtxhAdG+pvpDvWMDHTZER0RELXOZgGjmzJk4cOAAfv/9d6vtDzzwgPzv/v37IzY2FldddRWOHz+O7t27t+ta8+fPx5w5c+Sf9Xo9EhIS2tdwchmOXsKjeYVqIiJSlksMmc2aNQurVq3Cb7/9hvj4+IseO2zYMADAsWPHAAAxMTEoKiqyOkb6+UJ5RzqdDkFBQVYPcn/y1HsHLfK642QZACCtc6hDrkdERBfm1AGREAKzZs3C119/jV9//RVdu3a95HMyMjIAALGxsQCA9PR07N+/H8XFxfIxa9asQVBQEJKTk+3SbnJNUnHGkyU1aDSZ7XqtOoMJB/IsQ2ZDuoRd4mgiIrI3px4ymzlzJj777DN8++23CAwMlHN+goOD4evri+PHj+Ozzz7D1VdfjfDwcOzbtw+PP/44Ro0ahZSUFADAuHHjkJycjDvvvBOLFy9GYWEhnn76acycORM6nU7J2yMnExvsA3+tBjUGE7JLatArOtBu18rIrUCjSSA6SIeEMF+7XYeIiFrHqXuI3nnnHVRWVmLMmDGIjY2VH59//jkAQKvVYu3atRg3bhx69+6NJ554AjfddBO+//57+RwajQarVq2CRqNBeno67rjjDkyfPt2qbhERYJlp1ifWMjwq9d7Yy86m4bLBXcK4wj0RkRNw6h4iIcRF9yckJGDDhg2XPE9iYiJ+/PFHWzWL3Fj/+GDsPFWOfacrMXnQxfPVOmJ7U0A0lMNlREROwal7iIgcLaVpCY39duwhMprM2H2qHAAwuAsTqomInAEDIqJmpCnwmfmVMNopsfpwYRVqDCYE6rzQO4YzGImInAEDIqJmuob7I0DnhfpGs90WepWm2w9KDIVGzfwhIiJnwICIqBm1WoV+nSy9NtLCq7YmBURDuzJ/iIjIWTAgIjrHgKZhM2lpDVsSQmDHyab8oUTmDxEROQsGRETn6G/HxOpTpbU4U9UArUaNAQkhNj8/ERG1DwMionNIPUSHCvRoMJpsem5puKx/fDAXdCUiciIMiIjOER/qixA/bzSaBLIKq2x6bikg4nIdRETOhQER0TlUKhX6d7IMm9k6j2hnU/7QENYfIiJyKgyIiFpwNrG6wmbnPFPVgBMlNQCAwYnsISIiciYMiIhaICVW27KHaNcpy3BZUnQggv28bXZeIiLqOAZERC2QlvA4WlyNOoNtEqu3ZzcNl3XlcBkRkbNhQETUgpggH0QG6mAyCxwssE0v0c5TTKgmInJWDIiIWqBSqZBiw8Tq6gYjMvP1ABgQERE5IwZERBcgLfS63wYB0aYjZ2AyCySG+yEuxLfD5yMiIttiQER0AVIe0V4bzDRbc7AIAPCnPtEdPhcREdkeAyKiC5Bmmp0oqUFVfWO7z2M0mbHucDEA4E/JDIiIiJwRAyKiC4gI0KFTiC+EgJz/0x7bT5ahsq4RYf5apHFBVyIip8SAiOgizlasrmj3OaThsit7R8FLw7ccEZEz4l9noouQhs325rYvsVoIcTZ/iMNlREROiwER0UUM7WqZIr/xyJl2rXx/qKAKp8vr4OOtxqiekbZuHhER2QgDIqKLSOscipggH1Q1GLEh60ybny/1Do3oEQlfrcbWzSMiIhthQER0EWq1CtekxAIAvt9X0ObnrzlUCAAYx+EyIiKnxoCI6BKuHRAHAFh7sAi1BmOrn5dXUYcDeXqoVcBVfaLs1TwiIrIBBkREl5ASH4zEcD/UNZqw9lBxq5+3tmm4LC0xFOEBOns1j4iIbIABEdElqFQqXJti6SX6fm9+q5/H2WVERK6DARFRK0jDZhuyzqCy7tJVqyvrGrH1RCkA4E/JMXZtGxERdRwDIqJWSIoJRK/oABhMZvySWXjJ49dnFcNoFugZFYCuEf4OaCEREXUEAyKiVpKHzS4x20wIgRXbcwBwuIyIyFUwICJqJWnY7I9jJSitbrjgcV/tzsPWE2XQeakxdUhnRzWPiIg6gAERUSt1ifBHSnwwTGaBHw+0PGxWVmPA3384CAD489ie6Bzu58gmEhFROzEgImqDS802+/sPB1Fe24jeMYG4f2Q3RzaNiIg6wKMCoqVLl6JLly7w8fHBsGHDsH37dqWbRC5mUlPV6h0ny+RZZJI/jpXgq915UKmAhZP7w5sr2xMRuQyP+Yv9+eefY86cOXj22Wexe/duDBgwAOPHj0dxcesL7RHFhfhiZM8ICAFMfX8rHlu5B4WV9ahvNOGvX+8HAEy/LBGpnUMVbikREbWFSgghlG6EIwwbNgxDhgzBv/71LwCA2WxGQkICHn30UTz11FMXfa5er0dwcDAqKysRFBTkiOaSEyuvMWDxz4exckcuhAD8tBoMTAjB5uOliAnywZo5oxDo4610M4mIPF5bPr89oofIYDBg165dGDt2rLxNrVZj7Nix2LJly3nHNzQ0QK/XWz2IJKH+WiycnILvZo7AoM4hqDWYsPm4Zfjs+ev7MhgiInJBHhEQlZSUwGQyITrauiZMdHQ0CgvPny20cOFCBAcHy4+EhARHNZVcSP/4YPzfw5fj9VsHomdUAKanJ2J8X1alJiJyRV5KN8AZzZ8/H3PmzJF/1uv1DIqoRSqVCjekdsINqZ2UbgoREXWARwREERER0Gg0KCoqstpeVFSEmJjzv9HrdDrodFydnIiIyFN4xJCZVqtFWloa1q1bJ28zm81Yt24d0tPTFWwZEREROQOP6CECgDlz5mDGjBkYPHgwhg4ditdffx01NTW4++67lW4aERERKcxjAqJbb70VZ86cwYIFC1BYWIiBAwdi9erV5yVaExERkefxmDpEHcE6RERERK6HdYiIiIiI2oABEREREXk8BkRERETk8RgQERERkcdjQEREREQejwEREREReTwGREREROTxGBARERGRx2NARERERB7PY5bu6AipmLder1e4JURERNRa0ud2axblYEDUClVVVQCAhIQEhVtCREREbVVVVYXg4OCLHsO1zFrBbDYjPz8fgYGBUKlUNj23Xq9HQkICcnNz3XKdNHe/P8D975H35/rc/R55f67PXvcohEBVVRXi4uKgVl88S4g9RK2gVqsRHx9v12sEBQW57X90wP3vD3D/e+T9uT53v0fen+uzxz1eqmdIwqRqIiIi8ngMiIiIiMjjMSBSmE6nw7PPPgudTqd0U+zC3e8PcP975P25Pne/R96f63OGe2RSNREREXk89hARERGRx2NARERERB6PARERERF5PAZERERE5PEYECnk5MmTuPfee9G1a1f4+vqie/fuePbZZ2EwGKyO27dvH0aOHAkfHx8kJCRg8eLFCrW47f7xj3/g8ssvh5+fH0JCQlo8RqVSnfdYuXKlYxvaAa25x5ycHEyaNAl+fn6IiorC3LlzYTQaHdtQG+nSpct5r9eiRYuUblaHLF26FF26dIGPjw+GDRuG7du3K90km3juuefOe6169+6tdLM6ZOPGjbj22msRFxcHlUqFb775xmq/EAILFixAbGwsfH19MXbsWBw9elSZxrbDpe7vrrvuOu81nTBhgjKNbYeFCxdiyJAhCAwMRFRUFG644QZkZWVZHVNfX4+ZM2ciPDwcAQEBuOmmm1BUVOSQ9jEgUsjhw4dhNpvx3nvvITMzE6+99hreffdd/PWvf5WP0ev1GDduHBITE7Fr1y4sWbIEzz33HN5//30FW956BoMBU6ZMwcMPP3zR4z766CMUFBTIjxtuuMExDbSBS92jyWTCpEmTYDAYsHnzZnz88cdYtmwZFixY4OCW2s4LL7xg9Xo9+uijSjep3T7//HPMmTMHzz77LHbv3o0BAwZg/PjxKC4uVrppNtG3b1+r1+r3339XukkdUlNTgwEDBmDp0qUt7l+8eDHefPNNvPvuu9i2bRv8/f0xfvx41NfXO7il7XOp+wOACRMmWL2mK1ascGALO2bDhg2YOXMmtm7dijVr1qCxsRHjxo1DTU2NfMzjjz+O77//Hl9++SU2bNiA/Px8TJ482TENFOQ0Fi9eLLp27Sr//Pbbb4vQ0FDR0NAgb5s3b55ISkpSonnt9tFHH4ng4OAW9wEQX3/9tUPbYw8Xuscff/xRqNVqUVhYKG975513RFBQkNXr6ioSExPFa6+9pnQzbGbo0KFi5syZ8s8mk0nExcWJhQsXKtgq23j22WfFgAEDlG6G3Zz7t8NsNouYmBixZMkSeVtFRYXQ6XRixYoVCrSwY1r62zhjxgxx/fXXK9IeeyguLhYAxIYNG4QQltfL29tbfPnll/Ixhw4dEgDEli1b7N4e9hA5kcrKSoSFhck/b9myBaNGjYJWq5W3jR8/HllZWSgvL1eiiXYxc+ZMREREYOjQofjwww8h3Kg01pYtW9C/f39ER0fL28aPHw+9Xo/MzEwFW9Z+ixYtQnh4OFJTU7FkyRKXHf4zGAzYtWsXxo4dK29Tq9UYO3YstmzZomDLbOfo0aOIi4tDt27dcPvttyMnJ0fpJtlNdnY2CgsLrV7P4OBgDBs2zG1eTwBYv349oqKikJSUhIcffhilpaVKN6ndKisrAUD+3Nu1axcaGxutXsPevXujc+fODnkNubirkzh27Bjeeust/POf/5S3FRYWomvXrlbHSR+shYWFCA0NdWgb7eGFF17AlVdeCT8/P/zyyy945JFHUF1djdmzZyvdNJsoLCy0CoYA69fQ1cyePRuDBg1CWFgYNm/ejPnz56OgoACvvvqq0k1rs5KSEphMphZfn8OHDyvUKtsZNmwYli1bhqSkJBQUFOD555/HyJEjceDAAQQGBirdPJuT3k8tvZ6u+F5ryYQJEzB58mR07doVx48fx1//+ldMnDgRW7ZsgUajUbp5bWI2m/HYY49h+PDh6NevHwDLa6jVas/Lx3TUa8geIht76qmnWkwUbv44949tXl4eJkyYgClTpuD+++9XqOWt0577u5hnnnkGw4cPR2pqKubNm4e//OUvWLJkiR3v4NJsfY/Ori33O2fOHIwZMwYpKSl46KGH8Morr+Ctt95CQ0ODwndB55o4cSKmTJmClJQUjB8/Hj/++CMqKirwxRdfKN00aqepU6fiuuuuQ//+/XHDDTdg1apV2LFjB9avX69009ps5syZOHDggFNNomEPkY098cQTuOuuuy56TLdu3eR/5+fn44orrsDll19+XrJ0TEzMedn10s8xMTG2aXAbtfX+2mrYsGF48cUX0dDQoNiaNra8x5iYmPNmLSn9Gp6rI/c7bNgwGI1GnDx5EklJSXZonf1ERERAo9G0+B5zltfGlkJCQtCrVy8cO3ZM6abYhfSaFRUVITY2Vt5eVFSEgQMHKtQq++rWrRsiIiJw7NgxXHXVVUo3p9VmzZqFVatWYePGjYiPj5e3x8TEwGAwoKKiwqqXyFHvSQZENhYZGYnIyMhWHZuXl4crrrgCaWlp+Oijj6BWW3fYpaen429/+xsaGxvh7e0NAFizZg2SkpIUGy5ry/21R0ZGBkJDQxVd4M+W95ieno5//OMfKC4uRlRUFADLaxgUFITk5GSbXKOjOnK/GRkZUKvV8r25Eq1Wi7S0NKxbt06e2Wg2m7Fu3TrMmjVL2cbZQXV1NY4fP44777xT6abYRdeuXRETE4N169bJAZBer8e2bdsuOdPVVZ0+fRqlpaVWAaAzE0Lg0Ucfxddff43169eflxKSlpYGb29vrFu3DjfddBMAICsrCzk5OUhPT3dIA0kBp0+fFj169BBXXXWVOH36tCgoKJAfkoqKChEdHS3uvPNOceDAAbFy5Urh5+cn3nvvPQVb3nqnTp0Se/bsEc8//7wICAgQe/bsEXv27BFVVVVCCCG+++478e9//1vs379fHD16VLz99tvCz89PLFiwQOGWt96l7tFoNIp+/fqJcePGiYyMDLF69WoRGRkp5s+fr3DL227z5s3itddeExkZGeL48ePi008/FZGRkWL69OlKN63dVq5cKXQ6nVi2bJk4ePCgeOCBB0RISIjVrEBX9cQTT4j169eL7Oxs8ccff4ixY8eKiIgIUVxcrHTT2q2qqkp+jwEQr776qtizZ484deqUEEKIRYsWiZCQEPHtt9+Kffv2ieuvv1507dpV1NXVKdzy1rnY/VVVVYknn3xSbNmyRWRnZ4u1a9eKQYMGiZ49e4r6+nqlm94qDz/8sAgODhbr16+3+syrra2Vj3nooYdE586dxa+//ip27twp0tPTRXp6ukPax4BIIR999JEA0OKjub1794oRI0YInU4nOnXqJBYtWqRQi9tuxowZLd7fb7/9JoQQ4qeffhIDBw4UAQEBwt/fXwwYMEC8++67wmQyKdvwNrjUPQohxMmTJ8XEiROFr6+viIiIEE888YRobGxUrtHttGvXLjFs2DARHBwsfHx8RJ8+fcRLL73kMn+ML+Stt94SnTt3FlqtVgwdOlRs3bpV6SbZxK233ipiY2OFVqsVnTp1Erfeeqs4duyY0s3qkN9++63F99uMGTOEEJap988884yIjo4WOp1OXHXVVSIrK0vZRrfBxe6vtrZWjBs3TkRGRgpvb2+RmJgo7r//fpcK3i/0mffRRx/Jx9TV1YlHHnlEhIaGCj8/P3HjjTdadRTYk6qpkUREREQei7PMiIiIyOMxICIiIiKPx4CIiIiIPB4DIiIiIvJ4DIiIiIjI4zEgIiIiIo/HgIiIiIg8HgMiIiIi8ngMiIiIOmDMmDF47LHHlG4GEXUQAyIiIiLyeAyIiMglrVq1CiEhITCZTACAjIwMqFQqPPXUU/Ix9913H+64445Lnuv333/HyJEj4evri4SEBMyePRs1NTXy/rfffhs9e/aEj48PoqOjcfPNNwMA7rrrLmzYsAFvvPEGVCoVVCoVTp48adsbJSKHYEBERC5p5MiRqKqqwp49ewAAGzZsQEREBNavXy8fs2HDBowZM+ai5zl+/DgmTJiAm266Cfv27cPnn3+O33//HbNmzQIA7Ny5E7Nnz8YLL7yArKwsrF69GqNGjQIAvPHGG0hPT8f999+PgoICFBQUICEhwS73S0T2xcVdichlpaWlYdq0aXjyySdx4403YsiQIXj++edRWlqKyspKxMfH48iRI+jZs+cFz3HfffdBo9Hgvffek7f9/vvvGD16NGpqavDjjz/i7rvvxunTpxEYGHje88eMGYOBAwfi9ddft8ctEpGDsIeIiFzW6NGjsX79egghsGnTJkyePBl9+vTB77//jg0bNiAuLu6iwRAA7N27F8uWLUNAQID8GD9+PMxmM7Kzs/GnP/0JiYmJ6NatG+68804sX74ctbW1DrpDInIUBkRE5LLGjBmD33//HXv37oW3tzd69+6NMWPGYP369diwYQNGjx59yXNUV1fjwQcfREZGhvzYu3cvjh49iu7duyMwMBC7d+/GihUrEBsbiwULFmDAgAGoqKiw/w0SkcMwICIilyXlEb322mty8CMFROvXr79k/hAADBo0CAcPHkSPHj3Oe2i1WgCAl5cXxo4di8WLF2Pfvn04efIkfv31VwCAVquVE7uJyHUxICIilxUaGoqUlBQsX75cDn5GjRqF3bt348iRI63qIZo3bx42b96MWbNmISMjA0ePHsW3334rJ1WvWrUKb775JjIyMnDq1Cl88sknMJvNSEpKAgB06dIF27Ztw8mTJ1FSUgKz2Wy3+yUi+2FAREQubfTo0TCZTHJAFBYWhuTkZMTExMhBy8WkpKRgw4YNOHLkCEaOHInU1FQsWLAAcXFxAICQkBB89dVXuPLKK9GnTx+8++67WLFiBfr27QsAePLJJ6HRaJCcnIzIyEjk5OTY7V6JyH44y4yIiIg8HnuIiIiIyOMxICIitzZx4kSrKfXNHy+99JLSzSMiJ8EhMyJya3l5eairq2txX1hYGMLCwhzcIiJyRgyIiIiIyONxyIyIiIg8HgMiIiIi8ngMiIiIiMjjMSAiIiIij8eAiIiIiDweAyIiIiLyeAyIiIiIyOMxICIiIiKP9/9CG4W+K5hUjgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.random.random(1000)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "for w_true in np.linspace(-10,10):\n",
        "  #y = np.tanh(w_true*x)\n",
        "  w_ests  = np.linspace(-20,20, 100)\n",
        "  L  = [sum((np.sin(w_true*x) - np.sin(w_est*x))**2) for w_est in w_ests]\n",
        "  plt.plot(w_ests, L)\n",
        "  plt.title(f'X ~(0,1), y=sin(wx)., w_true={w_true}')\n",
        "  plt.xlabel('w_est')\n",
        "  plt.ylabel('L')\n",
        "  plt.show()\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEDtshO8yuQw"
      },
      "source": [
        "# Base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "Vt462ovTytte",
        "outputId": "98cfd710-5119-45d3-d677-df840f42b811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 0.1534],\n",
            "        [-0.3103]]), tensor([ 0.0768, -0.2162]), tensor([[0.1364, 0.4098],\n",
            "        [0.1585, 0.3280]]), tensor([-0.3000,  0.2731])]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize model\n",
        "\n",
        "# Simple linear model\n",
        "class LinearRegressionNN(nn.Module):\n",
        "    def __init__(self, hidden_nodes=(1,1)):\n",
        "        super(LinearRegressionNN, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(1, hidden_nodes[0], bias=True)  # One input, one output\n",
        "        self.linear2 = nn.Linear(hidden_nodes[0], hidden_nodes[1], bias=True)  # One input, one output\n",
        "        self.output = nn.Linear(hidden_nodes, 1, bias=False)  # One input, one output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        return x.sum(dim=1)#self.output(x)#(dim=1)\n",
        "\n",
        "\n",
        "base_model = LinearRegressionNN(hidden_nodes=(2,2)).requires_grad_(False)\n",
        "# Set parameters in model to random numbers\n",
        "for param in base_model.parameters():\n",
        "    param.data = -.5+torch.rand_like(param)\n",
        "\n",
        "\n",
        "base_model.eval()\n",
        "X_tensor = torch.tensor(-10+20*np.random.rand(1000, 1)).float()#.unsqueeze(dim=1)\n",
        "y_tensor = base_model(X_tensor)\n",
        "plt.plot(X_tensor.numpy().flatten(), y_tensor.detach().numpy().flatten(), '.')\n",
        "\n",
        "\n",
        "print([i.data for i in base_model.parameters()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "y09TFeeMH5Bh",
        "outputId": "40bb2751-6a97-4f4c-921d-952103c35055"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-380-e0d3a145a1ea>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegressionNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-379-e3d341801203>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_nodes)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearRegressionNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# One input, one output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# One input, one output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#self.output = nn.Linear(hidden_nodes, 1, bias=False)  # One input, one output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Loss landscape\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "n_iter = 10000\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_model = LinearRegressionNN(hidden_nodes=2).requires_grad_(False)\n",
        "params = torch.zeros(n_iter, torch.concat([i.data.flatten() for i in loss_model.parameters()]).shape[0])\n",
        "losses = torch.zeros(n_iter)\n",
        "# For a random set of parameters, compute the loss\n",
        "for i in range(n_iter):\n",
        "  for param_i, param in enumerate(loss_model.parameters()):\n",
        "      param.data = -.5+torch.rand_like(param)\n",
        "  params[i] = torch.concat([i.data.flatten() for i in loss_model.parameters()])\n",
        "  losses[i] = loss_fn(loss_model(X_tensor), y_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "E0WjMbLhOCaW",
        "outputId": "0908291b-35ba-4d82-c1ed-bc683954d33c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-366-6d6b88f02aed>:38: UserWarning: The following kwargs were not used by contour: 'ticker'\n",
            "  cp = plt.contourf(grid_x, grid_y, grid_z, cmap='Blues', ticker=ticker.LogLocator())\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "# Convert tensors to NumPy arrays\n",
        "X = params.detach().numpy()[:,0]\n",
        "Y = params.detach().numpy()[:,1]\n",
        "Z = losses\n",
        "\n",
        "# Create a grid to interpolate the scattered data\n",
        "grid_x, grid_y = np.mgrid[-.5:.5:1000j, -.5:.5:1000j]  # 100x100 grid\n",
        "\n",
        "# Interpolate the scattered data onto the grid\n",
        "grid_z = griddata((X, Y), Z, (grid_x, grid_y), method='cubic')\n",
        "\n",
        "# Create the surface plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot the surface\n",
        "surf = ax.plot_surface(grid_x, grid_y, grid_z, cmap='Blues')\n",
        "\n",
        "# Add labels\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_zlim(0,10)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "from matplotlib import cm, ticker\n",
        "\n",
        "\n",
        "# Create the contour plot\n",
        "plt.figure()\n",
        "cp = plt.contourf(grid_x, grid_y, grid_z, cmap='Blues', ticker=ticker.LogLocator())\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# Add colorbar\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZK2hSN2zqnU"
      },
      "source": [
        "## Perfectly parameterized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fOaf2nygzFmd",
        "outputId": "5ee47be0-d5de-4b04-c067-29ea723acb4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/1000], Loss: 0.0076\n",
            "[tensor([[-0.4169],\n",
            "        [-0.4541]]), tensor([ 0.0020, -0.0231]), tensor([[ 0.9613, -1.1522],\n",
            "        [-0.2805,  0.7247]]), tensor([ 0.3198, -0.0473])]\n",
            "Epoch [200/1000], Loss: 0.0026\n",
            "[tensor([[-0.4005],\n",
            "        [-0.4649]]), tensor([-0.0105, -0.0206]), tensor([[ 0.9514, -1.1628],\n",
            "        [-0.2902,  0.7144]]), tensor([ 0.3800, -0.0804])]\n",
            "Epoch [300/1000], Loss: 0.0036\n",
            "[tensor([[-0.3956],\n",
            "        [-0.4687]]), tensor([-0.0181, -0.0179]), tensor([[ 0.9488, -1.1659],\n",
            "        [-0.2924,  0.7118]]), tensor([ 0.3951, -0.0988])]\n",
            "Epoch [400/1000], Loss: 0.0032\n",
            "[tensor([[-0.3953],\n",
            "        [-0.4694]]), tensor([-0.0228, -0.0157]), tensor([[ 0.9490, -1.1657],\n",
            "        [-0.2918,  0.7125]]), tensor([ 0.4002, -0.1100])]\n",
            "Epoch [500/1000], Loss: 0.0035\n",
            "[tensor([[-0.3968],\n",
            "        [-0.4689]]), tensor([-0.0261, -0.0136]), tensor([[ 0.9502, -1.1643],\n",
            "        [-0.2901,  0.7144]]), tensor([ 0.4021, -0.1180])]\n",
            "Epoch [600/1000], Loss: 0.0026\n",
            "[tensor([[-0.3990],\n",
            "        [-0.4680]]), tensor([-0.0289, -0.0117]), tensor([[ 0.9518, -1.1626],\n",
            "        [-0.2881,  0.7166]]), tensor([ 0.4029, -0.1242])]\n",
            "Epoch [700/1000], Loss: 0.0031\n",
            "[tensor([[-0.4013],\n",
            "        [-0.4669]]), tensor([-0.0313, -0.0098]), tensor([[ 0.9535, -1.1607],\n",
            "        [-0.2860,  0.7189]]), tensor([ 0.4033, -0.1297])]\n",
            "Epoch [800/1000], Loss: 0.0021\n",
            "[tensor([[-0.4037],\n",
            "        [-0.4658]]), tensor([-0.0336, -0.0080]), tensor([[ 0.9552, -1.1589],\n",
            "        [-0.2839,  0.7212]]), tensor([ 0.4035, -0.1347])]\n",
            "Epoch [900/1000], Loss: 0.0030\n",
            "[tensor([[-0.4062],\n",
            "        [-0.4647]]), tensor([-0.0355, -0.0063]), tensor([[ 0.9569, -1.1570],\n",
            "        [-0.2818,  0.7235]]), tensor([ 0.4037, -0.1395])]\n",
            "Epoch [1000/1000], Loss: 0.0026\n",
            "[tensor([[-0.4083],\n",
            "        [-0.4639]]), tensor([-0.0374, -0.0046]), tensor([[ 0.9584, -1.1554],\n",
            "        [-0.2800,  0.7254]]), tensor([ 0.4037, -0.1441])]\n",
            "Prediction for input 0.5: 0.4037453830242157\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c6ea24fd2a0>]"
            ]
          },
          "execution_count": 382,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = LinearRegressionNN(hidden_nodes=(2,2))\n",
        "\n",
        "# Initialize weights using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        try:\n",
        "          torch.nn.init.xavier_uniform_(m.weight)\n",
        "          torch.nn.init.zeros_(m.bias)  # Initialize bias to zero\n",
        "        except:\n",
        "          pass\n",
        "model.apply(init_weights)\n",
        "\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=.1)  # Lower learning rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000  # Increase number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        print([i.data for i in model.parameters()])\n",
        "# Test model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor([[0.5]])\n",
        "    test_prediction = model(test_X)\n",
        "    print(f'Prediction for input 0.5: {test_prediction.item()}')\n",
        "\n",
        "\n",
        "\n",
        "pred_y = model(X_tensor).detach().numpy().flatten()\n",
        "true_y = base_model(X_tensor).detach().numpy().flatten()\n",
        "plt.plot(X_tensor, true_y, 'rX')\n",
        "plt.plot(X_tensor, pred_y, 'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C30hiNUE1OWk"
      },
      "source": [
        "# Overparameterized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jXlIbcegzkN-",
        "outputId": "d1eb687c-2efa-47d1-bfe6-8a9dfff8609f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/1000], Loss: 0.7206\n",
            "[tensor([[ 0.0410],\n",
            "        [ 0.0961],\n",
            "        [-0.7279]]), tensor([ 0.0161, -0.0284,  0.0000]), tensor([[-0.8384,  0.4151, -0.2631],\n",
            "        [-0.6359,  0.0119, -0.3619],\n",
            "        [ 0.4961,  0.5207, -0.4319]]), tensor([-0.0303,  0.0000, -0.0069])]\n",
            "Epoch [200/1000], Loss: 0.7847\n",
            "[tensor([[ 0.0252],\n",
            "        [ 0.0701],\n",
            "        [-0.7273]]), tensor([ 0.0635,  0.0154, -0.0031]), tensor([[-0.8387,  0.4145, -0.2631],\n",
            "        [-0.6359,  0.0119, -0.3619],\n",
            "        [ 0.4983,  0.5165, -0.4310]]), tensor([-0.0311,  0.0000,  0.0871])]\n",
            "Epoch [300/1000], Loss: 0.7814\n",
            "[tensor([[ 0.0148],\n",
            "        [ 0.0525],\n",
            "        [-0.7246]]), tensor([ 0.0946,  0.0457, -0.0115]), tensor([[-0.8389,  0.4141, -0.2631],\n",
            "        [-0.6359,  0.0119, -0.3619],\n",
            "        [ 0.5026,  0.5165, -0.4266]]), tensor([-0.0317,  0.0000,  0.1484])]\n",
            "Epoch [400/1000], Loss: 0.6608\n",
            "[tensor([[ 0.0084],\n",
            "        [ 0.0420],\n",
            "        [-0.7202]]), tensor([ 0.1128,  0.0640, -0.0206]), tensor([[-0.8389,  0.4140, -0.2631],\n",
            "        [-0.6359,  0.0119, -0.3619],\n",
            "        [ 0.5061,  0.5176, -0.4195]]), tensor([-0.0320,  0.0000,  0.1840])]\n",
            "Epoch [500/1000], Loss: 0.8559\n",
            "[tensor([[ 0.0017],\n",
            "        [ 0.0365],\n",
            "        [-0.7139]]), tensor([ 0.1238,  0.0754, -0.0304]), tensor([[-0.8389,  0.4140, -0.2631],\n",
            "        [-0.6359,  0.0119, -0.3619],\n",
            "        [ 0.5086,  0.5187, -0.4090]]), tensor([-0.0318,  0.0000,  0.2059])]\n",
            "Epoch [600/1000], Loss: 1.0179\n",
            "[tensor([[-0.0102],\n",
            "        [ 0.0304],\n",
            "        [-0.7063]]), tensor([ 0.1306,  0.0841, -0.0399]), tensor([[-0.8388,  0.4143, -0.2631],\n",
            "        [-0.6359,  0.0119, -0.3619],\n",
            "        [ 0.5106,  0.5194, -0.3965]]), tensor([-0.0304,  0.0000,  0.2215])]\n",
            "Epoch [700/1000], Loss: 0.9317\n",
            "[tensor([[-0.0327],\n",
            "        [ 0.0230],\n",
            "        [-0.6952]]), tensor([ 0.1372,  0.0946, -0.0505]), tensor([[-0.8386,  0.4149, -0.2631],\n",
            "        [-0.6359,  0.0118, -0.3619],\n",
            "        [ 0.5136,  0.5204, -0.3776]]), tensor([-0.0274, -0.0003,  0.2393])]\n",
            "Epoch [800/1000], Loss: 0.6067\n",
            "[tensor([[-0.0603],\n",
            "        [ 0.0190],\n",
            "        [-0.6795]]), tensor([ 0.1452,  0.1022, -0.0619]), tensor([[-0.8386,  0.4154, -0.2631],\n",
            "        [-0.6359,  0.0125, -0.3619],\n",
            "        [ 0.5182,  0.5212, -0.3497]]), tensor([-0.0273,  0.0007,  0.2539])]\n",
            "Epoch [900/1000], Loss: 0.0969\n",
            "[tensor([[-0.2338],\n",
            "        [-0.0751],\n",
            "        [-0.6005]]), tensor([ 0.1795,  0.1343, -0.0848]), tensor([[-0.8386,  0.4163, -0.2631],\n",
            "        [-0.6359,  0.0136, -0.3619],\n",
            "        [ 0.5748,  0.5325, -0.1577]]), tensor([-0.0283,  0.0042,  0.3183])]\n",
            "Epoch [1000/1000], Loss: 0.0204\n",
            "[tensor([[-0.2748],\n",
            "        [-0.1142],\n",
            "        [-0.5910]]), tensor([ 0.0803,  0.0399, -0.0653]), tensor([[-0.8386,  0.4163, -0.2631],\n",
            "        [-0.6361,  0.0134, -0.3619],\n",
            "        [ 0.5703,  0.5239, -0.1033]]), tensor([-0.0283,  0.1080,  0.2498])]\n",
            "Prediction for input 0.5: 0.3578054904937744\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c6ea37ae4d0>]"
            ]
          },
          "execution_count": 384,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMcElEQVR4nO3de1xUZf4H8M+gAloCKsolUEFXtLymSGBWKiuaULa/9bbtoqa266pleCksoESz1GzXy2qZgrtteWnTEg3XBi+rogjKekldkVFBAS/JIHhB4fz+ODEyMGeYM/cZPu/X67yU8zznnOdwwPn6nOf5PgpBEAQQERER2TEXWzeAiIiIqCEMWIiIiMjuMWAhIiIiu8eAhYiIiOweAxYiIiKyewxYiIiIyO4xYCEiIiK7x4CFiIiI7F5TWzfAHKqrq3H16lW0bNkSCoXC1s0hIiIiAwiCgNu3b8Pf3x8uLvr7UJwiYLl69SoCAwNt3QwiIiIyQkFBAQICAvTWcYqApWXLlgDEG/bw8LBxa4iIiMgQZWVlCAwM1HyO6+MUAUvNayAPDw8GLERERA7GkOEcHHRLREREdo8BCxEREdk9BixERERk9xiwEBERkd1jwEJERER2jwELERER2T0GLERERGT3GLAQERGR3WPAQkRERHaPAYuU5GTAxQVYsED76+Rk27aLiIioEXKK1Pxml5wMJCaKf09IAPbuBZRK8eva+4mIiMgqFIIgCLZuhKnKysrg6ekJtVptnrWEDFjTAI7/bSMiIrIpOZ/ffCVkrCFDbN0CIiKiRoMBiy7z5zdcJyODQQsREZGVMGAxRUaGrVtARETUKDBg0aVmYK0hOGuIiIjI4hiwmEpOcENERERGYcCiiyFjWGpjLwsREZFFcVqzFEOmNtfm+N9GIiIiq+K0ZnOQ28tCREREFsOARUpCAjB4sOH1g4Mt1xYiIqJGjgGLPjXp+A2hUjEvCxERkYUwYGmIu7vhdZmXhYiIyCIYsDTk7l159dnLQkREZHYMWAzh5WV4XfayEBERmR0DFkPcuiWvPvOyEBERmRUDFkPJmebM7LdERERmxYDFUAkJ8upzLAsREZHZyApYFi1ahNDQULRs2RLt2rXDyJEjce7cuQaP27JlC7p27Qp3d3f06NEDO3fu1CoXBAGJiYnw8/ND8+bNERkZifPnz8u7E2uQk5eFY1mIiIjMRlbAsm/fPkybNg2HDx/G7t278eDBAwwdOhQVFRWSxxw6dAjjxo3DpEmTcPz4cYwcORIjR47EqVOnNHUWL16M5cuXY82aNThy5Agee+wxREVF4d69e8bfmSUolfIG4BIREZFZmLSW0PXr19GuXTvs27cPzz33nM46Y8aMQUVFBdLS0jT7nnnmGfTu3Rtr1qyBIAjw9/fHrFmzMHv2bACAWq2Gj48PUlNTMXbs2AbbYZG1hPQxdJ2h+fPlv0oiIiJqJKy2lpBarQYAtG7dWrJOZmYmIiMjtfZFRUUhMzMTAKBSqVBcXKxVx9PTE2FhYZo6dd2/fx9lZWVam1UZOgA3MZEzhoiIiMzA6ICluroaM2fOxIABA9C9e3fJesXFxfDx8dHa5+Pjg+LiYk15zT6pOnUtWrQInp6emi0wMNDY2zCOnF4TzhgiIiIymdEBy7Rp03Dq1Cls3LjRnO0xSHx8PNRqtWYrKCiwehtk4cKIREREJjEqYJk+fTrS0tKwZ88eBAQE6K3r6+uLkpISrX0lJSXw9fXVlNfsk6pTl5ubGzw8PLQ2q5OTl0Wl4qshIiIiE8gKWARBwPTp07F161ZkZGQgKCiowWPCw8OhrLPq8e7duxEeHg4ACAoKgq+vr1adsrIyHDlyRFPHLiUkMJkcERGRlcgKWKZNm4Yvv/wSX331FVq2bIni4mIUFxfjbq0FAmNjYxEfH6/5+s0330R6ejo++eQTnD17Fu+//z6ys7Mxffp0AIBCocDMmTOxYMECfP/99zh58iRiY2Ph7++PkSNHmucuLYUzgIiIiKxCVsCyevVqqNVqvPDCC/Dz89NsmzZt0tS5fPkyioqKNF9HRETgq6++wueff45evXrhm2++wbZt27QG6s6dOxczZszA66+/jtDQUJSXlyM9PR3u7u5muEU7wuy3RERERjEpD4u9sHoeltqSk+W97mFuFiIiIgDyPr8ZsJiDoYnkajj+t5yIiMhkVkscR7+Qs8YQwGnOREREMjFgMQelEjBgxpQGpzkTERHJwoDFXPLz5dXnNGciIiKDMWAxJzl5WYiIiMhgDFjMSe7sH74WIiIiMggDFnOT08uSlGS5dhARETkRBizmJqeXRRCYTI6IiMgADFgsQU4vS0YGgxYiIqIGMHGcpTCZHBERkV5MHGcP5ORlYSI5IiIivRiwWEp+PuDlZXjdVq0s2hwiIiJHxoDFkm7dMjxtf2kpgxYiIiIJDFgacPQosGyZ+KdR9uwxvG5pKXOzEBER6cCARY8JE4D+/YFZs8Q/R40y4iQffCCvPlP2ExER1cOARcLRo8CGDdr7vvkGGDNG5okSEpiyn4iIyEQMWCT85z+692/eDHTrJvNkCQnypjkzLwsREZEWBiwSBg6ULjt7FhgwQOYJ5bwaysjgWBYiIqJaGLBICA0Ffvtb6fJDh4D33pNxQrkLI3IsCxERkQYDFj22bAFGj5YuX7gQKCyUcUK5Y1n4aoiIiAgAU/Mb5PHHgYoK3WVPPw3k5Mg4GVP2ExERAWBqfrNbuVK67NgxoGdPGSczNJFcDY5lISIiYg+Lodq1A65fly6fOBFYv97Ak7GXhYiIiD0slnDtGtC6tXR5SgqwdKmBJ+NYFiIiIlkYsMhw86Y4ZkXK3LkGDsKVm0wuI8PwukRERE6IAYtM330nXSYIwOTJBp5I7jRnjmUhIqJGjAGLTAEBwJIl0uW7dslYKFHOANzERAYtRETUaDFgMcLs2cDixdLln39u4ImUSnkXZjI5IiJqpBiwGGnOHDFxnC5ffAFERBh4IrnTnImIiBohBiwmiI2VLsvMBPz8DDiJ3F4WvhYiIqJGiAGLCQICxN4UKcXFwGuvGXAiOTOGOJaFiIgaISaOM4O0NCAmRro8K0tcTFEvJpMjIqJGhonjrCw6GggPly7v3x9Yt66BkwQFmbVNREREzoQBi5kcOgS0aiVdPnlyA0nl8vPlXTA4WF59IiIiB8aAxYxOnNBfHhnZwAnkjGVRqZiyn4iIGg3ZAcv+/fsRExMDf39/KBQKbNu2TW/9CRMmQKFQ1NueeuopTZ3333+/XnnXrl1l34ytNZRU7ty5BmYOyc1+y5T9RETUSMgOWCoqKtCrVy+sWrXKoPp//etfUVRUpNkKCgrQunVrjBo1SqveU089pVXvwIEDcptmF2bPBt59V7q8wZlDHMtCRERUT1O5BwwfPhzDhw83uL6npyc8PT01X2/btg23bt3CxIkTtRvStCl8fX3lNscuLVgAnD4NSHU+paQAU6dKzBzKz5c3Yyg4WP74FyIiIgdj9TEs69atQ2RkJDp06KC1//z58/D390dwcDBeffVVXL58WfIc9+/fR1lZmdZmb1as0F+ud+YQx7IQERFpsWrAcvXqVfzwww+YXGdJ47CwMKSmpiI9PR2rV6+GSqXCwIEDcfv2bZ3nWbRokabnxtPTE4GBgdZoviwBAcCMGfrrSM4cSkiQl7KfY1mIiMjJWTVg2bBhA7y8vDBy5Eit/cOHD8eoUaPQs2dPREVFYefOnSgtLcXmzZt1nic+Ph5qtVqzFRQUWKH18i1fDvTtq7/Om29KFDBlPxERkYbVAhZBELB+/Xr84Q9/gKurq966Xl5e6NKlC/Ly8nSWu7m5wcPDQ2uzV9nZ4pgVKd9+q6czRU4vC1dyJiIiJ2a1gGXfvn3Iy8vDpEmTGqxbXl6OCxcuwM+g1QPt34QJ+qc779kjZsutR24vCxERkZOSHbCUl5cjNzcXubm5AACVSoXc3FzNINn4+HjE6ljGeN26dQgLC0P37t3rlc2ePRv79u3DxYsXcejQIbzyyito0qQJxo0bJ7d5dmv2bGDePOnyHTuAo0d1FMgZgMvst0RE5KRkByzZ2dno06cP+vTpAwCIi4tDnz59kPjLK4mioqJ6M3zUajX+9a9/SfauFBYWYty4cQgJCcHo0aPRpk0bHD58GG3btpXbPLu2cCHw7LPS5TrjMznJ5FQqBi1EROSUuFqzlR09Kk5pljJxIrB+fZ2dQ4bImwk0f778rLlERERWJufzmwGLDURHi6+ApIwYAaSl1dkpJ5mcQgFUVxvVNiIiImuR8/nNxQ9tIC0NiIiQLt+xQ0d6fzljWWplFiYiInIG7GGxocGDxRlCUgoKxAR0GnJ6WVxcgKoqo9tGRERkaexhcRAZGRLrCf3i+efr7JDTy1JdDbRqZVS7iIiI7A0DFhvTt+h1fj6wbFmtHQkJ8lZzLi01tllERER2hQGLjYWGioNspcyaJSae08jPlxe0cGFEIiJyAgxY7EBaGvD009LlGzbUSSqXnw94eRl2ci6MSEREToABi53IyQFat5YuHzOmzg6+7iEiokaEAYsd2bBBukylArp1q7VDzmshZr8lIiIHx4DFjkRHA337SpefPVurpyU/3/BpzkzZT0REDo4Bi53JzgZeeEG6fPNmYNQoI06sUnEALhEROSwGLHboH//QX/7NN78Mwv3gA3kn5gBcIiJyUAxY7FBAALBkif4648dDfl4WAEhONrpdREREtsKAxU7Nng0sXixdfubML2n78/PlnTgx0aR2ERER2QIDFjs2Zw6wfbt0+ZUrwGuvQVyUSA6OZSEiIgfDgMXORUfrH2SbkgIUblDKezXEsSxERORgGLA4gM2b9afvf+MNyH81xF4WIiJyIAxYHERaGuDtrbts61axJ0YW9rIQEZEDYcDiQI4fly7bsQN4IyxT3gmbNzetQURERFbCgMWBBATonzm04sgzeK13juEnvHfP9EYRERFZAQMWBzNnDjBjhnR5Su7T6NnivOEnZMp+IiJyAAxYHNDy5foH4Z680xlv4C+GnUylMkubiIiILIkBi4NKSwMGDJAuX4E3UIgnDDsZZwwREZGdY8DiwD79VF+pAmnQ0w1TG2cMERGRnWPA4sBCQ4EXX5QufxuLkAY9FWrjGkNERGTHFIIgCLZuhKnKysrg6ekJtVoNDw8PWzfH6nr0AE6dkioVEIGDOIiB+k+iUADV1eZuGhERkSQ5n9/sYXECP/ygr1SBQxjQcE/L/PnmbBIREZFZMWBxAgEBwLvv6quhwBzoSeACAAkJHHxLRER2iwGLk1iwQP9U57N4EoOxS/9JMjIYtBARkV1iwOJE0tKAiROlShXYg18jGt/rPwlnDBERkR1iwOJk1q/XN3NIgR2IxlH0038S9rIQEZGdYcDihHbs0N/TMgD79Z8gI4PTnImIyK4wYHFS69cDffroLnsAd3jipv4TJCaav1FERERGYsDixL6XHK6iQBla4dmGelr4aoiIiOwEAxYnFhAAvPCCVKkCB/Gs/vEsHIBLRER2QnbAsn//fsTExMDf3x8KhQLbtm3TW3/v3r1QKBT1tuLiYq16q1atQseOHeHu7o6wsDBkZWXJbRrp8I9/6CtVIBnv6T8Bx7IQEZEdkB2wVFRUoFevXli1apWs486dO4eioiLN1q5dO03Zpk2bEBcXh6SkJBw7dgy9evVCVFQUrl27Jrd5VEdAALBkiXT5dsToz4KbmMighYiIbM6ktYQUCgW2bt2KkSNHStbZu3cvBg0ahFu3bsHLy0tnnbCwMISGhmLlypUAgOrqagQGBmLGjBl45513GmxHY19LyBDvvQcsXChVKmA8UpGK16RP4PhLThERkZ2xy7WEevfuDT8/P/z617/GwYMHNfsrKyuRk5ODyMjIR41ycUFkZCQyMzN1nuv+/fsoKyvT2ki/BQv0pe9XYAMm6O9pYS8LERHZkMUDFj8/P6xZswb/+te/8K9//QuBgYF44YUXcOzYMQDAjRs3UFVVBR8fH63jfHx86o1zqbFo0SJ4enpqtsDAQEvfhlNYsABYLLmkkAIxSMM6qV4WTnMmIiIbsnjAEhISgj/+8Y/o27cvIiIisH79ekRERODTTz81+pzx8fFQq9WaraCgwIwtdm5z5gDbt0uVKjAFn6MQT1izSURERA2yybTm/v37Iy8vDwDg7e2NJk2aoKSkRKtOSUkJfH19dR7v5uYGDw8PrY0MFx0NDB2qu0xAE0zGWt2FzMtCREQ2YpOAJTc3F35+fgAAV1dX9O3bF0qlUlNeXV0NpVKJ8PBwWzSvUVi3DlAodJftwjDd41mYsp+IiGxEdsBSXl6O3Nxc5ObmAgBUKhVyc3Nx+fJlAOLrmtjYWE39v/zlL/juu++Ql5eHU6dOYebMmcjIyMC0adM0deLi4rB27Vps2LABZ86cwdSpU1FRUYGJ0gvikIkCAoC1Eh0p4niW7ViCWfWLOJaFiIhsoKncA7KzszFo0CDN13FxcQCA8ePHIzU1FUVFRZrgBRBnAc2aNQtXrlxBixYt0LNnT/z4449a5xgzZgyuX7+OxMREFBcXo3fv3khPT683EJfMa9IkwMcHiInRVeqCuVgCBYDZ+MTKLSMiItJmUh4We8E8LKYJDQWys3WXKVCNy2iPAFx5tLNpU+DBA+s0joiInJZd5mEh+/W3v0mXCXDBG/iL9s6HD4HgYIu2iYiIqDYGLITQUGD8eADQ3dm2Ff+HUdiovVOlsni7iIiIajBgIQBAaiqwfbsCQLWOUgW+wWik4g/auznNmYiIrIQBC2lERwNLOn0O3T0tCkzEBkzA+ke7OM2ZiIishINuqZ7fKL7BVvxWolRAFvojFLVG6Tr+jxAREdkAB92SSZYHLoPuV0MAoMDouuNZ2MtCREQWxoCF6gm4fAgzsBxSg3AvIhhD8O9HO5hMjoiILIwBC+m0fP5tdMVPkBrPkoFIvIf51m4WERE1UgxYSLeEBJxBd/TFEUgFLQvx7qOVnflaiIiILIgBC0mbPx/ZCIcPrkpUcEEE/iP+la+FiIjIghiwkLSEBADAF3gdUuNZCtDx0crO7GUhIiILYcBC+g0ejGjs1DueJQlJ4l/Zy0JERBbCgIX0UyoBAGfQHQG4DF1ByzGEYgD2i19wjSEiIrIABizUsPnibKACdEQgLuqooMAhPIsOyBPXGOKrISIiMjNmuiXDKBQAgEI8gUBcAtBERyUBHZCPi+jM7LdERNQgZrol8xs8GAAQgCuYhw8hNZ7lUk1SOb4aIiIiM2LAQoZRKoGgIADAQiQiAgegL6lcmqobV3MmIiKzYcBChsvP1/z1IJ5De+RDKmiJwfdYl9HRWi0jIiInx4CF5Pnl1RAAXEJndJAMWppgCtaisNBqLSMiIifGgIXk+WWac42L6IzB+BG6ghYBLnhnxH+t1DAiInJmDFhIvlq9LACgxFAsRLzOqv880RPvPb/fGq0iIiInxoCF5KvTywIAsfgSQLWOygos3D8Qb7xh8VYREZETY8BCxqnTyxKAK/gjPpOorMCKFcCoUZZvFhEROScGLGQcpbJe0PIeFkKhs5dF9M03wNGjlm4YERE5IwYsZDylUsxo+0vq/gBcwVpM0Ru0/O531mocERE5EwYsZLpaqzRPwnpcRnt0R67Oqnl5wHvvWaldRETkNBiwkNkF4ArWYwp052cBFi4E87MQEZEsDFjIdL+8EqotFNkYgTRIBS0DBli4TURE5FQYsJDpEhJ07k7DS5JJ5S5fBrp1s3C7iIjIaTBgIYtSYij6IFtn2dmzwLJlVm4QERE5JAYsZB46XgtpivA+pF4NzZoFTJhgkRYREZETYcBC5pGQIBm0RGMnQvATpIKWDRuYn4WIiPRjwELmoydoOYvu6KonaPn8cwu2i4iIHB4DFjIviQG4AHAG3RGO/+gsy8gA0tIs1SgiInJ0DFjIqubhY+jqZcnPB2JigH79rN8mIiKyf7IDlv379yMmJgb+/v5QKBTYtm2b3vrffvstfv3rX6Nt27bw8PBAeHg4du3apVXn/fffh0Kh0Nq6du0qt2lkL/QMwI3GTkTgIB4FLdrBS04O8NprlmsaERE5JtkBS0VFBXr16oVVq1YZVH///v349a9/jZ07dyInJweDBg1CTEwMjh8/rlXvqaeeQlFRkWY7cOCA3KaRvdDzWggADmIgtiMaT+EEAEW98pQUYMkSC7WNiIgcUlO5BwwfPhzDhw83uP5f/vIXra8//PBDfPfdd9i+fTv69OnzqCFNm8LX11duc8hezZ+vtcZQXdHYCQCIQRp0BS1vvw2MGwcEBFiqgURE5EisPoaluroat2/fRuvWrbX2nz9/Hv7+/ggODsarr76Ky5cvS57j/v37KCsr09rIziQkAIMH660SjZ3oi6PQNaZFEIAFCyzUNiIicjhWD1iWLl2K8vJyjB49WrMvLCwMqampSE9Px+rVq6FSqTBw4EDcvn1b5zkWLVoET09PzRYYGGit5pMcSmWDVbIRholYB11By2efAUuXWqBdRETkcBSCIOhOjGHIwQoFtm7dipEjRxpU/6uvvsKUKVPw3XffITIyUrJeaWkpOnTogGXLlmHSpEn1yu/fv4/79+9rvi4rK0NgYCDUajU8PDxk3wdZkKL+6x5d/oRV+Ax/1llWUMBXQ0REzqisrAyenp4GfX5brYdl48aNmDx5MjZv3qw3WAEALy8vdOnSBXl5eTrL3dzc4OHhobWRnWrgtVCN9/AhgCqdZf/4hxnbQ0REDskqAcvXX3+NiRMn4uuvv8aIESMarF9eXo4LFy7Az8/PCq0jizLgtRAABOAKot0zdJbNmwesW2fORhERkaORHbCUl5cjNzcXubm5AACVSoXc3FzNINn4+HjExsZq6n/11VeIjY3FJ598grCwMBQXF6O4uBhqtVpTZ/bs2di3bx8uXryIQ4cO4ZVXXkGTJk0wbtw4E2+P7IKevCy1JbZaCanU/ZMnMxMuEVFjJjtgyc7ORp8+fTRTkuPi4tCnTx8k/jKFtaioSGuGz+eff46HDx9i2rRp8PPz02xvvvmmpk5hYSHGjRuHkJAQjB49Gm3atMHhw4fRtm1bU++P7IGeNYZqCy36HuORCqmgJSYGGDLEvE0jIiLHYNKgW3shZ9AO2ZCBA3DT8CJi8D2AJjrLu3YFzpwxY7uIiMgm7HLQLRGCggyqFo2dmI1PJMvPngVmzDBXo4iIyBEwYCHryc83OGh5E8uhQLVk+cqVQGGhuRpGRET2jgELWVd+vpjGtoHpzgG4gnlYCKnxLADw4YdmbhsREdktjmEh2zBwPMsIfI+diJEs79QJkEjXQ0REdo5jWMhp7MBLmBF2WLL8wgUgNdV67SEiIttgwEJ2b3lWBJYskS5/4w3rtYWIiGyDAQvZhoHJ5AAAnp6YPRv4RGLi0O3bQL9+5mkWERHZJwYsZBsGJpMDAJSWAsHBiIsDWrbUXSUnh0nliIicGQMWsh05QYtKBQD46ivpKhkZfD1EROSsGLCQbSUkGF43ORnR0UCPHtJVVqyA3vEuRETkmBiwkO0Z2suSmAgkJ+PECaBLF+lqc+cyqRwRkbNhHhayD61aiWNVGqJQANXVKCwEAgOlqz3zDJCZabbWERGRBTAPCzkeQ4IVABg0CAAQEAB88YV0/rnDh4HXXjNP04iIyPYYsJB9MHCNIWRkiL0xACZNAo4cka6akgKMGGGGthERkc0xYCH7IGNhRJSWaoKW0FBg3jzpqjt3Au+9Z3rziIjIthiwkP3Izzd4jSGUlgLJyQCAhQv196QsXMhBuEREjo4BC9mXDz4wvG5iouavaWlib4uU554zoU1ERGRzDFjIviQkAIMHG16/VnrbVaukq6lUTCpHROTIGLCQ/dmzx/C6GRmav4aGAr/9rXTVFSv4aoiIyFExYCH7I+e1UJ0FhLZsAUaPlq7ep4+RbSIiIpti4jiyT4YOvgUALy/g1i2tXT17AidP6q7erx9w9KjxTSMiIvNg4jhyfIam6we0pjnXOHEC6NhRd/XsbCaVIyJyNAxYyD4lJIg9J4bSkSk3KUm6ekoKEB0tu1VERGQjDFjIft26Bbi7G17/l7wsNSZMAJ54Qrr6jh2cOURE5CgYsJB9u3vX8GnOtfKy1CgsFMesSFmxAliyxMi2ERGR1TBgIfunVBr+eqjOrCFAHGA7caL0IXPncrozEZG9Y8BCjsHQ1ZwzMuq9GgKA9euBdu2kDxs1yrhmERGRdTBgIccgZwCujldDAPDxx9KHHD4sPauIiIhsjwELOYY6eVYapKOXpaFBuJcuAamp8i5DRETWwcRx5DjkJJMDAIkf7X79gJwc3YfoyEFHREQWwsRx5JzkLIoI6OxlAcTEcc88o/uQ0lKgWzd5lyEiIstjwEKOQ6mUV19iLAsgrjkk5exZYMYMeZciIiLLYsBCjkVOyn5AspclIACYN0/6sJUrgWXL5F2KiIgsh2NYyPGYaSwLAAwYABw6JH1op05AXp68yxERkWE4hoWcm5l6WQDg4EGgfXvpQy9c4MwhIiJ7IDtg2b9/P2JiYuDv7w+FQoFt27Y1eMzevXvx9NNPw83NDZ07d0aqjk+AVatWoWPHjnB3d0dYWBiysrLkNo0ai4QEeUGLnrEsgBi06PPmm4ZfioiILEN2wFJRUYFevXph1apVBtVXqVQYMWIEBg0ahNzcXMycOROTJ0/Grl27NHU2bdqEuLg4JCUl4dixY+jVqxeioqJw7do1uc2jxiIhQV794GDJooAA4IsvpA8tKwNee03e5YiIyLxMGsOiUCiwdetWjBw5UrLO22+/jR07duDUqVOafWPHjkVpaSnS09MBAGFhYQgNDcXKlSsBANXV1QgMDMSMGTPwzjvvNNgOjmFppJKTG+w90TJ/vt5Ap7AQCAkB7tzRXb5kCTB7tsw2EhGRJLsaw5KZmYnIyEitfVFRUcjMzAQAVFZWIicnR6uOi4sLIiMjNXXqun//PsrKyrQ2aoTk9rI0ENwEBACbNkmXz5nDRRKJiGzF4gFLcXExfHx8tPb5+PigrKwMd+/exY0bN1BVVaWzTnFxsc5zLlq0CJ6enpotMDDQYu0nO2fGAbgAEB0NdOkiXR4WJu9yRERkHg45Syg+Ph5qtVqzFRQU2LpJZCsJCfIy4BrwCklffrqrV5mfhYjIFiwesPj6+qKkpERrX0lJCTw8PNC8eXN4e3ujSZMmOuv4+vrqPKebmxs8PDy0NmrE5GbAbUBDg3BnzQLWrTPrJYmIqAEWD1jCw8OhrPOBsnv3boSHhwMAXF1d0bdvX6061dXVUCqVmjpEZtXAayEAmDQJ+OQT6fLJk4GjR83YJiIi0kt2wFJeXo7c3Fzk5uYCEKct5+bm4vLlywDE1zWxsbGa+n/605+Qn5+PuXPn4uzZs/jb3/6GzZs346233tLUiYuLw9q1a7FhwwacOXMGU6dORUVFBSZOnGji7VGjYebXQgAQFydmupXSvz97WoiIrEaQac+ePQKAetv48eMFQRCE8ePHC88//3y9Y3r37i24uroKwcHBQkpKSr3zrlixQmjfvr3g6uoq9O/fXzh8+LDBbVKr1QIAQa1Wy70dciZBQYIgJuJvePPyMvi0MTH6T1VQYMF7IiJyYnI+v7mWEDkXOesMNZCXpcbRo2JvipQnnwROnzb8skREJLKrPCxEVmWBV0OhocD48dLlP/0EpKUZflkiIpKPAQs5F7kzhgwYgAuICyDq64yJieF4FiIiS2LAQs7HjAsj1j2tt7d0+ZQpzIRLRGQpDFjI+chN2T9kiMFVjx+XLhMEvhoiIrIUBixEGRkGVw0IAN59V7p86lRxkUQiIjIvBizknOSuMRQcbHDVBQuAF1+ULp87FxgzRt7liYhIPwYs5JzkrjGkUhk8ABcAduzQ39OyeTPQsaPhlyciIv0YsJDzUiotMs25xoIFQFaWdOqXS5fE2UVERGQ6Bizk3Cw0zblGaCjw8cfS5TJjICIiksCAhZyfBXtZAGDOHKBXL91lBQXA6NGyT0lERHUwYCHnJ7eXpVUr2ZdYu1a6bMsWTncmIjIVAxaiukpLZR8SGgqMGiVdzky4RESmYcBCjUNQkMUvsXkzMGGCdDkz4RIRGY8BCzUO+fny6svIfltbSgrw/PO6ywQB+PBDo05LRNToMWChxkNOMjkZ2W/r+vJL6bLVq8VsuUREJA8DFmo8EhLkBS1GDL4FxIBk8WLp8itXgGefNerURESNFgMWalzkLIxYWio7L0uNOXP0ryl08CBw9KhRpyYiapQYsFDjI6eXxYTMb7Nni4shSlmwwOhTExE1OgxYqPGR08sCGD0AFwDmzZMu+/57eTntiIgaMwYs1DjJiRQyMowOWgICgOnTpcv37AE6dzbq1EREjQoDFmqc5Ga/NWHW0IoVQL9+0uUXLgADBhh9eiKiRoEBCzVeVkgmV+PoUeCTT6TLDx0C3nvPas0hInI4DFio8bJSMrkacXFA797S5QsXMhMuEZEUBizUuFkpmVyN7dv1l5sYExEROS0GLNS4yU0mZ2RelhoBAcAXX0iX/+9/QGqqSZcgInJKCkEQBFs3wlRlZWXw9PSEWq2Gh4eHrZtDjkihMLyuGX5lEhP1xz6jRomLKRIROTM5n9/sYSEC5E1zNrGXBRA7dby8pMu3bAHefdfkyxAROQ32sBDVsHIvCwCEhIivgaQUFHCxRCJyXuxhIbI0IxdGrKuhdDBhYWa5DBGRw2PAQlRDzuDb0lKzXDIgQP+rn6tXgYgIs1yKiMihMWAhqiFnjSEz9bAA4iKIgwZJl2dmAm+8YbbLERE5JAYsRLUZ2sty6xYQHGy2y2ZkiONZpKxYASxZYrbLERE5HAYsRLUlJBg+Y0ilMmvQ8uOP+svnzmUmXCJqvBiwENUlZ2FElcpslw0IAObN01+Hg3CJqLFiwEKki74kKXWZMZ/+woVi0jgpV68yEy4RNU5GBSyrVq1Cx44d4e7ujrCwMGRlZUnWfeGFF6BQKOptI0aM0NSZMGFCvfJhw4YZ0zQi87h1y/CgJSPDrK+GNm8GsrIAHx/d5SkpZrsUEZHDkB2wbNq0CXFxcUhKSsKxY8fQq1cvREVF4dq1azrrf/vttygqKtJsp06dQpMmTTCqzn8jhw0bplXv66+/Nu6OiMzl1i0gKMiwuiqVWTLg1ggNBT76SHfZ/v36e2GIiJyR7IBl2bJlmDJlCiZOnIgnn3wSa9asQYsWLbB+/Xqd9Vu3bg1fX1/Ntnv3brRo0aJewOLm5qZVr5UZp40SGU3OGJXERLNeesIEoFMn3WXffAO89ppZL0dEZNdkBSyVlZXIyclBZGTkoxO4uCAyMhKZmZkGnWPdunUYO3YsHnvsMa39e/fuRbt27RASEoKpU6fi5s2bkue4f/8+ysrKtDYiizC0h8VC8vKAsWN1l6WkANHR1m0PEZGtyApYbty4gaqqKvjUebnu4+OD4uLiBo/PysrCqVOnMHnyZK39w4YNw9///ncolUp8/PHH2LdvH4YPH46qqiqd51m0aBE8PT01W2BgoJzbIDJcfr68oMWMA3BrxMVJl+3YAYwZY/ZLEhHZHavOElq3bh169OiB/v37a+0fO3YsXnrpJfTo0QMjR45EWloajh49ir179+o8T3x8PNRqtWYrKCiwQuup0crPN7xuRoZZx7IA4niW556TLt+8GejXz6yXJCKyO7ICFm9vbzRp0gQlJSVa+0tKSuDr66v32IqKCmzcuBGTJk1q8DrBwcHw9vZGXl6eznI3Nzd4eHhobUR2w8xjWQDgn//Uv5h0Tg6Qlmb2yxIR2Q1ZAYurqyv69u0LZa3EWtXV1VAqlQgPD9d77JYtW3D//n38/ve/b/A6hYWFuHnzJvz8/OQ0j8hybDyWJSAAWLtWfx0LxElERHZD9iuhuLg4rF27Fhs2bMCZM2cwdepUVFRUYOLEiQCA2NhYxMfH1ztu3bp1GDlyJNq0aaO1v7y8HHPmzMHhw4dx8eJFKJVKvPzyy+jcuTOioqKMvC0iM5PzWggw+2shAJg0Cdi+Xbr8+HH2shCR85IdsIwZMwZLly5FYmIievfujdzcXKSnp2sG4l6+fBlFRUVax5w7dw4HDhzQ+TqoSZMmOHHiBF566SV06dIFkyZNQt++ffGf//wHbm5uRt4WkQUYujAiIHZ3WGAAbnQ08OKL0uUxMeJ0aCIiZ6MQBEGwdSNMVVZWBk9PT6jVao5nIcvSN5BEl/nzxQUVzWzIEHF8r5SsLHGwLhGRPZPz+c21hIjkkNPLAlhsYIlSCbz6qnT5tGkWuSwRkc0wYCGSIyEBGDxY3jEWeDUESKfuB4CjR4Fay3URETk8BixEcimV8npa9L27MUFAAPDuu9LlO3cCb7xhkUsTEVkdAxYiYyQkyB/PYgELFugfhLtiBbB0qfXaQ0RkKQxYiIz1wQeG1w0OtlgzduwAfskqoNPcuUBhocUuT0RkFQxYiIwlZzyLSmWxsSwAsH69dE+LIABTpljs0kREVsGAhcgUtbI+N8hCY1lq7NgBvPKK7rL0dKBbN4tenojIohiwEFmLBV8L1Vi+XHpozdmzFu3kISKyKAYsRNaSn2/xiCEgAFi8WLo8I0Oc8kxE5GgYsBCZSu4UZwusM1Tb7Nn6B+HGxVn08kREFsHU/ETm0FCu/Lqs8GvXrZv4GkiXrl2BM2cs3gQiIr2Ymp/I2pRKICjI1q3QcuYM0Lu37rKzZ4GICKs2h4jIJAxYiMwlP9/woMUKA3ABYPt26bLMTGbCJSLHwYCFyJwuXjSsnkpllaAlIACYPl26fMUKYMkSizeDiMhkDFiIzElO9luVynLtqGXFCqB9e+lyZsIlIkfAgIXInBIS5NW30quhgwf1l0dGWqUZRERGY8BCZG5ypjlbqZelofws584B771nlaYQERmFAQuRuSUkyAtaLJyXpcacOcDo0dLlCxcCaWlWaQoRkWwMWIgsQc6rocREqwUtmzYBvr7S5TExwIQJVmkKEZEsDFiI7EFiotUuVVQEhIRIl2/YwPT9RGR/GLAQWcrgwfLqW6mXBQB+/FF/+W9+Y512EBEZigELkaUolfLqW7GXJSAA+OIL6fLCQv3rERERWRsDFiJLktvLYuHVnGubNAkoKBCDF11SU4GlS63WHCIivRiwEFmSUikvaJGzgKIZBAQAq1dLl8+Zw6RyRGQfGLAQWZrcV0NW7GUBgOhooF8/6fK+fa3XFiIiKQxYiKxBTl4WK/eyAOKsoGHDdJdduya+HiIisiUGLETWIDdlv5V7WQBg7VrpslWr+GqIiGyLAQuRtQQFGV7XBr0sAQHSM4Oys4HAQK7sTES2w4CFyFry8wF3d8Pr26CXZf16oEMH6fK5czlziIhsgwELkTXdvWt43YwMmwQtFy+KgYkUzhwiIltgwEJkz2zwaggAZszQXy43vQwROZjgYEChAJo31/2nDf4zxYCFyNq8vOTVt2LK/hoBAcC8edLl588Db7xhvfYQkYXUBCZ1N5VKLL93T/efNugBVgiCIFj1ihZQVlYGT09PqNVqeHh42Lo5RA1r1gx4+NDw+jb6NR09GtiyRbpcX6ZcIrIjycmWWf7DxH+b5Hx+s4eFyBYePJA3ayg42HJt0WPzZmDqVOlyqdwtRGRDQ4Y86ilp1kz80xLBipV7WBiwENnKxYuG11WpbBa06Hs1dPq0VddsJKK6kpPrv86pPfZNTk+uHK1bN7zsu5kZFbCsWrUKHTt2hLu7O8LCwpCVlSVZNzU1FQqFQmtzrzO1UxAEJCYmws/PD82bN0dkZCTOnz9vTNOIHMcHH8irr1KJ/xhZeUxLQys7JycDPXtarz1EjVrt3hNL9ZwY4uefgchIq15SdsCyadMmxMXFISkpCceOHUOvXr0QFRWFa9euSR7j4eGBoqIizXbp0iWt8sWLF2P58uVYs2YNjhw5gsceewxRUVG4VzO4h8gZJSTIey1UIzHR6kHLpElASop0+cmTNpk0QOT8ambl6Oo9sTW566SZSpCpf//+wrRp0zRfV1VVCf7+/sKiRYt01k9JSRE8PT0lz1ddXS34+voKS5Ys0ewrLS0V3NzchK+//tqgNqnVagGAoFarDbsJInsiDluTv9lAx476m5SVZZNmETm2+fON/3fAltvgwSbfupzPb1k9LJWVlcjJyUFkrW4gFxcXREZGIjMzU/K48vJydOjQAYGBgXj55Zdx+vRpTZlKpUJxcbHWOT09PREWFiZ5zvv376OsrExrI3JYxiY1scHroc2b9ZfrG+9CRNA95sSRBoLVDOkYPNjqPSyyApYbN26gqqoKPj4+Wvt9fHxQXFys85iQkBCsX78e3333Hb788ktUV1cjIiIChb+kyqw5Ts45Fy1aBE9PT80WGBgo5zaI7Ispv/SJiVYdjBsaCowfL13+44/Ae+9ZrTlE9s3Rg5Pa5s8X+1Xu3hX/tPbrIFhhllB4eDhiY2PRu3dvPP/88/j222/Rtm1bfPbZZ0afMz4+Hmq1WrMVFBSYscVENmDMWJYaNYNxrTSIJDUV2L5dunzhQiaVo0as9qBYRwtO3N2lXwDJXXHeAmQFLN7e3mjSpAlKSkq09peUlMDX19egczRr1gx9+vRBXl4eAGiOk3NONzc3eHh4aG1EDk3uwoi6WDHzZHS0/pWbV6wARo2ySlOIrK9uz0ntgbH2NCjWEIMHPwpK5Kx1ZgOyAhZXV1f07dsXylpdQdXV1VAqlQgPDzfoHFVVVTh58iT8/PwAAEFBQfD19dU6Z1lZGY4cOWLwOYmcwt27pi/SY8V/LGfPBn73O+nyb74Bjh61WnOILKNVq4Zf6zjKjNbawUnNZoNXO8aS/UooLi4Oa9euxYYNG3DmzBlMnToVFRUVmDhxIgAgNjYW8fHxmvrz58/Hv//9b+Tn5+PYsWP4/e9/j0uXLmHy5MkAAIVCgZkzZ2LBggX4/vvvcfLkScTGxsLf3x8jR440z10SOQqlUvxHRO56Q7UpFOI/slbw8cf6y6OjrdIMIvPQta5OaamtW2WcmjEnDhqc6NJU7gFjxozB9evXkZiYiOLiYvTu3Rvp6emaQbOXL1+Gi8ujOOjWrVuYMmUKiouL0apVK/Tt2xeHDh3Ck08+qakzd+5cVFRU4PXXX0dpaSmeffZZpKen10swR9Ro3Lol/uNZswCZXKWlYje1hbt4a5LK/fL/j3quXQMGDAAOHrRoM4jkGzLE8V7fSJk/3y7GmFgaFz8ksmem/qMaFCSOj7Gwo0eB/v2ly0ePBjZtsngziHSru/CfiwtQXW279pjCSr/T1sLFD4mchVJpnhlEFs7XEhoKzJghXb55M9P3kxXUTVsvNebEUYKVoKD6r3WcKFiRiwELkb3Lzxe7fE1hhXT+y5cDXbtKl588yenOZGY1AUrr1o45Q6eGrvEmjTw40YWvhIgcSbNmpq2+aoXu5O7dxVWcpRQUiGNfiAxW+9Voq1biGC9HoVCIwUcNJ3ulYyq+EiJyVg8emDaDSKUSB+NaUHq6/nL2slCDaqYS1+Q3qd1z4gjBSs3vaFCQ+PqJvSZmwYCFyNHcumVavpZ79yyazr9m5pCUrVuZVI5+ITXmpGYqsaPkNwHEIKUmKLl1i8GJBTBgIXJESqVp41osnM5/0iQgK0u6nEnlGqG6wUmzZo475kRXAjZH6PlxcAxYiBxVQoL4D6Ups4gyMizW2xIaCixeLF3+0ksWuSzZA10J2OoGJ6aMxbImB88O60wYsBA5uvx806c+W2hcy5w50tOdi4vFoIacQN21dYxNeGgrNWNOGJzYNQYsRM4gP9/0cS3NmpmvPbUsXy4dT2Vnm758EllR3dc6Li6OuSoxoB2c1Iw5YXBi1xiwEDkLpdK0T/+HDy22BtHy5dJle/Zw5pBdqglO9K1EbK9ZMZrWWXXGCdfVaYwYsBA5E1MH45aWWmQwbnQ00KOHdPmKFcC775r1kiRHcrLYW1J7ZeKa4MTeZ+q41PkYmz9fnP5fOzhpBOvsNAZMHEfkrExZPBEA3N3NvnhiaKj4GkjKkiXA7NlmvSTV5ciL/rm4iAFIx47iz/bgwewpcXBMHEdE4rgWU5LM3btn9ldER48CvXtLl8+ZAxQWatdPTARWr9beTwaqOxjWUdPX16ypU1UlJmLLz+drnUaIAQuRM7t1y7SgpbTU7NOet2/XX75ypfjnhAniCtDJycCf/wwEBgJjxjB/iyRdSdgcaTDs4MHi6xyFov6YEyZgI/CVEFHjYOprgKZNxXEBZvLaa0BKinT5008Dx45Jl3fqBMyaBcTENNJ1iVq1EoNJd3f7H2PSEK6t06jxlRARabOzGUTr1wM+PtLl+oIVALhw4VGvy/DhYkeCU/a81H2l06SJ46au15XjhL0nJAN7WIgak+Rk018TmHGgY0MrO8vVuTMQFycGMllZgJ+f+PcffxQT1T32GNCvn7hv7VrgwAHgzh0xFuvUCWjZUpzFW1gIeHqKn6fXrwNt2wIdOgBt2gBubuKrqjt3gJs3H137xx/FoKl160czgb29gRYtxPI7d8S4r0cP8dzp6UBennjdiAjgxg3gxoEzqPzfBfTESXiiFEfwDIrhgwdQQIXOaInbGIeNcMM97EQMWuJnNIOAYfgBAShCDvrgBLrDFWIW2WvwhgsEBKIAHXAJmXgGeeiMdihBK5TCB9egxuM4h6fwLPZjBlajHI/jEgKxCaOhhge8cQPBuIj2uISDGIDraIubaA0XVAFQoAXuIQo/oAxeuI2W6IR8qNESJ1sNwoO+4WjWTLznjh3F70NmJpCbC/zqV8BTT4m9ZCUlwObNwOOPA/fvi9+Lpk3F56RSAc8/D/TpAxw/DmzcKD7H8ePFMeHnzonPxM1NfHt54ID4M1VSIn7P27cXv8eCIB7n5iae84kngGefFduSmSk+8yeeEM9x545Y5+5d8dl07QocOQKcOAFUVorn+fln8edl4EDxmYeEiM89Kwt48UUxIP/nP8XzVFaKHUn374s/F8HBj+L/ixfFnwMfH+DKFeD8eTEubdlS/LkTBDFFkpcX8N//Ak8+CYSHi297T5wA1Gqx3vXr4hCfGzfEYT6tWomxoIsLEBYGdOkC7N0rXvOxx8T25OWJvxcTJojlKSmPEhBXVor35u8v3lPnzuL+gQPNm/BRzuc3AxaixsjUV0Rm6sYvLBQ/UBz/XyFrEwAo9Hxt6nl1nU/fNcx1fXIE48cDqanmORdfCRGRfkqlGCW4uxt3vJnS+QcE6F9viKTUDQ7MFSwo6vxp6DUYrDQmGzbY5hUsAxaixuzuXeNnEd27Z5agZfZsYN48/XWSkpjCn8ieHDxo/WsyYCFq7G7dMn7xRDPlalm4EBg9Wrp88mSxUygrC3jrrUfv04nINgYMsP41GbAQkTgepe76K4aqSedvYr6WTZvEga11ffHFo6nLoaHAsmXiwMSsLFtOabb1oJu617d1e6gxGT/eNiutG/kvFBE5nQcPHuX3MIZKJU5nMCFfy8WL4mC+jRvFjLjTp0sHJaGhQEEBkJYmvlN/8ECcLdG3r3hMdjbg6yv+PSPj0SyhmvLas4RatxZ7bR5/XBzWc2XrEXgIN6GAAtfRFm1xHe1RgDa4AXfcRz9k4y5a4CZaa9qzG4OQjTC0wnW0wF24QAFv3EAL3AEA3EELPEBT9MApeKIMP2AoLqAzOuMCwpGJm2iDa2iLh5o6ahxBGIrhg0q4QIVOaIly/A4b4Yp7SMcIPIZSuKEaQ5GOQFxFDvrgJLqjGR5CAHDdNQCKfv3RoYM4uPnQITE29fYW79nXV+xg+9//xP8xz5gBVFSIz2HLFnEGire3GIsGBIizaUpKxGNqlvBp3hwYOhQoLwdu3xbrqtXAyZPijJOmTbVnCR06JM526dxZ3D9ihHjOb74RZ1Q9eABcuwa4uoqzdi5dEmem1OTm2bxZrBcbK3bw/e9/Yl13d/EaBw8CP/0knrO8XNxXE0s/9pj4I3rpkjj7ZcAAcabNwYPiAPDAQO1ZQnfuiDNyunXTniXUooX4PWjZUpxp5O0tzrJxdxd/7oYNE2f9fP21OAX/4UMxGK/51ag7S+jCBfFn98oVceZO06biz6K3t/h/gWbNAA8P8Xvatas4c+nWLfFrtVqcvXbzpnj+mzfF4WmenuK5FQrgmWfE7/e+fWJZy5Zie/LyxO91bKzY/prfI0EQ2/zss+JMu+xssc0uLuL3zBbBCsBZQkRUlzmmPs+f7zgLzpm65pItNW36aB6qI33PiX4h5/ObPSxEpC0hQUzYYMq055qAx94+QB154b/azJx5mMgRcAwLEdWnVIr/YzdFYqIYINiKrrV1HDFYqbuujiAwWKFGia+EiEi/5s1NSwHv5SW+cLeU2r0mQUGO+3oHMGsWYSJHwMRxRGQ+puRqAcRBvOZah6ju2jp1e00cKVjRtbYOgxUiSQxYiKhht24ZnxUXMG7qc3KyOC0hOPhRcGLqYGBb0fVah8EJkSwcdEtEhrl71/RBqyqV2NtS9xWRvunUjtRrAnC2DpGFMGAhIsPV9AqYkq+lprfFGXDMCZHV8JUQEcl361bjW9yHY06IbIoBCxEZR6l03qCFwQmR3WHAQkTGc/SgRddgWAYnRHaJAQsRmcaeg5baq1DrCk44OJbIYRgVsKxatQodO3aEu7s7wsLCkJWVJVl37dq1GDhwIFq1aoVWrVohMjKyXv0JEyZAoVBobcOGDTOmaURkC+bIjGtONa908vMZnBA5CdkBy6ZNmxAXF4ekpCQcO3YMvXr1QlRUFK5du6az/t69ezFu3Djs2bMHmZmZCAwMxNChQ3HlyhWtesOGDUNRUZFm+/rrr427IyKyjYQEMTCwdm8Lx5sQNQqyU/OHhYUhNDQUK1euBABUV1cjMDAQM2bMwDvvvNPg8VVVVWjVqhVWrlyJ2NhYAGIPS2lpKbZt2yb/DsDU/ER2xxwrPtfm5fVoGjXznBA5DYul5q+srEROTg4iIyMfncDFBZGRkcjMzDToHHfu3MGDBw/QunVrrf179+5Fu3btEBISgqlTp+LmzZuS57h//z7Kysq0NiKyIwkJpr8iCgp61GNy6xZf7RA1crIClhs3bqCqqgo+Pj5a+318fFBcXGzQOd5++234+/trBT3Dhg3D3//+dyiVSnz88cfYt28fhg8fjqqqKp3nWLRoETw9PTVbYGCgnNsgImuoeUVk7Jafb+s7ICI7YtVMtx999BE2btyIvXv3wr3WuiRjx47V/L1Hjx7o2bMnOnXqhL1792KIjuXp4+PjERcXp/m6rKyMQQsREZETk9XD4u3tjSZNmqCkpERrf0lJCXx9ffUeu3TpUnz00Uf497//jZ49e+qtGxwcDG9vb+Tl5eksd3Nzg4eHh9ZGREREzktWwOLq6oq+fftCWWsEfnV1NZRKJcLDwyWPW7x4MZKTk5Geno5+/fo1eJ3CwkLcvHkTfn5+cppHRERETkr2tOa4uDisXbsWGzZswJkzZzB16lRUVFRg4sSJAIDY2FjEx8dr6n/88cdISEjA+vXr0bFjRxQXF6O4uBjl5eUAgPLycsyZMweHDx/GxYsXoVQq8fLLL6Nz586Iiooy020SERGRI5M9hmXMmDG4fv06EhMTUVxcjN69eyM9PV0zEPfy5ctwcXkUB61evRqVlZX47W9/q3WepKQkvP/++2jSpAlOnDiBDRs2oLS0FP7+/hg6dCiSk5Ph5uZm4u0RERGRM5Cdh8UeMQ8LERGR47FYHhYiIiIiW2DAQkRERHaPAQsRERHZPQYsREREZPesmunWUmrGDXNNISIiIsdR87ltyPwfpwhYbt++DQBMz09EROSAbt++DU9PT711nGJac3V1Na5evYqWLVtCoVCY9dw16xQVFBQ45ZRpZ78/wPnv0dnvD3D+e3T2+wN4j87AEvcnCAJu374Nf39/rRxuujhFD4uLiwsCAgIseg1nX7PI2e8PcP57dPb7A5z/Hp39/gDeozMw9/011LNSg4NuiYiIyO4xYCEiIiK7x4ClAW5ubkhKSnLadY2c/f4A579HZ78/wPnv0dnvD+A9OgNb359TDLolIiIi58YeFiIiIrJ7DFiIiIjI7jFgISIiIrvHgIWIiIjsXqMPWBYuXIiIiAi0aNECXl5eOutcvnwZI0aMQIsWLdCuXTvMmTMHDx8+1Hven3/+Ga+++io8PDzg5eWFSZMmoby83AJ3IM/evXuhUCh0bkePHpU87oUXXqhX/09/+pMVWy5Px44d67X3o48+0nvMvXv3MG3aNLRp0waPP/44/u///g8lJSVWarHhLl68iEmTJiEoKAjNmzdHp06dkJSUhMrKSr3H2fszXLVqFTp27Ah3d3eEhYUhKytLb/0tW7aga9eucHd3R48ePbBz504rtVS+RYsWITQ0FC1btkS7du0wcuRInDt3Tu8xqamp9Z6Xu7u7lVosz/vvv1+vrV27dtV7jCM9P0D3vykKhQLTpk3TWd/en9/+/fsRExMDf39/KBQKbNu2TatcEAQkJibCz88PzZs3R2RkJM6fP9/geeX+HsvR6AOWyspKjBo1ClOnTtVZXlVVhREjRqCyshKHDh3Chg0bkJqaisTERL3nffXVV3H69Gns3r0baWlp2L9/P15//XVL3IIsERERKCoq0tomT56MoKAg9OvXT++xU6ZM0Tpu8eLFVmq1cebPn6/V3hkzZuit/9Zbb2H79u3YsmUL9u3bh6tXr+I3v/mNlVpruLNnz6K6uhqfffYZTp8+jU8//RRr1qzBvHnzGjzWXp/hpk2bEBcXh6SkJBw7dgy9evVCVFQUrl27prP+oUOHMG7cOEyaNAnHjx/HyJEjMXLkSJw6dcrKLTfMvn37MG3aNBw+fBi7d+/GgwcPMHToUFRUVOg9zsPDQ+t5Xbp0yUotlu+pp57SauuBAwck6zra8wOAo0ePat3f7t27AQCjRo2SPMaen19FRQV69eqFVatW6SxfvHgxli9fjjVr1uDIkSN47LHHEBUVhXv37kmeU+7vsWwCCYIgCCkpKYKnp2e9/Tt37hRcXFyE4uJizb7Vq1cLHh4ewv3793We66effhIACEePHtXs++GHHwSFQiFcuXLF7G03RWVlpdC2bVth/vz5eus9//zzwptvvmmdRplBhw4dhE8//dTg+qWlpUKzZs2ELVu2aPadOXNGACBkZmZaoIXmtXjxYiEoKEhvHXt+hv379xemTZum+bqqqkrw9/cXFi1apLP+6NGjhREjRmjtCwsLE/74xz9atJ3mcu3aNQGAsG/fPsk6Uv8m2aOkpCShV69eBtd39OcnCILw5ptvCp06dRKqq6t1ljvS8wMgbN26VfN1dXW14OvrKyxZskSzr7S0VHBzcxO+/vpryfPI/T2Wq9H3sDQkMzMTPXr0gI+Pj2ZfVFQUysrKcPr0acljvLy8tHosIiMj4eLigiNHjli8zXJ8//33uHnzJiZOnNhg3X/+85/w9vZG9+7dER8fjzt37lihhcb76KOP0KZNG/Tp0wdLlizR+xovJycHDx48QGRkpGZf165d0b59e2RmZlqjuSZRq9Vo3bp1g/Xs8RlWVlYiJydH63vv4uKCyMhIye99ZmamVn1A/L10hGcFiM8LQIPPrLy8HB06dEBgYCBefvllyX9z7MH58+fh7++P4OBgvPrqq7h8+bJkXUd/fpWVlfjyyy/x2muv6V1w15GeX20qlQrFxcVaz8jT0xNhYWGSz8iY32O5nGLxQ0sqLi7WClYAaL4uLi6WPKZdu3Za+5o2bYrWrVtLHmMr69atQ1RUVIOLR/7ud79Dhw4d4O/vjxMnTuDtt9/GuXPn8O2331qppfK88cYbePrpp9G6dWscOnQI8fHxKCoqwrJly3TWLy4uhqura71xTD4+Pnb3zOrKy8vDihUrsHTpUr317PUZ3rhxA1VVVTp/z86ePavzGKnfS3t/VoC4uvzMmTMxYMAAdO/eXbJeSEgI1q9fj549e0KtVmPp0qWIiIjA6dOnLb7Yq1xhYWFITU1FSEgIioqK8MEHH2DgwIE4deoUWrZsWa++Iz8/ANi2bRtKS0sxYcIEyTqO9PzqqnkOcp6RMb/HcjllwPLOO+/g448/1lvnzJkzDQ4KcyTG3HNhYSF27dqFzZs3N3j+2uNvevToAT8/PwwZMgQXLlxAp06djG+4DHLuMS4uTrOvZ8+ecHV1xR//+EcsWrTIbtNmG/MMr1y5gmHDhmHUqFGYMmWK3mPt4RkSMG3aNJw6dUrvGA8ACA8PR3h4uObriIgIdOvWDZ999hmSk5Mt3UxZhg8frvl7z549ERYWhg4dOmDz5s2YNGmSDVtmGevWrcPw4cPh7+8vWceRnp+jcMqAZdasWXojXwAIDg426Fy+vr71RjnXzBzx9fWVPKbuIKOHDx/i559/ljzGVMbcc0pKCtq0aYOXXnpJ9vXCwsIAiP+7t9aHnSnPNSwsDA8fPsTFixcREhJSr9zX1xeVlZUoLS3V6mUpKSmx2DOrS+79Xb16FYMGDUJERAQ+//xz2dezxTPUxdvbG02aNKk3I0vf997X11dWfXsxffp0zSB8uf/LbtasGfr06YO8vDwLtc58vLy80KVLF8m2OurzA4BLly7hxx9/lN0z6UjPr+Y5lJSUwM/PT7O/pKQEvXv31nmMMb/HspllJIwTaGjQbUlJiWbfZ599Jnh4eAj37t3Tea6aQbfZ2dmafbt27bKrQbfV1dVCUFCQMGvWLKOOP3DggABA+O9//2vmllnGl19+Kbi4uAg///yzzvKaQbfffPONZt/Zs2ftdtBtYWGh8Ktf/UoYO3as8PDhQ6POYU/PsH///sL06dM1X1dVVQlPPPGE3kG30dHRWvvCw8PtdtBmdXW1MG3aNMHf31/43//+Z9Q5Hj58KISEhAhvvfWWmVtnfrdv3xZatWol/PWvf9VZ7mjPr7akpCTB19dXePDggazj7Pn5QWLQ7dKlSzX71Gq1QYNu5fwey26nWc7iwC5duiQcP35c+OCDD4THH39cOH78uHD8+HHh9u3bgiCIP2Tdu3cXhg4dKuTm5grp6elC27Zthfj4eM05jhw5IoSEhAiFhYWafcOGDRP69OkjHDlyRDhw4IDwq1/9Shg3bpzV70/Kjz/+KAAQzpw5U6+ssLBQCAkJEY4cOSIIgiDk5eUJ8+fPF7KzswWVSiV89913QnBwsPDcc89Zu9kGOXTokPDpp58Kubm5woULF4Qvv/xSaNu2rRAbG6upU/ceBUEQ/vSnPwnt27cXMjIyhOzsbCE8PFwIDw+3xS3oVVhYKHTu3FkYMmSIUFhYKBQVFWm22nUc6Rlu3LhRcHNzE1JTU4WffvpJeP311wUvLy/N7Lw//OEPwjvvvKOpf/DgQaFp06bC0qVLhTNnzghJSUlCs2bNhJMnT9rqFvSaOnWq4OnpKezdu1fred25c0dTp+49fvDBB8KuXbuECxcuCDk5OcLYsWMFd3d34fTp07a4Bb1mzZol7N27V1CpVMLBgweFyMhIwdvbW7h27ZogCI7//GpUVVUJ7du3F95+++16ZY72/G7fvq35vAMgLFu2TDh+/Lhw6dIlQRAE4aOPPhK8vLyE7777Tjhx4oTw8ssvC0FBQcLdu3c15xg8eLCwYsUKzdcN/R6bqtEHLOPHjxcA1Nv27NmjqXPx4kVh+PDhQvPmzQVvb29h1qxZWtH1nj17BACCSqXS7Lt586Ywbtw44fHHHxc8PDyEiRMnaoIgezBu3DghIiJCZ5lKpdL6Hly+fFl47rnnhNatWwtubm5C586dhTlz5ghqtdqKLTZcTk6OEBYWJnh6egru7u5Ct27dhA8//FCrR6zuPQqCINy9e1f485//LLRq1Upo0aKF8Morr2gFAfYiJSVF589s7Q5TR3yGK1asENq3by+4uroK/fv3Fw4fPqwpe/7554Xx48dr1d+8ebPQpUsXwdXVVXjqqaeEHTt2WLnFhpN6XikpKZo6de9x5syZmu+Hj4+P8OKLLwrHjh2zfuMNMGbMGMHPz09wdXUVnnjiCWHMmDFCXl6eptzRn1+NXbt2CQCEc+fO1StztOdX87lVd6u5h+rqaiEhIUHw8fER3NzchCFDhtS77w4dOghJSUla+/T9HptKIQiCYJ6XS0RERESWwTwsREREZPcYsBAREZHdY8BCREREdo8BCxEREdk9BixERERk9xiwEBERkd1jwEJERER2jwELERER2T0GLERERGT3GLAQERGR3WPAQkRERHaPAQsRERHZvf8HbbKr8nEotx8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = LinearRegressionNN(hidden_nodes=(3,3))\n",
        "\n",
        "# Initialize weights using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        try:\n",
        "          torch.nn.init.xavier_uniform_(m.weight)\n",
        "          torch.nn.init.zeros_(m.bias)  # Initialize bias to zero\n",
        "        except:\n",
        "          pass\n",
        "model.apply(init_weights)\n",
        "\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=.1)  # Lower learning rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000  # Increase number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        print([i.data for i in model.parameters()])\n",
        "# Test model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor([[0.5]])\n",
        "    test_prediction = model(test_X)\n",
        "    print(f'Prediction for input 0.5: {test_prediction.item()}')\n",
        "\n",
        "\n",
        "\n",
        "pred_y = model(X_tensor).detach().numpy().flatten()\n",
        "true_y = base_model(X_tensor).detach().numpy().flatten()\n",
        "plt.plot(X_tensor, true_y, 'rX')\n",
        "plt.plot(X_tensor, pred_y, 'b.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "f46-fn8z18E2",
        "outputId": "a6a7328e-dcb7-42ad-fd9c-490c8a8a6a12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c6ec4b6a7d0>]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/j0lEQVR4nO3deXyU1d3//9eEJQlKFmogLAFncImWzRKI4IYZKhRQaRWxeluCGNv+FKumIAjC1wxIBVpUbNWCLHprRb3rQrRWIIrWUgIIRRSoERAIDaKSBJAESK7fH9cwM5FckwQy+/v5eMzjzrlyhpzmxuTNWT7HZhiGgYiIiEiEiAv1AERERESaQuFFREREIorCi4iIiEQUhRcRERGJKAovIiIiElEUXkRERCSiKLyIiIhIRFF4ERERkYjSMtQDaG61tbXs27ePtm3bYrPZQj0cERERaQTDMDh06BCdOnUiLs7/3ErUhZd9+/aRkZER6mGIiIjIadizZw9dunTx2yfqwkvbtm0B8398UlJSiEcjIiIijVFZWUlGRobn97g/URdeTi4VJSUlKbyIiIhEmMZs+dCGXREREYkoCi8iIiISURReREREJKIovIiIiEhEUXgRERGRiKLwIiIiIhFF4UVEREQiisKLiIiIRBSFFxEREYkoQQkvf/zjHzn33HNJSEggOzub4uJiv/1feeUVMjMzSUhIoGfPnrz99tvBGKaISORyOMBm875atarbDtYrVF9Xr9C9nM6g/3UP+PUAy5Yt4/777+fpp58mOzubxx57jCFDhrB9+3bat29/Sv9//vOf/PznP2fWrFmMGDGCF198kZEjR/Lxxx/To0ePQA9XRCTyJCZCVRV76cxz/A/LGU7FiSR68ym38gJ7yGA7F3CMVuynA33ZSCdKeY7b2ENnjtEagxb8mL/Tlu84xNlk8TGJfEcROZzNYc7ha6CWd/gJNdg4ny9ox7fsJ52+rKcXW1iJk7ITHUhnP4NZxXe0YQM/ooTunEcJfdnEFjJ5mxFczKcM4F/s4lw+5DK+Jo0WnKAthxnK3/icC9hKJq05xnFak8ZXZLCX7uwglXIOksIX2LFh4xgt2UYmCVRxFkfoxRYOcTYlnM/FbGE7mVSQRAtq+JZ2tOcA17Kc3XRjP+0xsHE2R2jNMTbSm6MkMoR36cB+lnEzLTlOb/7NXjqzg+7YqKEtRzhBHN/yAxzs5DxK2EhvKkimN1s4n+28Tw7xHKUbu2nNMXZip4Y4DtOWb0mlikQu4D/8lDc4SjyvM5IUDtKFfeynPWdxhC1czFHOZhTL+Jo0islmGIX0YCvf8AMADpLMV3TgW1LYTG9SOci1LOd9rmIzvWnJceKppidbGMrf2URvPuNidtKNE7QkjloOkcwJbKTzFb34N5/RgxacwCCO9nzFxWwFainkWmqx0Z915PMY/VgPRUVmgFm1Kmh/5W2GYRiB/ALZ2dn069ePJ598EoDa2loyMjIYP348kyZNOqX/6NGjOXLkCIWFhZ5nl156KX369OHpp59u8OtVVlaSnJxMRUWF7jYSkeiXmgrl5cwlnwnM5tQJdQOw1fPG+p5//1lD7aa8tzF/RlP6NFaoxtHU9zbU3/fzZzL25nyvwRiWsITb3c0zixNN+f0d0GWjY8eOsWHDBgYPHuz9gnFxDB48mDVr1tT7njVr1tTpDzBkyBDL/tXV1VRWVtZ5iYjEBKcTysuZQz4TmEP9P9KtflHV9/z7zxpqn07fxvxCb67gEspxNPW9DfW3WXzc1D+vOd9rYym5rCMr6EtHAQ0vX3/9NTU1NXTo0KHO8w4dOlBWVlbve8rKyprUf9asWSQnJ3teGRkZzTN4EZFwV1TEXjozkdk07y98kcay8VHij2HlyqB+1Yg/bTR58mQqKio8rz179oR6SCIigeVymRslgeUMJwp+lEvEMrjs6Ar43opJoAV0w+4555xDixYt2L9/f53n+/fvJz09vd73pKenN6l/fHw88fHxzTNgEZFw53SaGySBOeQzkUcbeIP2vGjPS1PH1bQ9L/1YD8HbqwsEOLy0bt2avn37smrVKkaOHAmYG3ZXrVrF3XffXe97BgwYwKpVq7j33ns9z1asWMGAAQMCOVQRkfDncnmCy1QKmMlUrH7JdGAfObzPLbzIXrrwHy6gitYcoD0/YiMd2cfz3MYeOnGMeAziGMy7JHOEQ5xNXz4mgaO8z9W04TDt+ZpaDN5lCDW04HxKSOGg+8/7mJ5soYgcyjBPG+VQxFHasIFL+ILudOcL+rKRT7iIvzGci/iUgaxlF934B5fzFT+gBbUkU8k1/J0SLmArFxLPcappRRoH6MYeHD6njXbgAKCaVvyHC2lNNW05TE+2UElbdnAemXzK51xABSme00Zp7GcEb7GXrpRhblM4iyO04hj/pjffkcgQVtCe/bzMTbSkhl78m3105gvPaaPDHCeOg5yD3X3aaBO9PKeNzuM/rGYQCVTTld204hhf0o3jtOQIZ/Mt7ThKAhfyOT/ldb4jgTe4nlTK6UQpB2hPIkf4jIv5jrO4kVf4mjTW05+hvEVPtvIN7QA4SAoHaM/XpLKFXqTwLSN4iw+4ik30oiUniKeK3mzhGt5lM73ZwsXsois1tMJGLZUkUUMcHdlPDzazjR7YOIENG2kc4Id8Ri0Gb3EtNdi4lGLu43EzuADk5AT6b//3/ooH2EsvvWTEx8cbS5YsMT777DPjzjvvNFJSUoyysjLDMAzjtttuMyZNmuTp/9FHHxktW7Y05s6da2zdutWYPn260apVK+OTTz5p1NerqKgwAKOioiIg/3tERELGPM9hTKHAgNqTzTovGzXGHPJP/USwXi1bhu5r6xWaV05Os/z1bsrv74DXeRk9ejQHDhxg2rRplJWV0adPH9555x3Pptzdu3cTF+ddrx04cCAvvvgiU6dO5cEHH+T888/n9ddfV40XEYltDnOWYTzzeJLfUN+Mi40a1nKp91/DKSlw8GDwxigSJAGv8xJsqvMiIlHHXcvlJpbxCqOwWiqazQQm8HuzmZMT1KJhImcqbOq8iIjIGXIHl0KG+Q0uU5jhDS42m4KLRDWFFxGRcOUuQvcst3Mty/EXXGYwzfvo4YeDNUKRkFB4EREJV+4idHewgPp/XBvczeN1g0tBATz0ULBGKBISAd+wKyIiTeReKgJ4jv/B6t+ZvdnEfO7zPoiuLYwilhReRETCifuGaICebGALl1h0NFjAnd5mQUHgxyYSJhReRETChdPpCS4JHKGaRKz2udzIy94j0Xa7lookpmjPi4hIOPCpntuRL/0GlynM4BVuNpspKbBjR7BGKRIWNPMiIhIOppmbblP5mnLaYRVcljOCEbxtNlWETmKUZl5ERELNXT33Xub4DS492OgNLjk5Ci4SszTzIiISSu4NusN5k7cZgVVwiecon9DX+0hF6CSGaeZFRCRU3Bt0RzQQXNLZQxVneR8F+wZfkTCj8CIiEipFRawji7f8BJcUvuW/dPM+SknRrIvEPIUXEZFgc7nM+4eAAqZSf3CBn7GMg5zjfWC3a5+LCNrzIiISXA4H7NwJwD3Mo5DrLDrW8ji/9TYTEnQkWsRN4UVEJFh8quc6eZciBlP/rEstC8mjC6VmU0eiRepQeBERCYbUVE9wyWING8jGKrgUk123eq5mXETqUHgREQk0l8tz0WJ3trGDC7DaoDuHid7goqUikXppw66ISKC5q+dezmq/wWU8j/Nbfu99dPRoUIYnEmkUXkREAsnpBGAKBXzEFVgFlxxW8gT3eR/plmgRSzbDMIxQD6I5VVZWkpycTEVFBUlJSaEejojEMqcTiorYS2cy+BJoUW+3LuxiD3bvg5wc1XKRmNOU39+aeRERCQSf4PJr/ohVcAGDvzLK21RwEWmQwouISHNzB5dnuZ2u7KaQ6y06GoxhiXeDbkGBgotIIyi8iIg0J5fLM+NyBwswLH7M/ohiiunPEm43H+TkwEMPBXGgIpFLR6VFRJqLy+U5WZTDSqz+fRhHDW/wM28ROrtdMy4iTaCZFxGR5uATXKbxEJ9zoUVHg0d5wBtccnJUy0WkiTTzIiLSHNzBpRcb+YTeWB2JfpAZ3lou2pwrclo08yIicqYSEwGzCJ2/4DKWRczEDDkKLiKnT+FFRORMOBxQVcXUBorQ9WUdi7jD+0jBReS0adlIROR0uW+J3ktnZjKF+oML/Jh3eJdh3geqnityRjTzIiJyOtzBBeAyPsT6x6nhXSoC82SRjkSLnBGFFxGRpnK5PMElniPs5lyLjgajeMVbhE4ni0SahcKLiEhT+ByJbs8+jpGI1T6Xu3mclxltNlNStM9FpJkovIiINJZPcBnI+xwgHavgMp7HmX/ylui4ODh4MGjDFIl2Ci8iIo3hE1zu4THWcCVWwSWTLTxxMrgkJEBNTdCGKRILdNpIRKQhPsFlPPN4knuwCi6tOcpWenkfHT0alCGKxBKFFxGRhriDy00s4xVGYXUkOoVvOcg53gc6Ei0SEFo2EhHxJzUVgCXc5je4QK27uq6bjkSLBIxmXkRErLRqBSdOMII3eYsRWAcXg9lM9F62mJKiI9EiAaTwIiJSH3dwGc4bvN1AcJnCDCacvGwxIUEni0QCTOFFROT7UlPhxAnWkcXbXIu/paLZTPQGl5QUBReRIFB4ERHx5XBAeTkAV1KEv+BSTLa3eq7drqUikSDRhl0RkZOcTti5E4D2lFLF2RYdDaYwU8FFJEQUXkRETioqAqAjezlAR6xquQzkQ2acvGxR9xWJBJ3Ci4gImLMumEeiy+iEVXDpwm4+4irvI91XJBJ02vMiIpKaCuXlrCOLSfwOq30uZ1HBHt8bpFWETiQkFF5EJLYlJkJVFaNYxqsNFKHbRg9vU0XoREJGy0YiErscDqiqYioFDQaXheR5i9Bpg65ISGnmRURik8MBO3eyl87MZApWwcVGLWt9j0Sreq5IyCm8iEjsce9xAcjjaawnoc2y/57gouq5ImFBy0YiElucTk9wcfIu7zDcoqPBg8zgtyer59rtcPRoUIYoIv4pvIhIbHHXcnHyLkUMxupI9HieYKZquYiEJYUXEYkd7louUyjwG1yGU8gT3Gs2U1JUy0UkzAQ0vHz77bfceuutJCUlkZKSwrhx4zh8+LDf9wwaNAibzVbn9atf/SqQwxSRWJCYCEVFrCOLR3gQq+CSy7MUcp33kfa4iISdgIaXW2+9lU8//ZQVK1ZQWFjIBx98wJ133tng+/Ly8vjvf//rec2ePTuQwxSRaOeu5fIst9OftUCLertdwgYWk+d9oCJ0ImEpYKeNtm7dyjvvvMO6devIysoCYP78+QwbNoy5c+fSqVMny/e2adOG9PT0QA1NRGKJywVVVeylM3ewEH+1XN5kpLdZUKAidCJhKmAzL2vWrCElJcUTXAAGDx5MXFwca9eu9fveF154gXPOOYcePXowefJkvvvuO8u+1dXVVFZW1nmJiABmcJlmbrq9jaVYB5eaukXoDEPBRSSMBWzmpaysjPbt29f9Yi1b0q5dO8rKyizfd8stt9CtWzc6derE5s2beeCBB9i+fTt//etf6+0/a9YsHn744WYdu4hEAZ9aLmNZwPvkWHQ0KOZSby0XLRWJhL0mh5dJkybx6KOP+u2zdevW0x6Q756Ynj170rFjR5xOJ1988QXdu3c/pf/kyZO5//77Pe3KykoyMjJO++uLSBRo1QpOnADgXEr4EgdWG3TH8qw3uOTkaMZFJAI0Obzk5+eTm5vrt4/D4SA9PZ2vvvqqzvMTJ07w7bffNmk/S3Z2NgAlJSX1hpf4+Hji4+Mb/eeJSJRLTPQEl3Z8zUHaYRVcOrOHRSc36Obk6Ei0SIRocnhJS0sjLS2twX4DBgygvLycDRs20LdvXwCKioqora31BJLG2LRpEwAdO3Zs6lBFJNY4nVBVBUB7Sv0Gl5v4C8u41ftIwUUkYgRsw+5FF13E0KFDycvLo7i4mI8++oi7776bm2++2XPSqLS0lMzMTIqLiwH44osvcLlcbNiwgV27dvHmm2/yi1/8giuvvJJevXoFaqgiEg1cLk/13B+wnwN0xF8tlzrBRftcRCJKQOu8vPDCC2RmZuJ0Ohk2bBiXX345f/7znz2fP378ONu3b/ecJmrdujUrV67kmmuuITMzk/z8fG644QaWL18eyGGKSKTzOVXUjzV8SxpWwaUbO06t5aJ9LiIRxWYYhhHqQTSnyspKkpOTqaioICkpKdTDEZFgsJlB5R7mMZ/fYBVcUvmWbznH+0jBRSRsNOX3t+42EpHI5nAAMJd8v8Eljf/WDS52u4KLSIRSeBGRyOV0ws6d7KUzE5iDVRG6tpTzFZ29D+x23RItEsEUXkQkMjkcng26vdiIv7L/n9HT2ywoUHARiXAKLyISedwzLgDns5WDvstBdRiM5wlv2X8VoROJCtqwKyKRx71Btwu7KKUrVvtcevJvNnOJ2UxIgKNHgzZEEWkabdgVkai3hNv8Bhcnf/cGF1BwEYkiCi8iEjkcDnPWJSGB1/gZVsElnVJW8hPvIxWhE4kqAbtVWkSkWSUmQlUVe+nMc1X/wwHaAQZ1A4zBeWzncy7yPlItF5Goo/AiIuHP4YCqKuaSzwRm4500NvAGGIPO7K4bXFTLRSQqadlIRMKbywU7dzKHfHctF98fW+asSzYfsZgx7OVc76dSUnQkWiRKKbyISPhy31m0l85M5FHq3+NiI4uN5PK895HdDgcPBmuUIhJkWjYSkfDkcHhqufyEt4EWFh0NhvI38yh0VZVZy2XVqqANU0SCT+FFRMKPe3MuQHv2cYB0i44GfVnHCN6GKiC6ylaJiAUtG4lIeHFvzgW4iM3u4FL/kejxPM56ss2m3R60IYpIaGnmRUTCi3up6CK2sI2LsQouyxlhzriAuWSkzbkiMUMzLyISPlJTAbiHeX6Dy2V86A0udruq54rEGIUXEQkPTieUl7OXzsznHqxuiT6LQ/yDq7wPNOMiEnMUXkQk9JxOKCoC4DaWYv2jyeA9nN5mTk7AhyYi4Ud7XkQktHyCSy828gm9LToajOIV+rHebCYk6Ei0SIzSzIuIhI7L5QkuY1ngDi7173PJ5VleZrTZTEjQPheRGKbwIiKhM20aAHPJZwnjsAouwylkMXlmUxt0RWKewouIhIbLBcBeOrsvW6w/uNzEXyjkOrOp+4pEBO15EZFgc99XBGZw+TV/ov5/Rxn05N8s41azmZKi+4pEBNDMi4gEk09wuYfHyGC3d1ble4byNpu5xPtAwUVE3DTzIiLB4w4uvfiYT+iDVS0XqGEBv/Q2dSRaRHxo5kVEgiMxETBnXPwHF4PZPEAXSs2mbokWke9ReBGRwHPfEt1Q9dw4apjDBCbwe/NBQYGCi4icQstGIhJYqameW6IH8iFWweVyVvMXbq074/LQQ0EapIhEEs28iEjgpKZCeTkAvdnAHs616GjUDS6acRERPzTzIiKB4XB4gst5bOMLLsCqlsscJtQNLppxERE/NPMiIs3P6YSdOwFzOchfcMnlWX57co9LSoqCi4g0SOFFRJqXz0WL9zCPj7gCq+CSyWfesv8JCarlIiKNovAiIs3H56LFqRQwn99gtUHXwedspYfZ0EWLItIECi8i0nzcRejmkM9MpmJdy6WW1bgLzym4iEgTKbyISPNITQVgHVlM5FH8FaFbSJ53g66Ci4g0kcKLiJw5pxPKy3mW2+nPWqCFRUeD5YxgHIvMpsr+i8hpUHgRkTNXVMReOnMHC7D+sWIwhiWM4G2zqbL/InKaFF5E5PS5XGAzl4eu5TWsfqRcxCcU058l3G4+UHARkTOgInUicnocDk8tFyfvsoksi44GS7mdfqw3mwouInKGFF5EpOl8yv7fzgKKGIxVLZcbecUbXAwjWCMUkSim8CIiTeNT9j+Hd3nPT3AZyyIWcYfZ1OZcEWkm2vMiIo3nctVZKvIXXK5mhTe4pKRoqUhEmo3Ci4g0nrsI3VQK/CwVwSVsoIgh3gcq+y8izUjhRUQax+EAzCJ0DVXPfZOR3qaWi0SkmSm8iEjDUlNh5053Ebp/4a967hwmeqvnarlIRAJA4UVE/HO5oLzcpwiddfXcu3mc3/J7s2m3a7lIRAJCp41ExJrT6bkl+jpex1/13OEUMp/7zKZquYhIAGnmRUTq5xNc/sA9bKSvRccaHmQGhVxnNu12BRcRCSjNvIjIqVwuT3DJYi0b6IfVkejFjCWX582m3Q47dgRtmCISmzTzIiJ1uVyeI9G3s9BvcOnJJm9wKShQcBGRoLAZRnTV666srCQ5OZmKigqSkpJCPRyRyOO+aHEqBX6PRF/Cej6mn/dBdP0oEZEga8rvb828iIiXu5bLHPIbqOVSU7eWS0FBoEcmIuIRsPAyc+ZMBg4cSJs2bUhJSWnUewzDYNq0aXTs2JHExEQGDx7M559/Hqghiogv9y3Re+nMRB7FXxG6hdzpreWSkwMPPRSsUYqIBC68HDt2jFGjRvHrX/+60e+ZPXs2TzzxBE8//TRr167lrLPOYsiQIVRVVQVqmCIC5ski951FN/Iy1rVcaikmm3EsMps6Ei0iIRCw00YPP/wwAEuWLGlUf8MweOyxx5g6dSrXX389AM899xwdOnTg9ddf5+abbw7UUEVim8+R6H6sYT3ZFh1rWMid9GO92VRwEZEQCZs9Lzt37qSsrIzBgwd7niUnJ5Odnc2aNWss31ddXU1lZWWdl4g0kk9wuYgt7uBy6nLRpfyDPXTzzriolouIhFDYhJeysjIAOnToUOd5hw4dPJ+rz6xZs0hOTva8MjIyAjpOkajhU8tlLAvYxsVYHYl+gvvq7nHRkWgRCaEmhZdJkyZhs9n8vrZt2xaosdZr8uTJVFRUeF579uwJ6tcXiVjTpwMwl3yWMA6r4DKcQu9SUUGBZlxEJOSatOclPz+f3Nxcv30c7qOWTZWeng7A/v376dixo+f5/v376dOnj+X74uPjiY+PP62vKRKzHA4wDNaRxQRmYxVcslhbt+y/ThWJSBhoUnhJS0sjLS0tIAOx2+2kp6ezatUqT1iprKxk7dq1TTqxJCINSEyEqirmks9EZmM1AduVXaxjgPeBlopEJEwEbM/L7t272bRpE7t376ampoZNmzaxadMmDh8+7OmTmZnJa6+9BoDNZuPee+9lxowZvPnmm3zyySf84he/oFOnTowcOTJQwxSJLampUFXFVAqYwBwMyx8BtXzEFd6mitCJSBgJ2FHpadOmsXTpUk/7kksuAeC9995j0KBBAGzfvp2KigpPn4kTJ3LkyBHuvPNOysvLufzyy3nnnXdISEgI1DBFYod7xqWh6rk2aljgW4ROy0UiEmZ0t5FILEhNhfJy9tKZDHZjNel6I8uYR37d4KLlIhEJAt1tJCJeDgeUlwMwkA+x/s/eYCJzvcFFt0SLSJgK2LKRiIQB931FAP34F3s416KjwShe8R6Jjq4JWRGJMpp5EYlWPsElizWspz9WR6Lv5nFeZrTZtNuDNkQRkdOhmReRaOQTXLqznR2cj1VwGcsi5nOf2UxI0FKRiIQ9zbyIRBuXyxNcLmO13+CSyWcs4g6zmZICR48Ga5QiIqdN4UUk2kybBsBUCvgnV2BdPbeYrfQwmwkJcPBg0IYoInImFF5Eoon7eo69dGYmU7AKLn1Zyzou9T7SjIuIRBCFF5Fo4bPP5UZepv7/vA0cfM5637L/qp4rIhFG4UUkGvgEl9G8yFrfcOLjh2zmCy70PigoUPVcEYk4Om0kEul8Nuj2YiOf0Jv6l4tqeYfh3mZOjoKLiEQkzbyIRDr3Bt3bWeAnuBhMYaa3em5ODqxaFbQhiog0J4UXkUjldILNDCpzyWcx47AKLsMoZAZmyKGgQMFFRCKawotIJHI6oagIgEKGMYHZWAWXm3iJt7jObOqGaBGJAgovIpHIHVxyWcS1FGJ1sqgn/2YZt5hN3RAtIlFC4UUk0rhruawji6XkYl32/1k2c4n3kYKLiEQJhReRSOJ0ek4W3cLzWJ0qmsMEFpHnfaRaLiISRRReRCKFy+VZLrqHeZT41mvxcQcL+C2/9z5QLRcRiTIKLyKRwufOovn8BqvlojtZ6G0quIhIFFJ4EQl3LpfnSPQc8pnJVKyCy3AK6cd6s6ngIiJRSuFFJJy5XJ4Zl0KGMdHPkegcVlJ48ki0queKSBRTeBEJVz7BZRTL/B6JHs/jrOIas5mSoiJ0IhLVFF5EwpFPcBnNC7zKKKzL/s/gCe4zm3Y7HDwYtGGKiISCLmYUCUfu4NKPYtaThdWR6NlMZMLJk0W6r0hEYoTCi0i4SUwEYAm3+QkuBsu5lhG87X2k4CIiMULLRiLhJDERqqqYSz5jWYJVcLmRV+oGFxWhE5EYovAiEi4cDqiqYg75TGAOVv95DqKIVxjtfaAj0SISYxReRMKBywU7d7KXzn6OQwPU8jxjvE0FFxGJQdrzIhJqTqen7P+VvI/1vykMHmQmXSg1m6rlIiIxSjMvIqHkE1xG8yI76W7R0SCHVczEPIWkWi4iEss08yISKj4XLY7iJV7lJqw26F7NSm8RuoQE1XIRkZimmReRUPhe2X9/weUm/kKRb3A5ejRowxQRCUcKLyKh4A4uc8jnWt7EKrj05N8s41azqeAiIgIovIgEn8MBwFzymcgcoEW93QZRxGYu8T5QcBERARReRILL6fQciZ7QlCPROTnBGJ2ISETQhl2RYGnVCk6cAGA+/x/W/3aoYSF3eo9E2+06WSQi4kPhRSQYUlM9weUyVvNPrrDoaLCc67yl/3XZoojIKbRsJBJoLheUlwPQhS/dwaX+DbrDKPQGl4ICBRcRkXoovIgEks+R6D9wD6VkYBVchlPIW1xnNlU9V0TEksKLSCC5g8tUCshnHlbBZTFjKDwZXBISNOMiIuKH9ryIBIr7SPQolvEqo7AKLgP5kFyeN5t2O+zYEbQhiohEIs28iATKzp2sI8tPcIEkDvIRV5mNnBwFFxGRRlB4EQkUu53lDMe6lovBSoaYH7ZsqaUiEZFGUngRaU5OJ9hsMHgw7NhBR8osOhoMYzn9WG82jx8P2hBFRCKd9ryINBen03NL9LpVFXxoy+cC9gC11P13gkEOq3iL682mqueKiDSJwotIc3EHl1wWsZRczOUig4F8xL+4lFpaAjU8yCPMxDyFREqKlotERJpI4UWkObhcAKwjyye4ANj4J5exnBGczRHOo8Rb9j8hAQ4eDMVoRUQimva8iJwJl8vc4+Ku51L/Bl0bJVzAION9uthbm4/sdt0SLSJymjTzInK6fKrn7qUzj3MPc8mvp6PBZfzDs4lXRETOjMKLyOnw2Zz7LLdzJ3+mlhb1dDQYwxLzVJG2toiINAstG4k0lU9wKWQYd7DQIrjAU/yKJdxuNnSqSESkWSi8iDSVz6miaynEqghdC04wgrfMRk6OThWJiDQThReRpnA6gfpOFdVlo4Zn+KV5sshmU3AREWlGAQsvM2fOZODAgbRp04aUlJRGvSc3NxebzVbnNXTo0EANUaRpHA7PrMtdPIF12f9a1nIp41hkNh9+OCjDExGJFQHbsHvs2DFGjRrFgAEDePbZZxv9vqFDh7J48WJPOz4+PhDDE2maxESoqgLgdhayjkstOhqMZ7637H9BATz0UHDGKCISIwIWXh52/2tzyZIlTXpffHw86enpARiRyGnyCS7nsY0vuID6Z10MuvMfnuBes6ngIiISEGG35+X999+nffv2XHjhhfz617/mm2++8du/urqaysrKOi+RZuN0eoLLuZT4DS7n8TklZJrNlBQFFxGRAAmr8DJ06FCee+45Vq1axaOPPsrq1av5yU9+Qk1NjeV7Zs2aRXJysueVkZERxBFL1HPvcXGygi9xYLXPZShv8zkXmo2WLVX2X0QkgJoUXiZNmnTKhtrvv7Zt23bag7n55pu57rrr6NmzJyNHjqSwsJB169bx/vvvW75n8uTJVFRUeF579uw57a8vUofPyaIinPjboLuAX5ofpqTA8ePBGJ2ISMxq0p6X/Px8cnNz/fZxOBxnMp5T/qxzzjmHkpISnO5fJN8XHx+vTb3S/HwK0d3HHKyDi8FC8ryXLWrGRUQk4JoUXtLS0khLSwvUWE6xd+9evvnmGzp27Bi0rymCwwE7dwIwhQI+4qp6u51FOdvo4Q0uBQXBGqGISEwL2J6X3bt3s2nTJnbv3k1NTQ2bNm1i06ZNHD582NMnMzOT1157DYDDhw8zYcIE/vWvf7Fr1y5WrVrF9ddfz3nnnceQIUMCNUyRulJTPcHlHh7jEaZitUH3PX7sDS45OdqgKyISJAE7Kj1t2jSWLl3qaV9yySUAvPfeewwaNAiA7du3U1FRAUCLFi3YvHkzS5cupby8nE6dOnHNNdfgcrm0LCTBkZoK5eUAOHmXIgZjFVxG8YpquYiIhIjNMAwj1INoTpWVlSQnJ1NRUUFSUlKohyORwmePSz/WsJ5srILLeB7nCe4zm7qzSESkWTTl93dYHZUWCQmXyxNcerHRb3B5kBne4GK3K7iIiIRAwJaNRCKCz4zLEm7jE3pjdbLoZ/wfM5lmNjTjIiISMpp5kdjlM+Myh3zGshh/tVweP1n2X8FFRCSkNPMisWuaOYsyl3wmNlDLZQozvSeLFFxEREJKMy8Sm1wuAPbSmQkNBJdhFDLj5HKRarmIiIScZl4k9vgciZ7PXfifcZnhDS52u45Ei4iEAYUXiS0+wWU883iS8RYdDX7PvdzPE2YzJQV27AjGCEVEpAEKLxI7HA5PcOnFRj8niwzS2ecNLtqgKyISVrTnRWKD0+kp+z+aF/wGl+78h//SxWympCi4iIiEGYUXiX4+R6JHsYyX+Tn1B5caFjOGEjLNZlycbokWEQlDWjaS6OZThK6QYbzKKKw26P6Uv5LL82bDbtceFxGRMKWZF4lePsFlDvlcy3L8FaHzlP0vKFBwEREJYwovEp18loqmUuAuQmf1172WheR5i9DpOLSISFjTspFEJ3f13DnkM5Op+JtxKSabfqw3mypCJyIS9jTzItGnRQvArJ7bUNn/2UysG1w06yIiEvY08yLRJS4ODAOAy/gQf8FlLM8ygd+bTQUXEZGIofAi0cPh8ASXs6ngCG0tOhr0ZBOLyDObOTkKLiIiEUTLRhIdXC5PEbpulLiDS/1F6G7iJTbzI7OpInQiIhFH4UUin8vl2aD7B+5hNw6sgsvVrGQZt5jNlBQVoRMRiUBaNpLI5w4uuSxiKblYBZdMPqWIa8ymitCJiEQszbxIVFhHlp/gAq2oZis9zYZuiBYRiWgKLxL5Cgr4kMvxd7LoI64wP9R9RSIiEU/hRSKTy2UGkRkzALiCfwBGPR0NhlPoreVSUxO0IYqISGBoz4tEnsREqKoCYO9DT/M553M+/2UMS763dGQwjEIKuc5s5uSEYrQiItLMFF4ksvgEl7EsYAljgRbEUcOfuZO7+BN/4WYAfs5L3hkXHYkWEYkaCi8SORwOT3DpwpeUksHJWZZaWvBLnmEX5/KHxIfg6FHv+3SySEQkqmjPi0QGp9NThO5aXqsTXE6qoSUltgvgu+/Mcv82m/l/FVxERKKKZl4k/LlcUFQEwCiWUcj11H+yqIbzjP+Ym3gfekgl/0VEopRmXiS8OZ2eInSFDONVRmFVhC6XxXSh1NNfRESik8KLhC+fGZe55HMthVjVcjmHr1h88qLFhx8O0gBFRCQUbIZh1FccI2JVVlaSnJxMRUUFSUlJoR6OnAmbGVTmkM9E5mBdhK6GPXQzZ10SEupu1hURkYjQlN/fmnmR8JSaCsBeOjcQXGpZyJ1mcAEFFxGRGKDwIuEnNRXKywFw8i5WwWUYb7KHroxjkfmgoCA44xMRkZBSeJHw4hNcLmM1/+Eii44G/w+Xd8aloECni0REYoSOSkv4qBNcPuCflpctGuSw0ls9N7q2bYmISAM08yLhweHwBJfLWe03uHTnc1ZxjdnUfUUiIjFH4UVCz+XyVM/N4e98xBVYBZe+rKWEC81mQoLuKxIRiUEKLxJaLpenqNxUCniPH2O1QfeHbGY9A8yGjkSLiMQshRcJHZ/gso4sZjIVf0ei32G4+WFKioKLiEgMU3iR0HEHl2e5nf4U47+WS573ZNHBg0EZnoiIhCedNpLQ8ClCdwcLsA4uBsVke08WqZaLiEjMU3iR4EtMhKoqAK7ldawnAA3G83jd4KJaLiIiMU/LRhJcqame4NKPNWyir0VHg0w+4wnuM5sKLiIi4qbwIsHjcnlquVzEFtaTjdWR6AF8yFZ6mM2UFAUXERHxUHiR4PA5WXQPj7GNi7EKLmN5ln9yldlMSNAGXRERqUPhRQLPJ7jspTNPMh6rDbrnsZ1F5JkNu11HokVE5BQKLxJ47uAC8By3YfjZoPsit5kf2u2wY0fgxyYiIhFHp40ksBITPR9mUcwGsiw6GoxhifdkkYKLiIhY0MyLBE6LFp6TRYP5mzu4fH+5yOAK3qeY/izhdvORLlsUERE/NPMigZGYCLW1AKSxj69Jp759LmNY4g0tYAYXXbYoIiJ+aOZFmp/D4ZlxyWCXZXABgxt51fywoAAMQ8FFREQapPAizcvhgJ07ARjIavbSFavg0pf1jOBts6k6LiIi0kgBCy+7du1i3Lhx2O12EhMT6d69O9OnT+fYsWN+31dVVcVdd93FD37wA84++2xuuOEG9u/fH6hhSnNyOj3B5XYWsIYrsAouTv7OevqbTe1xERGRJghYeNm2bRu1tbU888wzfPrpp8ybN4+nn36aBx980O/77rvvPpYvX84rr7zC6tWr2bdvHz/72c8CNUxpTkVFAIziJRYzDqvgcg5lrOQnZtNu11KRiIg0ic0wDCNYX2zOnDk89dRT7LA4BltRUUFaWhovvvgiN954I2CGoIsuuog1a9Zw6aWXNvg1KisrSU5OpqKigqSkpGYdv/jhLkS3jiz6U4xVEbpz2M8B0s1GQoKK0ImICNC0399BPW1UUVFBu3btLD+/YcMGjh8/zuDBgz3PMjMz6dq1q2V4qa6uprq62tOurKxs3kFLw5xOz6xLAQ9hFVyglo0nL2KMi1NwERGR0xK0DbslJSXMnz+fX/7yl5Z9ysrKaN26NSkpKXWed+jQgbKysnrfM2vWLJKTkz2vjIyM5hy2NMQnuBQyjEKutehYy0Ly6EKp2aypCc74REQk6jQ5vEyaNAmbzeb3tW3btjrvKS0tZejQoYwaNYq8vLxmGzzA5MmTqaio8Lz27NnTrH+++OETXHJZxLUUUv+sSy3FZDOORWazoCBoQxQRkejT5GWj/Px8cnNz/fZxOByej/ft28fVV1/NwIED+fOf/+z3fenp6Rw7dozy8vI6sy/79+8nPT293vfEx8cTHx/f6PFLM/E5Er2E21hKLlYbdGcz0Vv2v6BAx6JFROSMNDm8pKWlkZaW1qi+paWlXH311fTt25fFixcTF+d/oqdv3760atWKVatWccMNNwCwfft2du/ezYABA5o6VAmUxERPEbpRvMSr3ER9wcVGLbOZyG/5vflAwUVERJpBwDbslpaWMmjQILp168bcuXM5cOCA53MnZ1FKS0txOp0899xz9O/fn+TkZMaNG8f9999Pu3btSEpKYvz48QwYMKBRJ40kCFq1ghMnABjNi5bBBQze5FpvEbqcHAUXERFpFgELLytWrKCkpISSkhK6dOlS53MnT2cfP36c7du3891333k+N2/ePOLi4rjhhhuorq5myJAh/OlPfwrUMKUpnE5PcMliLRvoh1VwGcMSb3DRjIuIiDSjoNZ5CQbVeQkgmxlUChnmZ3MuPMAj/I4pZkMXLYqISCM05fe37jaShjmdnuACMJL/w18tl7txz5QpuIiISAAovIh/PsehAQbzN2qwOt1lnizy1HJRcBERkQAIaoVdiUA+wSWHd3mPwVjtcxnLIib4niwSEREJAIUXseZTr+cyPuCfXI5VcOnMbhZxh9nUySIREQkgLRtJ/XyK0Dl5129wOZsK9nKu2dQt0SIiEmAKL3Iqp9MTXMYzjyI/S0VO/s4hUs1mQgJY3BguIiLSXLRsJHX5zLjMJZ8n+Q1WweVqVrKSn5jNlBQ4eDBYoxQRkRimmRfxSk2tc1/RBGZjFVwG8g+KuMZs2u0KLiIiEjSaeRGTywXl5QD0o5j1ZGEVXHJYyaqTwUW1XEREJMg08yKmadMAc4+LdXAxC9QpuIiISCgpvIi5QRfYS2ee5B6sgouNWuZzr9nQqSIREQkRhZdY51NBdxKPYP1X4gQLyDOr59rtOlUkIiIhoz0vsczl8gSXKRTwArfV260LX7KGy8zgoqUiEREJMc28xCqXy7PPZS75PMJU6l8uqvEGFy0ViYhIGNDMSyzyWSpaR5bfI9GzecB70aKWikREJAxo5iXW+ASXueTTn2Lq/2tgMIUZumhRRETCjsJLLPHZ4zKVAiYwB6uTRbfyHDMwl5V00aKIiIQTm2EYRqgH0ZwqKytJTk6moqKCpKSkUA8nvMTFgWEwh3wm+gkuUMseumqDroiIBE1Tfn9r5iVWOJ1gGOylMw/wKNbBxWA2E737XBRcREQkzGjDbixITISqKgCWMwKDFhYda5nDRH6rfS4iIhLGFF6iXatWcOIEADm8y3sMtuhYSzHZ9GO92dQ+FxERCVNaNopmPsHlXErcwaX+Wi4LyfMGl4ICLReJiEjYUniJVg6HJ7h0YSdf4qC+4DKGxeyhG+NYZD4oKNCMi4iIhDWFl2jkcMDOnQBcxBZK6YZVEbq7+JN3c66Ci4iIRADteYk2qalQXg7ANB5iGxdjFVyuZqV3qSi6TsyLiEgUU3iJJk6nJ7j0o5j1ZGF1JLoD/6WIa8xGTk5wxiciItIMtGwULXyq545lgd/gArWsp7/5YUKCNueKiEhEUXiJFj43RC9hHP6Cy0LyvPtcjh4NyvBERESai5aNokFiIgB76eznhmg4n08pYkjdDboiIiIRRjMvkS411VM9N49nsP5/qcEffMv+62SRiIhEKM28RDr3Bt0sitlAlkUngyzWM4K33U2dLBIRkcilmZdI5XSCzQYtW1LIMHdwqf9IdC7Psu7kBl27PZijFBERaXYKL5HI6fScLFp3og//j+lYBZcpzGAxeWYzJQV27AjWKEVERAJC4SUSuYNLLovoTzEbTs6qfM9Q3mYG5ikkcnLg4MFgjVBERCRgFF4iicNhLhUB68hiKbn4OxK9gF+aH+bkqJaLiIhEDYWXSOFzX9FeOnMzL2AVXGzuW6K7UKoidCIiEnUUXiKFO7g8y+1ksJsdXFBvt95sZHeL7uYt0SkpKkInIiJRR0elI0GLFoA543IHC/BXy+Ve5tGl5ksdhxYRkail8BLubN6loev4K/6CSzd2kcvzQRmWiIhIqGjZKJy5y/4DtOUgG+ln0dHgJl5iFw6zmZIS8KGJiIiEisJLuHI4PGX/O7KHwyRjVctlPI+zjFvMZkKCjkSLiEhUU3gJRy6XZ4PuZaymjM5YBZdhLOcJ7jObOTnaoCsiIlFP4SXcuFwwzSwsN4UC/skVWB2JvoR1vMX1ZqOgQEeiRUQkJii8hBOf4LKXzjzCFKyL0Bm8yc/MD3VDtIiIxBCFl3DhE1wAHuEB/J0sWsgdZhG6uDgFFxERiSk6Kh0OvhdczqWEL0+eHPqeDpSynmwzuADU1ARjhCIiImFD4SUc+ASXthz0e7JoOSO9Zf+1OVdERGKQlo1CzaeWy+UU+Q0uY1hCP9abTQUXERGJUZp5CaVWreDECQB6sZFP6I1VcPk993I/T5jNnJygDVFERCTcKLyESlyc5/6hm3jBb3Dpxo66wUVHokVEJIYpvIRCYqInuGSxhg1kY3UkOoEj7OI8s6HgIiIiovASdKmpPmX/91JGJ6xrudTyOZnmh3FxCi4iIiIEcMPurl27GDduHHa7ncTERLp378706dM5duyY3/cNGjQIm81W5/WrX/0qUMMMrtRUKC8HYDQvNhhcFpJnnixq2VJHokVERNwCNvOybds2amtreeaZZzjvvPPYsmULeXl5HDlyhLlz5/p9b15eHgUFBZ52mzZtAjXM4HG5PMFlDvm8zM34q55bTLZ5siglRRctioiI+AhYeBk6dChDhw71tB0OB9u3b+epp55qMLy0adOG9PT0QA0tNHzK/k9kDv6Cy3ieMIOLbogWERE5RVDrvFRUVNCuXbsG+73wwgucc8459OjRg8mTJ/Pdd99Z9q2urqaysrLOK+y0aOH58Erew19w6cs6nuBes6laLiIiIqcI2obdkpIS5s+f3+Csyy233EK3bt3o1KkTmzdv5oEHHmD79u389a9/rbf/rFmzePjhhwMx5OaRmAi1tQBcxBZ2njw5dAqDxYwhl+fNps+ymYiIiHjZDMN9ZreRJk2axKOPPuq3z9atW8nMzPS0S0tLueqqqxg0aBALFy5s0gCLiopwOp2UlJTQvXv3Uz5fXV1NdXW1p11ZWUlGRgYVFRUkJSU16Ws1O4cDdu4EYDQv8DI/x6qWy3ie8M646JZoERGJMZWVlSQnJzfq93eTw8uBAwf45ptv/PZxOBy0bt0agH379jFo0CAuvfRSlixZQlxc01aqjhw5wtlnn80777zDkCFDGuzflP/xAeVz2eJw3uRtRmAVXDL5jK30MJsKLiIiEoOa8vu7yctGaWlppKWlNapvaWkpV199NX379mXx4sVNDi4AmzZtAqBjx45Nfm/IOJ1QVATAVAr8BBew87k3uOTkKLiIiIg0IGAbdktLSxk0aBBdu3Zl7ty5HDhwgLKyMsrKyur0yczMpLi4GIAvvvgCl8vFhg0b2LVrF2+++Sa/+MUvuPLKK+nVq1eghtq8HA5PcNlLZx5hCv426H6A+54iVc8VERFplIBt2F2xYgUlJSWUlJTQpUuXOp87uVJ1/Phxtm/f7jlN1Lp1a1auXMljjz3GkSNHyMjI4IYbbmDq1KmBGmbzcrk8e1wAJvEIhmU+NJjCDLMIXUKCgouIiEgjNXnPS7gL2Z4Xnz0u0PA+lxt5hVcYbZb9V/VcERGJcU35/R3UOi9R63vB5XYWWAaXvhRTTH8zuCQkKLiIiIg0kWZemoPNG1IuZzUfcQX1z7jUsIdu5lKR3Q47dgRnfCIiImEuoKeN5HsSEz0fnksJX+LAaqloCo+YwQUUXERERE6Tlo3OhNMJVVVAw8FlGIXMwL20pOq5IiIip00zL6fLp3quk3f9BpexPMsi8symitCJiIicEYWX05GaCuXlACzhNooYjFUtlx584g0u0bW9SEREJCQUXprK6fQEl1G8xKvchHURulr+xjDzw5ycYIxOREQk6mnPS1N8r+y//+BiMJuJKkInIiLSzBReGsvlqlP2fyZT8TfjMocJTOD3ZvPo0aAMUUREJBZo2aixfIrQ9eDfWAWXq3mX57jdeyRaJ4tERESalWZeGsOnlks/1lBBO4uOBo8ypW5w0ckiERGRZqWZl4Y4HJ5aLqN5kfVk46+WSz/Wu5s6WSQiIhIICi8NcddyyWItG+iHVXDpxUbe4jqzqaUiERGRgNGyUUPsdgoZ5je4pFHGv+nr6a+lIhERkcBReGnIjh28zXCsNuheyFa+opPZSEjQnUUiIiIBpvDSEIeDYbwF1LeHpZaVXGN+mJKiI9EiIiJBoPDSkJ07GcHbDOQj6gaYWhaSZ54sstvh4MFQjVBERCSmaMNuQ+x22LmTj7iCQobxKjdyIdu4jRe81XO1VCQiIhI0Ci8N2bHDc4P0CMc2Rnwx1t12z7gouIiIiASVwktjfD+gKLCIiIiEjPa8iIiISERReBEREZGIovAiIiIiEUXhRURERCKKwouIiIhEFIUXERERiSgKLyIiIhJRFF5EREQkoii8iIiISERReBEREZGIEnXXAxiGefNzZWVliEciIiIijXXy9/bJ3+P+RF14OXToEAAZGRkhHomIiIg01aFDh0hOTvbbx2Y0JuJEkNraWvbt20fbtm2x2WyhHk7IVVZWkpGRwZ49e0hKSgr1cKKWvs/Bo+91cOj7HDz6XpsMw+DQoUN06tSJuDj/u1qibuYlLi6OLl26hHoYYScpKSmm/6MIFn2fg0ff6+DQ9zl49L2mwRmXk7RhV0RERCKKwouIiIhEFIWXKBcfH8/06dOJj48P9VCimr7PwaPvdXDo+xw8+l43XdRt2BUREZHoppkXERERiSgKLyIiIhJRFF5EREQkoii8iIiISERReIlB1dXV9OnTB5vNxqZNm0I9nKiza9cuxo0bh91uJzExke7duzN9+nSOHTsW6qFFvD/+8Y+ce+65JCQkkJ2dTXFxcaiHFHVmzZpFv379aNu2Le3bt2fkyJFs37491MOKer/73e+w2Wzce++9oR5KRFB4iUETJ06kU6dOoR5G1Nq2bRu1tbU888wzfPrpp8ybN4+nn36aBx98MNRDi2jLli3j/vvvZ/r06Xz88cf07t2bIUOG8NVXX4V6aFFl9erV3HXXXfzrX/9ixYoVHD9+nGuuuYYjR46EemhRa926dTzzzDP06tUr1EOJGDoqHWP+9re/cf/99/N///d//PCHP2Tjxo306dMn1MOKenPmzOGpp55ix44doR5KxMrOzqZfv348+eSTgHmPWUZGBuPHj2fSpEkhHl30OnDgAO3bt2f16tVceeWVoR5O1Dl8+DA/+tGP+NOf/sSMGTPo06cPjz32WKiHFfY08xJD9u/fT15eHs8//zxt2rQJ9XBiSkVFBe3atQv1MCLWsWPH2LBhA4MHD/Y8i4uLY/DgwaxZsyaEI4t+FRUVAPr7GyB33XUXw4cPr/N3WxoWdRczSv0MwyA3N5df/epXZGVlsWvXrlAPKWaUlJQwf/585s6dG+qhRKyvv/6ampoaOnToUOd5hw4d2LZtW4hGFf1qa2u59957ueyyy+jRo0eohxN1XnrpJT7++GPWrVsX6qFEHM28RLhJkyZhs9n8vrZt28b8+fM5dOgQkydPDvWQI1Zjv9e+SktLGTp0KKNGjSIvLy9EIxc5PXfddRdbtmzhpZdeCvVQos6ePXv4zW9+wwsvvEBCQkKohxNxtOclwh04cIBvvvnGbx+Hw8FNN93E8uXLsdlsnuc1NTW0aNGCW2+9laVLlwZ6qBGvsd/r1q1bA7Bv3z4GDRrEpZdeypIlS4iL078VTtexY8do06YNr776KiNHjvQ8HzNmDOXl5bzxxhuhG1yUuvvuu3njjTf44IMPsNvtoR5O1Hn99df56U9/SosWLTzPampqsNlsxMXFUV1dXedzUpfCS4zYvXs3lZWVnva+ffsYMmQIr776KtnZ2XTp0iWEo4s+paWlXH311fTt25f//d//1Q+hZpCdnU3//v2ZP38+YC5pdO3albvvvlsbdpuRYRiMHz+e1157jffff5/zzz8/1EOKSocOHeLLL7+s82zs2LFkZmbywAMPaJmuAdrzEiO6du1ap3322WcD0L17dwWXZlZaWsqgQYPo1q0bc+fO5cCBA57Ppaenh3Bkke3+++9nzJgxZGVl0b9/fx577DGOHDnC2LFjQz20qHLXXXfx4osv8sYbb9C2bVvKysoASE5OJjExMcSjix5t27Y9JaCcddZZ/OAHP1BwaQSFF5FmtmLFCkpKSigpKTklGGqi8/SNHj2aAwcOMG3aNMrKyujTpw/vvPPOKZt45cw89dRTAAwaNKjO88WLF5Obmxv8AYnUQ8tGIiIiElG0g1BEREQiisKLiIiIRBSFFxEREYkoCi8iIiISURReREREJKIovIiIiEhEUXgRERGRiKLwIiIiIhFF4UVEREQiisKLiIiIRBSFFxEREYkoCi8iIiISUf5/3Ni7cqf7YSEAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "YbdEQOQg5AEW",
        "outputId": "8243691f-5161-4ebe-fecc-85aac94263a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7962e00ad330>]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def LinearFunction(seed_val, x):\n",
        "  np.random.seed(seed_val+2)\n",
        "  return 2*x + 1#np.random.random()*x + np.random.random()\n",
        "\n",
        "def piecewise(x, max=2):\n",
        "  for k in range(max+1):\n",
        "    if x <= k:\n",
        "      return LinearFunction(k, x)\n",
        "\n",
        "\n",
        "max = 1\n",
        "\n",
        "# Generate 100 random numbers between -10 and 10\n",
        "x_vals = np.random.uniform(0, max, size=(256,1)).astype(float)\n",
        "\n",
        "\n",
        "#functions = {i: lambda x: LinearFunction(i+11, x) for i in range(-10, 11)}\n",
        "y = np.array([piecewise(x, max) for x in x_vals.flatten()]) #+ .1*np.random.randn(len(x_vals)).astype(float)\n",
        "plt.plot(x_vals.flatten(), y.flatten(), '.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LieVxtg76jU6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # Define two hidden layers with 12 nodes each\n",
        "        #self.hidden1 = nn.Linear(1, 2)  # 1 input node to 12 hidden nodes\n",
        "        #self.hidden2 = nn.Linear(2, 2) # 12 hidden nodes to 12 hidden nodes\n",
        "        #self.output = nn.Linear(2, 1)   # 12 hidden nodes to 1 output node\n",
        "        self.output2 = nn.Linear(1, 1)   # 12 hidden nodes to 1 output node\n",
        "        torch.nn.init.xavier_uniform_(self.output2.weight)\n",
        "        torch.nn.init.zeros_(self.output2.bias)  # Initialize bias to zero\n",
        "        # Initialize parameters\n",
        "        #nn.init.xavier_uniform_(self.hidden1.weight)\n",
        "        #nn.init.xavier_uniform_(self.hidden2.weight)\n",
        "        #nn.init.xavier_uniform_(self.output.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = torch.relu(self.hidden1(x))  # ReLU activation function for first layer\n",
        "        #x = torch.relu(self.hidden2(x))  # ReLU activation function for second layer\n",
        "        x = self.output2(x)               # Output layer without activation (regression)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5HUEwGo685p"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "x_vals = np.random.uniform(0, max, size=(256,1)).astype(float)\n",
        "\n",
        "X_tensor = torch.tensor(x_vals).float().unsqueeze(dim=1)\n",
        "y_tensor = torch.tensor(y).float()\n",
        "\n",
        "# Create a DataLoader for mini-batch processing\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3g9ZNGh_aww",
        "outputId": "d24e3aee-c11f-4cc3-f6c3-eee235fc7566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[1.6599]], requires_grad=True), Parameter containing:\n",
            "tensor([0.], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function (MSE for regression)\n",
        "loss_fn = nn.MSELoss()\n",
        "model = SimpleNN()\n",
        "# Define the optimizer (Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=.001)\n",
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzLJCL5n_b0Q",
        "outputId": "805612b2-7a72-43ec-8b03-dc9cd1a82023"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/1000], Loss: 0.7088\n",
            "[Parameter containing:\n",
            "tensor([[1.8733]], requires_grad=True), Parameter containing:\n",
            "tensor([0.8587], requires_grad=True)]\n",
            "Epoch [100/1000], Loss: 0.3888\n",
            "[Parameter containing:\n",
            "tensor([[1.7510]], requires_grad=True), Parameter containing:\n",
            "tensor([1.0518], requires_grad=True)]\n",
            "Epoch [150/1000], Loss: 0.5810\n",
            "[Parameter containing:\n",
            "tensor([[1.5987]], requires_grad=True), Parameter containing:\n",
            "tensor([1.1507], requires_grad=True)]\n",
            "Epoch [200/1000], Loss: 0.3191\n",
            "[Parameter containing:\n",
            "tensor([[1.4558]], requires_grad=True), Parameter containing:\n",
            "tensor([1.2303], requires_grad=True)]\n",
            "Epoch [250/1000], Loss: 0.6329\n",
            "[Parameter containing:\n",
            "tensor([[1.3250]], requires_grad=True), Parameter containing:\n",
            "tensor([1.3016], requires_grad=True)]\n",
            "Epoch [300/1000], Loss: 0.4535\n",
            "[Parameter containing:\n",
            "tensor([[1.2084]], requires_grad=True), Parameter containing:\n",
            "tensor([1.3653], requires_grad=True)]\n",
            "Epoch [350/1000], Loss: 0.2794\n",
            "[Parameter containing:\n",
            "tensor([[1.1030]], requires_grad=True), Parameter containing:\n",
            "tensor([1.4228], requires_grad=True)]\n",
            "Epoch [400/1000], Loss: 0.4241\n",
            "[Parameter containing:\n",
            "tensor([[1.0067]], requires_grad=True), Parameter containing:\n",
            "tensor([1.4748], requires_grad=True)]\n",
            "Epoch [450/1000], Loss: 0.2291\n",
            "[Parameter containing:\n",
            "tensor([[0.9212]], requires_grad=True), Parameter containing:\n",
            "tensor([1.5215], requires_grad=True)]\n",
            "Epoch [500/1000], Loss: 0.4147\n",
            "[Parameter containing:\n",
            "tensor([[0.8436]], requires_grad=True), Parameter containing:\n",
            "tensor([1.5636], requires_grad=True)]\n",
            "Epoch [550/1000], Loss: 0.2484\n",
            "[Parameter containing:\n",
            "tensor([[0.7730]], requires_grad=True), Parameter containing:\n",
            "tensor([1.6020], requires_grad=True)]\n",
            "Epoch [600/1000], Loss: 0.4148\n",
            "[Parameter containing:\n",
            "tensor([[0.7109]], requires_grad=True), Parameter containing:\n",
            "tensor([1.6361], requires_grad=True)]\n",
            "Epoch [650/1000], Loss: 0.4101\n",
            "[Parameter containing:\n",
            "tensor([[0.6533]], requires_grad=True), Parameter containing:\n",
            "tensor([1.6670], requires_grad=True)]\n",
            "Epoch [700/1000], Loss: 0.3220\n",
            "[Parameter containing:\n",
            "tensor([[0.6009]], requires_grad=True), Parameter containing:\n",
            "tensor([1.6955], requires_grad=True)]\n",
            "Epoch [750/1000], Loss: 0.4332\n",
            "[Parameter containing:\n",
            "tensor([[0.5541]], requires_grad=True), Parameter containing:\n",
            "tensor([1.7210], requires_grad=True)]\n",
            "Epoch [800/1000], Loss: 0.4344\n",
            "[Parameter containing:\n",
            "tensor([[0.5121]], requires_grad=True), Parameter containing:\n",
            "tensor([1.7439], requires_grad=True)]\n",
            "Epoch [850/1000], Loss: 0.2892\n",
            "[Parameter containing:\n",
            "tensor([[0.4750]], requires_grad=True), Parameter containing:\n",
            "tensor([1.7645], requires_grad=True)]\n",
            "Epoch [900/1000], Loss: 0.3675\n",
            "[Parameter containing:\n",
            "tensor([[0.4399]], requires_grad=True), Parameter containing:\n",
            "tensor([1.7830], requires_grad=True)]\n",
            "Epoch [950/1000], Loss: 0.2823\n",
            "[Parameter containing:\n",
            "tensor([[0.4075]], requires_grad=True), Parameter containing:\n",
            "tensor([1.8003], requires_grad=True)]\n",
            "Epoch [1000/1000], Loss: 0.2685\n",
            "[Parameter containing:\n",
            "tensor([[0.3801]], requires_grad=True), Parameter containing:\n",
            "tensor([1.8159], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 1000  # You can change this depending on your dataset and problem\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        # Zero the gradients from the previous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute predictions\n",
        "        predictions = model(batch_X)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "    # Print the loss every 50 epochs for monitoring\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        print(list(model.parameters()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "FtXUNpM-_evB",
        "outputId": "d781e000-8ae5-422e-de6b-2b7e69c71eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.3376)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'a' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-eda72717f9c8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on new data\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # No need to track gradients during inference\n",
        "    y_test = model(X_tensor)\n",
        "    print(loss_fn(y_test, y_tensor))\n",
        "a\n",
        "plt.plot(X_tensor, y_tensor, '.')\n",
        "plt.plot(X_tensor, y_test, '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QChi7ImPN1GM"
      },
      "outputs": [],
      "source": [
        "\n",
        "loss_fn(y_test, y_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCpbSsKzBrPn"
      },
      "outputs": [],
      "source": [
        "for p in model.parameters(): print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUZekTs7EVb2"
      },
      "outputs": [],
      "source": [
        "model(torch.Tensor([1000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VDyCyM-VR0-3",
        "outputId": "2b443a38-16ae-4a43-8b26-63ed21e14d27"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 1x2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-250-9a96e85c2cf7>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-250-9a96e85c2cf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 1x2)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Simple linear model\n",
        "class LinearRegressionNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionNN, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(1, 2)  # One input, one output\n",
        "        self.linear3 = nn.Linear(2, 1)  # One input, one output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        return self.linear3(x)\n",
        "\n",
        "# Initialize model\n",
        "model = LinearRegressionNN()\n",
        "\n",
        "# Initialize weights using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        torch.nn.init.zeros_(m.bias)  # Initialize bias to zero\n",
        "\n",
        "model.apply(init_weights)\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X)\n",
        "y_tensor = abs(X_tensor)torch.tensor(y, requires_gard = )\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Lower learning rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000  # Increase number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor([[0.5]])\n",
        "    test_prediction = model(test_X)\n",
        "    print(f'Prediction for input 0.5: {test_prediction.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CybLE-agfpb"
      },
      "source": [
        "# Base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "rx2wF97ia70s",
        "outputId": "46a639dd-7b9d-4efa-d730-89b3688f09b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7962b6519690>]"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+W0lEQVR4nO3df1yV9f3/8ecBBUTlqKH8UBTEpuUPUFTCynKy0H5MV5n6aQtdWWvW6sMyZd+lbu02+uGalZSuadpaZWXTz5ZhjVJXUSamqUlLUkE9ICieA4igcL5/WBR5LvQgXOcHj/vtdt3qXK/rXL7OMeXZdb3f78vidDqdAgAA8GIBnm4AAADgXAgsAADA6xFYAACA1yOwAAAAr0dgAQAAXo/AAgAAvB6BBQAAeD0CCwAA8HodPN1Aa2hoaNDhw4fVtWtXWSwWT7cDAADOg9PpVGVlpaKjoxUQ0Pw1FL8ILIcPH1ZMTIyn2wAAAC1QXFysPn36NHuMXwSWrl27SjrzgcPCwjzcDQAAOB8Oh0MxMTGNP8eb4xeB5ZvbQGFhYQQWAAB8zPkM52DQLQAA8HoEFgAA4PUILAAAwOsRWAAAgNcjsAAAAK9HYAEAAF6PwAIAALwegQUAAHg9AgsAAPB6BBYAAOD1CCwAAKBZNnuNPiwsl81e47Ee/OJZQgAAoG0s21yorPUFkiSLpEduGqqpo/qa3gdXWAAAgEvLNn0bViTJKWnemp0eudJCYAEAAGex2WuU9VbBWfudkvL3V5jeD4EFAACcJX3FFsOaxWJiI18jsAAAgCZmPL9F/y2tMqyP6NfdxG7OILAAAIBGO4ortPGLMsP67HHxirJ2MrGjMwgsAACg0dpPDxvWxvTvoTlpg0zs5lsEFgAA0Gj11iKX+/t0C9FLd6aY3M23CCwAAECSNGRBjk7UNbisZd86wuRumiKwAAAA/WlDgapq613W+od3VkKM+QNtv4vAAgBAO7f6kyI9/V6hYf3/XeeZcSvfRWABAKAds9lrNHfNTsN6tDVE4y+JNLEj1wgsAAC0YwvW7jasBQVa9GHmeBO7MeZ2YNm8ebNuuOEGRUdHy2KxaO3atc0eP2PGDFkslrO2wYMHNx6zcOHCs+qDBnn+8hMAAP7MZq/R23tKDeuv/cJzs4K+z+3AUl1drYSEBGVnZ5/X8U8++aRsNlvjVlxcrB49emjKlClNjhs8eHCT495//313WwMAAG548PXPDGsp/Xt4fKDtd3Vw9w0TJ07UxIkTz/t4q9Uqq9Xa+Hrt2rWqqKjQzJkzmzbSoYMiIz1/jwwAgPYgd0+J/vNluctafHhnvezBNVdcMX0My/Lly5Wamqp+/fo12f/ll18qOjpa/fv316233qqiItcL10hSbW2tHA5Hkw0AAJyf1Z8U6fZV+S5rI/t1U+4DV5vb0HkwNbAcPnxYb731lu64444m+5OTk7Vy5Url5OTo2Wef1b59+3TllVeqsrLS5XmysrIar9xYrVbFxMSY0T4AAD7vXLOCHrr+UhO7OX+mBpZVq1apW7dumjx5cpP9EydO1JQpUzRs2DClpaVp/fr1On78uF599VWX58nMzJTdbm/ciouLTegeAADfd+MzHxjWxl4c7lXjVr7L7TEsLeV0OrVixQr97Gc/U1BQULPHduvWTT/4wQ+0d+9el/Xg4GAFBwe3RZsAAPitnz+/RTZ7rWH90ZuHmdiNe0y7wrJp0ybt3btXt99++zmPraqqUmFhoaKiokzoDAAA/7ejuELvflFmWL9uSKSirJ1M7Mg9bgeWqqoqbd++Xdu3b5ck7du3T9u3b28cJJuZmanbbrvtrPctX75cycnJGjJkyFm1Bx54QJs2bdL+/fv14Ycf6ic/+YkCAwM1ffp0d9sDAAAu3Pfyp4a1Th0ClP3TJBO7cZ/bt4S2bt2qcePGNb7OyMiQJKWnp2vlypWy2WxnzfCx2+1as2aNnnzySZfnPHjwoKZPn66jR4+qZ8+euuKKK/TRRx+pZ8+e7rYHAAC+Z0dxhfYfqzGsvzvnavOaaSGL0+l0erqJC+VwOGS1WmW32xUWFubpdgAA8Co/X7lF7xa4vh2UntJPv5t09t0PM7jz85tnCQEA4MfmvLrdMKz0D+/ssbDiLtNmCQEAAHNd/9R/tOuw68VVL43qqvX3jTW5o5bjCgsAAH4od0+JYVixSFo+Y5S5DV0gAgsAAH5o1guul96XpOmj+3r1FGZXCCwAAPiZP20oUEMzU2ruHT/AvGZaCYEFAAA/88/PbIa12ePife7qikRgAQDAryzbXKj9R0+4rPUPD9WctEEmd9Q6mCUEAICfWLapUFlvFbishXSw6N0Hxrms+QKusAAA4Ads9ho9YhBWfjI8SgV/uNbkjloXgQUAAD+wr7xarsbZBlikBydcYno/rY3AAgCAH/hb3gGX++dOHOSTg2y/j8ACAICP21Fcobd2lZy1f0ZKP901Nt4DHbU+AgsAAD5uy/5jLvfH9Ag1uZO2Q2ABAMCH2ew1CukY6LI2Mra7yd20HaY1AwDgox54dbte33bIZe2mEb2VEENgAQAAHnR5Vq4O2U822WeRdO8PB2j8Jb38KqxI3BICAMDnvLa16KywIklOSSnx4X4XViQCCwAAPuexnC8Ma7Hh/jPQ9rsILAAA+JDrn/qPyqrqXNYSeof5xZorrhBYAADwEbl7SrTrsMOw/vvJQ0zsxlwEFgAAfMQT7/zXsDZxSKRfjl35BoEFAAAf8HhOgXYfrnRZuyyuu579aZLJHZmLwAIAgJdbtrlQ2RsLDet/njbcxG48g8ACAIAXs9lrlLW+wLCeea1/PNzwXAgsAAB4sXmvf2ZYm5wY7TcPNzwXAgsAAF7q7hfztenLcsP63ImDTOzGswgsAAB4oR3FFXprV4lhffbV8e3iVtA3CCwAAHihX/59m2FtQM/OmjOh/VxdkQgsAAB4nQl/3qRDx89+VtA3/nRLgondeAcCCwAAXiR3T4kKSqsM6/6+QJwRAgsAAF7kb3kHDGtTknr7/QJxRggsAAB4iWWbC7Xxv65nBfXuFqLHpySa25AX6eDpBgAAgLRsU6Gy3jJeIO6ZW0eY2I334QoLAAAeZrPXNBtWxl4c3i7HrXwXgQUAAA9b8f6+ZuuP3jzMpE68l9uBZfPmzbrhhhsUHR0ti8WitWvXNnv8xo0bZbFYztpKSpouhpOdna3Y2FiFhIQoOTlZW7Zscbc1AAB8js1eo782E1gyJ7aPZwWdi9uBpbq6WgkJCcrOznbrfV988YVsNlvj1qtXr8ba6tWrlZGRoQULFmjbtm1KSEhQWlqajhw54m57AAD4lPte/lROp+ta5rWDdNdV7eNZQefi9qDbiRMnauLEiW7/Qr169VK3bt1c1p544gnNmjVLM2fOlCQtXbpUb775plasWKF58+a5/WsBAOALFm0o0Jb9FWftt0haO3tMux+38l2mjWFJTExUVFSUfvSjH+mDDz5o3F9XV6f8/HylpqZ+21RAgFJTU5WXl+fyXLW1tXI4HE02AAB8ic1eoyXvFbqsXTc0irDyPW0eWKKiorR06VKtWbNGa9asUUxMjK6++mpt23bmGQnl5eWqr69XREREk/dFREScNc7lG1lZWbJarY1bTExMW38MAABa1S1LPzSszRobZ2InvqHN12EZOHCgBg4c2Ph6zJgxKiws1J///Gf97W9/a9E5MzMzlZGR0fja4XAQWgAAPmPhul0qrnD9rKDk2O5cXXHBIwvHjR49Wu+//74kKTw8XIGBgSotLW1yTGlpqSIjI12+Pzg4WMHBwW3eJwAArc1mr9HKZpbfXzx9uInd+A6PrMOyfft2RUVFSZKCgoKUlJSk3NzcxnpDQ4Nyc3OVkpLiifYAAGgzza25kp7SjynMBty+wlJVVaW9e/c2vt63b5+2b9+uHj16qG/fvsrMzNShQ4f0wgsvSJIWL16suLg4DR48WCdPntRf//pXvfvuu3r77bcbz5GRkaH09HSNHDlSo0eP1uLFi1VdXd04awgAAH+wo7hCf/2P68AyMKKLfjdpiMkd+Q63A8vWrVs1bty4xtffjCVJT0/XypUrZbPZVFRU1Fivq6vTr3/9ax06dEihoaEaNmyY/v3vfzc5x9SpU1VWVqb58+erpKREiYmJysnJOWsgLgAAvmr1J0Wau2any1rapRFadttIkzvyLRan02i5Gt/hcDhktVplt9sVFhbm6XYAAGjCZq/RmKx35eoHrsUifTjvh+3yVpA7P795lhAAAG3s35+XugwrkpQc16NdhhV3EVgAAGhDqz8p0kPrdhvWZ13Jmivng8ACAEAbsdlrDMetSNKQ6DCNv8T1Eh5oisACAEAbmbTkA8PaqH7d9a9fXWliN77NIwvHAQDg70b94R2VVdUZ1n97/SUmduP7uMICAEAre21rUbNhZezF4Sy/7yYCCwAArexPb/+32fqjNw8zqRP/QWABAKAV2ew1KnHUGtZnXx3PNOYWILAAANCK0pd/bFgb2jtMcyYMMrEb/0FgAQCglcxcuUX/PVLtshYaFKB/3susoJYisAAA0Ap2FFfovYIyw/rLsy4zsRv/Q2ABAKAV/OrlTw1r4wb2ZFbQBSKwAABwgR7PKdCBYzUua326hej5maNN7sj/EFgAALgANnuNsjcWGtazbx1hYjf+i8ACAMAFSF++xbB2NbeCWg1L8wMA0ELJf/y3Sg3WXBkT30MruRXUarjCAgBAC7y2tcgwrFwe30MvzUoxuSP/RmABAKAFHnmrwLD2IIvDtToCCwAAbpr+lzwdrT7lspbUrxvjVtoAgQUAADc8vqFAeV8dc1nrEhyoNXdfbnJH7QOBBQCA82Sz1yj7PeMpzH+/I9nEbtoXAgsAAOdpxfv7DGusZtu2CCwAAJwHm71Gyw0CS0r/Hqxm28YILAAAnIf8AxVqcJ69f3JilF6+kynMbY3AAgDAOaz+pEj3vnT2ww0DLNLciZd4oKP2h8ACAEAzbPYaZb6xU9+/uBJgkbJuHKooayeP9NXesDQ/AADNePrd/7q8FfTUtOG6PiHa/IbaKQILAAAGRjz8to65WCAu0GJRUiwzgszELSEAAFx4bnOhy7AiSX+8cQi3gkxGYAEAwIVlm79yuX9Y7zBNHdXX5G5AYAEA4Ht+uOg9lVfVuazdwLgVjyCwAADwHdP/kqevyk+4rHUNDtSssfEmdwSJwAIAQKMdxRWGDzaUpLczrjKxG3wXgQUAgK+t3X7YsDZjTD8G2noQgQUAAJ1ZIG7TF0dc1nqEdtTCHw8xuSN8l9uBZfPmzbrhhhsUHR0ti8WitWvXNnv8G2+8oR/96Efq2bOnwsLClJKSog0bNjQ5ZuHChbJYLE22QYMGudsaAAAtsvqTIqVkvWs4duX5maNM7gjf53Zgqa6uVkJCgrKzs8/r+M2bN+tHP/qR1q9fr/z8fI0bN0433HCDPv206TMZBg8eLJvN1ri9//777rYGAIDbbPYazV2z07A+9uJwJcSwSJynub3S7cSJEzVx4sTzPn7x4sVNXv/xj3/UunXr9M9//lPDhw//tpEOHRQZGeluOwAAXJB/f17abP3Rm4eZ1AmaY/oYloaGBlVWVqpHjx5N9n/55ZeKjo5W//79deutt6qoqMjwHLW1tXI4HE02AABa4uN9Rw1rs8fFM9DWS5geWBYtWqSqqirdcsstjfuSk5O1cuVK5eTk6Nlnn9W+fft05ZVXqrKy0uU5srKyZLVaG7eYmBiz2gcA+JHrn/qP/vVZicvamP49NCeN8ZTewuJ0Ol08g/I832yx6B//+IcmT558Xse/9NJLmjVrltatW6fU1FTD444fP65+/frpiSee0O23335Wvba2VrW1tY2vHQ6HYmJiZLfbFRYW5vbnAAC0PwvX7dLKvAMua+MG9tTzM0eb3FH743A4ZLVaz+vnt2lPa37llVd0xx136LXXXms2rEhSt27d9IMf/EB79+51WQ8ODlZwcHBbtAkAaAds9hrDsDJxSISe/elIkzvCuZhyS+jll1/WzJkz9fLLL+u666475/FVVVUqLCxUVFSUCd0BANqba57YZFi7OamPiZ3gfLl9haWqqqrJlY99+/Zp+/bt6tGjh/r27avMzEwdOnRIL7zwgqQzt4HS09P15JNPKjk5WSUlZ+4VdurUSVarVZL0wAMP6IYbblC/fv10+PBhLViwQIGBgZo+fXprfEYAABr9+On/qLK23mUtMixY4y9hxqo3cvsKy9atWzV8+PDGKckZGRkaPny45s+fL0my2WxNZvj85S9/0enTpzV79mxFRUU1bvfdd1/jMQcPHtT06dM1cOBA3XLLLbrooov00UcfqWfPnhf6+QAAaPR4ToE+O2Q8s3TZz5JM7AbuuKBBt97CnUE7AID2yWavUUrWu4b1Ef266Y27LzexI7jz85tnCQEA2oX7Xv7UsNYjtCNhxcsRWAAAfm9HcYW27K9wWYsLD9W2+deY3BHcRWABAPi9xzZ8YVh7adZlJnaCliKwAAD8ms1eow/2ul5+/4oBF7H0vo8gsAAA/Nrc1z4zrM1JG2hiJ7gQpq10CwCA2SYs3qSCkiqXtdGx3ZUQ093kjtBSXGEBAPilhf+3yzCsWCQ9OX24uQ3hghBYAAB+x2av0coPXT8rSJLmXTuIsSs+hsACAPA7979ivObKsN5humtsvIndoDUQWAAAfuXxDQX6eJ/rNVck6eHJQ0zsBq2FwAIA8Bs2e42y3ys0rN80ojcDbX0Us4QAAH7j7hfzDWu/uCpO8yZeamI3aE1cYQEA+IWF63Zpe7HdsJ4+Js7EbtDaCCwAAJ+3bFOhVuYZzwrKnMisIF9HYAEA+DSbvUZZbxUY1meM6ae7rmJWkK8jsAAAfNrr+cWGtcQ+Vi38MbOC/AGBBQDg03YfchjWnv1ZkomdoC0RWAAAPmvZpkLl7C51WUu/rB/jVvwI05oBAD7p8Q0FhmuuDO0dpt+xQJxfIbAAAHzOss2FhmHlrrFxyryW9Vb8DbeEAAA+xWavUdZ641lBw/p0M68ZmIbAAgDwKQvX7W62PqIfS+/7IwILAMBnLNtUqA2fux5kK0npKQy09VcEFgCATzjXAnFR1mD9bhIDbf0VgQUA4BNuWZrXbP2NX15uUifwBAILAMDrLVi7S8UVNYb1e8bFcyvIzxFYAABezWav0aqPjB9sOG5gTz2QNsjEjuAJBBYAgFe7/5VPDWvJsd31/MzRJnYDTyGwAAC81o7iCn28r8Kwvnj6cBO7gScRWAAAXmvL/mOGtcxrBzFupR0hsAAAvJLNXqPaUw0ua+mX9dNdY+NN7giexLOEAABeZ/UnRZq7ZqfL2rVDI3mwYTtEYAEAeBWbvcYwrCxPT9L4SyJN7gjegFtCAACv8uhbewxroUEdTewE3oTAAgDwGss2F2rtdpvLmkVSbHiouQ3Ba7gdWDZv3qwbbrhB0dHRslgsWrt27Tnfs3HjRo0YMULBwcEaMGCAVq5cedYx2dnZio2NVUhIiJKTk7VlyxZ3WwMA+DCbvUZZ642fFTRvIrOC2jO3A0t1dbUSEhKUnZ19Xsfv27dP1113ncaNG6ft27fr/vvv1x133KENGzY0HrN69WplZGRowYIF2rZtmxISEpSWlqYjR4642x4AwEf9z18+MqzdMy5ed13FrKD2zOJ0Op0tfrPFon/84x+aPHmy4TFz587Vm2++qV27djXumzZtmo4fP66cnBxJUnJyskaNGqUlS5ZIkhoaGhQTE6N7771X8+bNO2cfDodDVqtVdrtdYWFhLf04AAAPyd1TottX5busTU6M0uJpI0zuCGZw5+d3m49hycvLU2pqapN9aWlpyss789TNuro65efnNzkmICBAqampjcd8X21trRwOR5MNAOC7lry317A2d+IlJnYCb9XmgaWkpEQRERFN9kVERMjhcKimpkbl5eWqr693eUxJSYnLc2ZlZclqtTZuMTExbdY/AKBt2ew12l5kd1lLuzSCcSuQ5KOzhDIzM2W32xu34uJiT7cEAGih/AMVMhqbsHDSYFN7gfdq84XjIiMjVVpa2mRfaWmpwsLC1KlTJwUGBiowMNDlMZGRrhcHCg4OVnBwcJv1DAAwx4K1u7TqowMua5nMCsJ3tPkVlpSUFOXm5jbZ98477yglJUWSFBQUpKSkpCbHNDQ0KDc3t/EYAID/mbB4k8uwEmA582BDZgXhu9y+wlJVVaW9e78dHLVv3z5t375dPXr0UN++fZWZmalDhw7phRdekCT94he/0JIlS/Tggw/q5z//ud599129+uqrevPNNxvPkZGRofT0dI0cOVKjR4/W4sWLVV1drZkzZ7bCRwQAeJuF/7dLBSVVLmtPTRuu6xOiTe4I3s7twLJ161aNGzeu8XVGRoYkKT09XStXrpTNZlNRUVFjPS4uTm+++ab+93//V08++aT69Omjv/71r0pLS2s8ZurUqSorK9P8+fNVUlKixMRE5eTknDUQFwDg+2z2Gq380PVtIElKiu1uYjfwFRe0Dou3YB0WAPAdVz6aq+KKky5r6Zf140nM7YhXrcMCAMA3Fq7bZRhWYrqHEFZgiMACADCFzV6jlXnGt4Je/cUYE7uBryGwAABMMWHxZsPajDH9mMKMZhFYAABt7rWtRbLXnHZZs4Z00MIfcysIzSOwAADa3LJNhYa1J6YmmNgJfFWbr3QLAGjfRjz8to5Vn3JZ69e9k8Zf4npVc+C7uMICAGgzz20uNAwr4V2CtGnuD03uCL6KwAIAaDPLNru+FRQRFqStv/2Ryd3AlxFYAABtYuG6XSqvcn11ZWi01eRu4OsYwwIAaHW/fDFf63eVGNbvHX+xid3AH3CFBQDQqnYUVzQbVq4dGqmEGJ4XBPcQWAAArerhf31uWEu7JELP3JpkYjfwFwQWAECrsdlrtPXAccP6wsmDzWsGfoXAAgBoNfe/st2wlp7C8vtoOQILAKBVLFi7Sx/vO+ayNjCii343ieX30XIEFgDABVu2uVCrPnL9JOaRsd204X+vMrkj+BsCCwDggtjsNcpaX2BYf+i6S03sBv6KwAIAuCC3r/zEsMYUZrQWAgsAoMVmPr9Fn9sqXdbSLu3FFGa0GgILAKBFdhRX6L0vygzrCxlki1ZEYAEAtMjM541vBd0zLp4pzGhVBBYAgNsWrtulYydcP9gwtkeIHkgbZHJH8HcEFgCAW2z2Gq3Mcz2FWZKenD7CxG7QXhBYAABuWbhut2FtTP8ezApCmyCwAADO290v5mvD56Uua1HWYL10Z4rJHaG9ILAAAM7LjuIKvbWrxLD+xi8vN7EbtDcEFgDAefn3HtdXViQpc+IgZgWhTRFYAADn5d09R1zuTxscobuuije5G7Q3BBYAwDnNfjFfuw1WtF3448Emd4P2qIOnGwAAeLcbnvqPdh52uKxdPzSKW0EwBVdYAACGcveUGIYVSZo1Ns7EbtCeEVgAAIZe23rQsHbtEJ7EDPMQWAAALt39Yr5ydrueGXTdkEg981OexAzzEFgAAGdZtKHAcM2VodFhyiaswGQEFgBAEzZ7jZa8V+iydv3QSP3zV1ea3BHQwsCSnZ2t2NhYhYSEKDk5WVu2bDE89uqrr5bFYjlru+666xqPmTFjxln1CRMmtKQ1AMAFSl/+sWHtsviLTOwE+Jbb05pXr16tjIwMLV26VMnJyVq8eLHS0tL0xRdfqFevXmcd/8Ybb6iurq7x9dGjR5WQkKApU6Y0OW7ChAl6/vnnG18HBwe72xoA4AI9vqFA/z1SbVgff0mEid0A33L7CssTTzyhWbNmaebMmbr00ku1dOlShYaGasWKFS6P79GjhyIjIxu3d955R6GhoWcFluDg4CbHde/OyHMAMJPNXqNsg1tBkpR+WT/WXIHHuBVY6urqlJ+fr9TU1G9PEBCg1NRU5eXlndc5li9frmnTpqlz585N9m/cuFG9evXSwIEDdffdd+vo0aOG56itrZXD4WiyAQAuzKNv7TGsxYd31u8mDzGxG6AptwJLeXm56uvrFRHR9JJgRESESkqMn+D5jS1btmjXrl264447muyfMGGCXnjhBeXm5urRRx/Vpk2bNHHiRNXX17s8T1ZWlqxWa+MWExPjzscAAHzPjuIKrd1uc1nr0y1EuQ9cbW5DwPeYujT/8uXLNXToUI0ePbrJ/mnTpjX++9ChQzVs2DDFx8dr48aNGj9+/FnnyczMVEZGRuNrh8NBaAGAFlr9SZHmrtlpWM++dYSJ3QCuuXWFJTw8XIGBgSotbbqQUGlpqSIjI5t9b3V1tV555RXdfvvt5/x1+vfvr/DwcO3du9dlPTg4WGFhYU02AID7bPaaZsPKtUNZzRbewa3AEhQUpKSkJOXm5jbua2hoUG5urlJSUpp972uvvaba2lr99Kc/Peevc/DgQR09elRRUVHutAcAcNNtzUxhvjy+h565lQXi4B3cniWUkZGh5557TqtWrdKePXt09913q7q6WjNnzpQk3XbbbcrMzDzrfcuXL9fkyZN10UVN5/BXVVVpzpw5+uijj7R//37l5uZq0qRJGjBggNLS0lr4sQAA57KjuEJfNjOF+cEJg0zsBmie22NYpk6dqrKyMs2fP18lJSVKTExUTk5O40DcoqIiBQQ0zUFffPGF3n//fb399ttnnS8wMFCfffaZVq1apePHjys6OlrXXHONHn74YdZiAYA29PePDhjWLu7VhVtB8CoWp9Pp9HQTF8rhcMhqtcputzOeBQDOw69f3a412w4Z1vMyf8iaK2hz7vz85llCANDO7CiuaDasZE4cRFiB1zF1WjMAwPPmrfnMsJZ57SDdNTbexG6A88MVFgBoR3YUV2hPSZXL2py0HxBW4LUILADQjjyWU2BYu3FEHxM7AdzDLSEAaCcmLN6kAoOrK7PHxTNuBV6NwAIA7cCQBTmqqnX9fLbkuB6ak8aaK/Bu3BICAD83+8V8w7AiSYunJZrXDNBCBBYA8GM2e43e3FViWJ99NbeC4BsILADgx17PLzasDYrsojksvw8fQWABAD+1+pMi/entL13WggItyrn/KpM7AlqOwAIAfshmr9HcNTsN66/9IsXEboALR2ABAD9099/yDWtjLw7nwYbwOQQWAPAzO4ortP2g3WXNIunRm4eZ2xDQCggsAOBnfrZ8i2GNBeLgqwgsAOBHbnzmAzlOnnZZi+garAdYIA4+isACAH5i0YYCbSs6blj/441DzGsGaGUEFgDwAzZ7jZa8V2hYjw8P1fhLIk3sCGhdBBYA8AM3PPUfw5o1pINyHxhnYjdA6yOwAICPe21rkcqrTxnWX7h9tIndAG2DwAIAPu7Fjw4Y1iYOiWTNFfgFAgsA+LDHNxRox0GHy9qAnqF69qdJJncEtI0Onm4AANAyv3wxX+ubeRLzn25JNK8ZoI1xhQUAfNCO4opmw8pNI3pzKwh+hSssAOCDfr16u2FtTtoPNHvcxeY1A5iAKywA4GN++fd87S0/YVi/cUQfE7sBzEFgAQAfsqO4Qut3Gt8KmpLUm2cFwS8RWADAhzyV+6VhrXunjnp8SqJ5zQAmIrAAgI9YtqlQuQVlhvX1919pYjeAuQgsAOADbPYaZb1VYFh/9Kah3AqCXyOwAIAPuGXph4a1P0werKmj+prYDWA+AgsAeLncPSUqrjhpWB9/SYSJ3QCeQWABAC/3xDv/NazNHhfPrSC0CwQWAPBiO4ortPtwpctan24hmpM2yOSOAM8gsACAl1r9SZEmZRuPXcm+dYSJ3QCeRWABAC9ks9do7pqdhvVrh0TyrCC0Ky0KLNnZ2YqNjVVISIiSk5O1ZcsWw2NXrlwpi8XSZAsJCWlyjNPp1Pz58xUVFaVOnTopNTVVX35pvDgSAPi7Bet2GdZmj4vXMz9NMrEbwPPcDiyrV69WRkaGFixYoG3btikhIUFpaWk6cuSI4XvCwsJks9katwMHDjSpP/bYY3rqqae0dOlSffzxx+rcubPS0tJ08qTxqHgA8FfLNhXq7c9d/53688tjGbeCdsntwPLEE09o1qxZmjlzpi699FItXbpUoaGhWrFiheF7LBaLIiMjG7eIiG+n4DmdTi1evFi//e1vNWnSJA0bNkwvvPCCDh8+rLVr17boQwGArzrXAnGTEqNN7AbwHm4Flrq6OuXn5ys1NfXbEwQEKDU1VXl5eYbvq6qqUr9+/RQTE6NJkyZp9+7djbV9+/appKSkyTmtVquSk5MNz1lbWyuHw9FkAwB/8M7nxg82nMi4FbRjbgWW8vJy1dfXN7lCIkkREREqKXH9h2zgwIFasWKF1q1bpxdffFENDQ0aM2aMDh48KEmN73PnnFlZWbJarY1bTEyMOx8DALzWroOu/wdsVGx3Pcu4FbRjbT5LKCUlRbfddpsSExN11VVX6Y033lDPnj21bNmyFp8zMzNTdru9cSsuLm7FjgHAM/7nL3l6Nf+gy9pvr7vE5G4A79LBnYPDw8MVGBio0tLSJvtLS0sVGRl5Xufo2LGjhg8frr1790pS4/tKS0sVFRXV5JyJiYkuzxEcHKzg4GB3WgcArzb2sXdVdKzGZe2mEb25FYR2z60rLEFBQUpKSlJubm7jvoaGBuXm5iolJeW8zlFfX6+dO3c2hpO4uDhFRkY2OafD4dDHH3983ucEAF82/S95hmHl55fH6k+3JJrbEOCF3LrCIkkZGRlKT0/XyJEjNXr0aC1evFjV1dWaOXOmJOm2225T7969lZWVJUn6/e9/r8suu0wDBgzQ8ePH9fjjj+vAgQO64447JJ2ZQXT//ffrD3/4gy6++GLFxcXpoYceUnR0tCZPntx6nxQAvNCO4grlfXXMsM6sIOAMtwPL1KlTVVZWpvnz56ukpESJiYnKyclpHDRbVFSkgIBvL9xUVFRo1qxZKikpUffu3ZWUlKQPP/xQl156aeMxDz74oKqrq3XnnXfq+PHjuuKKK5STk3PWAnMA4G/SVxgvvDk4KoxbQcDXLE6n0+npJi6Uw+GQ1WqV3W5XWFiYp9sBgPPy2tYizXndePn9vMwf8iRm+DV3fn7zLCEA8JDfrjVefv/Rm4YSVoDvILAAgAfM/nu+ak+7vsD9/64dpKmj+prcEeDdCCwAYLJlmwr15k7XC2OGdLBo1th4kzsCvB+BBQBMdK5nBT08eYiJ3QC+g8ACACa67+VPDWthIR00ZSS3ggBXCCwAYJIdxRXasr/CsP6320eb2A3gWwgsAGCSeWs+M6xdy5OYgWYRWADABAvX7dKekiqXtSlJvfUMT2IGmkVgAYA2tmxToVbmHXBZGxTZRY9PSTS3IcAHEVgAoA2da1bQozcNM7EbwHcRWACgDd2y9EPDWnJcD8atAOeJwAIAbWTB2l0qrjhpWF88LdG8ZgAfR2ABgDZgs9do1Ueux61IUubEQTwrCHADgQUA2sAtS/MMa1OSeuuuq1h+H3AHgQUAWlnunhIVV9S4rPXqEsSsIKAFCCwA0MqWvLvXsPZc+kgTOwH8B4EFAFrRjuIKfVpsd1m7NKors4KAFiKwAEArWf1JkSZnG09jXj5jlIndAP6FwAIArcBmr9G8NTvlNKgzKwi4MB083QAA+INbln7oMqxYJM27dpDuGsusIOBCEFgA4AI1t0Dc7ycN1s9SYs1tCPBD3BICgAtwrgXiUi+NMLEbwH8RWADgAjT3rKC0wRGMWwFaCYEFAFpozmvbm31W0MIfDzaxG8C/EVgAoAWWbSrUa/mHDOuZ1zIrCGhNBBYAcJPNXqOstwoM6zcP782sIKCVEVgAwE23Lf/YsBbTPUSLpiaa1wzQThBYAMANO4or9OWRasP6q78YY2I3QPtBYAEAN7zYzBTmKUm9GbcCtBEWjgOA8/TrV7drzTbXA22tIR30+JREcxsC2hGusADAedhRXGEYViQp53/HmtgN0P4QWADgPDyVu9ewdufYOG4FAW2MwAIA57BsU6FyC464rFkkzbw8ztyGgHaIwAIAzTjXmiuP3DSUqyuACVoUWLKzsxUbG6uQkBAlJydry5Ythsc+99xzuvLKK9W9e3d1795dqampZx0/Y8YMWSyWJtuECRNa0hoAtKpHmwkr62aP0dRRfU3sBmi/3A4sq1evVkZGhhYsWKBt27YpISFBaWlpOnLE9eXSjRs3avr06XrvvfeUl5enmJgYXXPNNTp0qOngtQkTJshmszVuL7/8css+EQC0kmWbC7V2+2GXtf8Z3VcJMd1N7ghovyxOp9PpzhuSk5M1atQoLVmyRJLU0NCgmJgY3XvvvZo3b945319fX6/u3btryZIluu222ySducJy/PhxrV271v1PIMnhcMhqtcputyssLKxF5wCA77LZazQm610Z/QWZl/lDbgUBF8idn99uXWGpq6tTfn6+UlNTvz1BQIBSU1OVl5d3Xuc4ceKETp06pR49ejTZv3HjRvXq1UsDBw7U3XffraNHj7rTGgC0qvte/tQwrGRO5MGGgNncWjiuvLxc9fX1ioiIaLI/IiJCBQXG93m/a+7cuYqOjm4SeiZMmKAbb7xRcXFxKiws1G9+8xtNnDhReXl5CgwMPOsctbW1qq2tbXztcDjc+RgA0KxFGwq0ZX+Fy9rscfG66yoebAiYzdSVbh955BG98sor2rhxo0JCQhr3T5s2rfHfhw4dqmHDhik+Pl4bN27U+PHjzzpPVlaWfve735nSM4D2xWav0ZL3Cl3Wxg/qpTlpg0zuCIDk5i2h8PBwBQYGqrS0tMn+0tJSRUZGNvveRYsW6ZFHHtHbb7+tYcOGNXts//79FR4err17XS/UlJmZKbvd3rgVFxe78zEAwNBNz3xgWPvV+AEmdgLgu9wKLEFBQUpKSlJubm7jvoaGBuXm5iolJcXwfY899pgefvhh5eTkaOTIkef8dQ4ePKijR48qKirKZT04OFhhYWFNNgC4UNOfy9Nhe63L2ujY7swKAjzI7WnNGRkZeu6557Rq1Srt2bNHd999t6qrqzVz5kxJ0m233abMzMzG4x999FE99NBDWrFihWJjY1VSUqKSkhJVVVVJkqqqqjRnzhx99NFH2r9/v3JzczVp0iQNGDBAaWlprfQxAaB5O4orlFd4zLD+5PThJnYD4PvcHsMydepUlZWVaf78+SopKVFiYqJycnIaB+IWFRUpIODbHPTss8+qrq5ON998c5PzLFiwQAsXLlRgYKA+++wzrVq1SsePH1d0dLSuueYaPfzwwwoODr7AjwcA52fums8Ma+kp/ZgVBHiY2+uweCPWYQFwIX646D19VX7CZe2izkHKf+hHJncEtA9ttg4LAPib//lLnmFYkaQVM8497g5A2yOwAGi3dhRX6MOvjMetXDskkoG2gJcgsABot4yeEyRJPbsE6ZmfJpnYDYDmEFgAtFtllScNaw9OGGhiJwDOhcACoF1atqlQ//qsxGWte2hHTRnZ1+SOADSHwAKg3Vm2uVBZbxk//2z9fVea2A2A80FgAdCu2Ow1ylpvHFYevWkoa64AXsjUhx8CgKc9YnBlxSJp7ewxzAoCvBRXWAC0G4s2FGidwcyg6aP7ElYAL0ZgAdAuLNtUqCXvFRrW7+VJzIBXI7AA8Hs2e02zg2wzJw5i3Arg5QgsAPze6/nFhrWfJEbrrqviTewGQEsQWAD4va/Kqg1rD04cZGInAFqKwALAr9nsNYrv2cVlLf2yftwKAnwE05oB+K1lmwv1yFsFcjrPrg3tHabfTR5iflMAWoTAAsAvLdt09mq2Fkk3Do/WtcOiNP6SSM80BqBFCCwA/I7RrCCnpJtH9lVK/EXmNwXggjCGBYDfWfh/uw1rseGhJnYCoLUQWAD4FZu9Rht2l7qsjYjpxiBbwEcRWAD4lfwDFYa12T9kvRXAVzGGBYDfeHxDgbINlt8fEh3GQFvAhxFYAPiFX76Yr/W7Ss7ab5GUPqafFv6YKcyALyOwAPB5O4orXIYVSXp6+nBdnxBtckcAWhtjWAD4vIfW7nK53yIpKba7uc0AaBMEFgA+bUdxhT475HBZ++W4eGYFAX6CwALAp/3+X5+73H9JZFfNSePBhoC/YAwLAJ/1w0Ub9VW56ycxP3LTUJO7AdCWuMICwCdNfy7PMKyMvThcCTGMXQH8CYEFgM/ZUVyhvMJjhvVHbx5mYjcAzEBgAeBznsr90rA2I6UfA20BP0RgAeBTbPYa5RaUuaxFWYO1cBILxAH+iMACwKfc/8qnhrU3fnm5iZ0AMBOBBYDPeDynQB/vc/1ww8xrB3ErCPBjBBYAPsFmr1H2RtcPNhw/qKfuGsuTmAF/RmAB4BOuffI/hrVfjb/YxE4AeEKLAkt2drZiY2MVEhKi5ORkbdmypdnjX3vtNQ0aNEghISEaOnSo1q9f36TudDo1f/58RUVFqVOnTkpNTdWXXxrPAgDQvvzkmQ9UceKUy9q1QyNZcwVoB9xe6Xb16tXKyMjQ0qVLlZycrMWLFystLU1ffPGFevXqddbxH374oaZPn66srCxdf/31eumllzR58mRt27ZNQ4acGc3/2GOP6amnntKqVasUFxenhx56SGlpafr8888VEhJy4Z/yAtjsNdq6/5gsFouS+p35SzH/QIV2Hjqu7cXH1alDoOIu6qwvj1SqX3hndQnpoB3Fx3WsqlaOk/WS06nuoUEK6hBw5v66ReoXHqo+3ULllFMWi0WOE6dUXlWr4A4BOni8RpUnT6tThwAdPXFKfbqHKCgwUOVVtTpWXafqutMK+LqXlPhwdeoYoPf3HtWRypM6Xl2nYyfqFG3tpEhriI5W1anfRaHqGtJRR6tqVVffoEMVNaqqPa3yypOqd0rhnYN0aW+r7DWnVF5Zq9MNDaqurdeAXl00eXhv1Zxq0P7yatWerldYSEd9sLdckjS0t1UHK05oQK+uuuLicH120K6dB+06UXdaoUEd1LGDRYVHqlRaeVIBsiipb3edamhQTV2DDhyrVkCARSEdA3W6vkFDelt1Wf+LFBoUqB0H7TpWVavdhx1ynDyl2Is6a0BEFwUFBigxppu27D+mzf8tU0iHQKVeEqFBUV21vfi4eoWF6Ij9pDZ8XqKeXYI1pLe18bOGdAzQ0epTuii0ow5W1KjmVL2mje6r6xOi9Xp+sXYfcqhfeKjCgjvqwNETkpw6Vd+ggpJKhXQM1ODoMNnsNSqvqlP/8M461eDUoYoaWTt1VJegDtqy/6gCAiwaHG1VYt9uCpBFJ0/Xq2+PUH2yv0JHHCfVu3sn1Z1uOPN7WHtagQFSfYN0UZdgXRoVpr1HKvXf0ioNjgrTkapafVVWpeq6eg2K7KrJw3tr92GHyitrJYtToR07akifMPXu1knvf1mu6rp6dexg0dHKOl3UJUgl9pM6fLxG8b266Cdf/x5WnKjTtgPHlL+/QhZJ9U6nQjoGqk+3TiqtrFVAgEVxF3WWtXNHFZZW6diJOjU0SN1Dg2SvqVNV3Wn17BKsTh0DFRhg0YlT9QoODFB8ry6y15xS1cnT6h7aURUnTul0vVPVdadVXlWr2lP1Gt63m2LDu6igpFJdQzqoT7dOOnbilOR0qt9FnRUW2lHb9h/Tx18dU0CARQl9u+nG4b3VKaiDaupOa8PuUn1adNzln8/YHp30zK1J5vxlAMCjLE6n0+nOG5KTkzVq1CgtWbJEktTQ0KCYmBjde++9mjdv3lnHT506VdXV1frXv/7VuO+yyy5TYmKili5dKqfTqejoaP3617/WAw88IEmy2+2KiIjQypUrNW3atHP25HA4ZLVaZbfbFRYW5s7HadbqT4o0b81OufUFATDN4zcP1ZSRfT3dBoAWcufnt1u3hOrq6pSfn6/U1NRvTxAQoNTUVOXl5bl8T15eXpPjJSktLa3x+H379qmkpKTJMVarVcnJyYbnrK2tlcPhaLK1Npu9hrACeLHuoR0JK0A74lZgKS8vV319vSIiIprsj4iIUElJicv3lJSUNHv8N/9055xZWVmyWq2NW0xMjDsf47zsK68mrABebP19V3q6BQAm8slZQpmZmbLb7Y1bcXFxq/8aceGdZWn1swK4UBZJj940lDVXgHbGrcASHh6uwMBAlZaWNtlfWlqqyMhIl++JjIxs9vhv/unOOYODgxUWFtZka21R1k565KahZ4UWQgzgOXde2V8fZv5QU0dxKwhob9yaJRQUFKSkpCTl5uZq8uTJks4Mus3NzdU999zj8j0pKSnKzc3V/fff37jvnXfeUUpKiiQpLi5OkZGRys3NVWJioqQzg3A+/vhj3X333e5/olY0dVRfjf1BzzMzKyzSiK9nCW37ziyhkI6BiuvRWXvLqtTvotAzs4QOHtexyjo5Tp6W03nmXntQx0BFhYVIFik2PFR9uofK6ZQsFn09Q6dOwR0tOlRxZpZQyHdnCXUI1NHKWh078fUME4tFI76eJRTSMUAfuJol1O17s4Qq61RXX3/m/LWnVV51UvUNUniXYA2ODpP95CmVVdaqvsGpqtrTZ2YJJfbWyVMN2n+0WnWnGtQlpIM+2Fsui3RmFs7XM1GuGBCunYe+niVUe1qhwR3UMdCivWVVOuI4KcvXs4ROO506UVuvA8eqFRhoUUiHQJ1qaNDQaKsu6x+uTkEB+uygXUera/X5IYcctV/PEurVRUGBgUqIseqT/ce0+csyBQcGKvXSCA2K7KodxXb1CgtWqf2k3vm8ROFdz8wSOnisRtV1pxXcIUDHqk+pR+evZwnVNWja6BhdnxCtNfkHtfuwvfF7Kjp6QnJKdfX1+qK0UsEdAjW4d5hKjp9UWXWt+od31ukGpw4eq1G30I7q3LGDthw4qgCLRUN6W5UY010Wi1R3qkF9enTS1gMVKrWfVJ/unXSq3qmj1bWqPnlagQEW1Tc4Fd41WIOiwrS3tFJ7j1TrksiujbOETnw9S2jS8N7aY6tUWeVJSVJoUAcN6W1VdLcQvf/lUZ04dVodAy0qr6xTeNevZwlVnPm9mTz8zO9hxYk65R84pm37z6wSW++UQoICzswSctQqMMCiuPDOsoZ21N7SKlWcqFP9d2YJVdedVniXYIUGfTtLKCggQAMizswSqvxmllD117OETp3W0cpa1Z6u1/CY7ooN76yC0kp1De6g3t07nZmi3CD1DQ+VtVNH5R84po+/qlCARUrs200/Gd5boUEddaLuVOPv7/hLIriqArRnTje98sorzuDgYOfKlSudn3/+ufPOO+90duvWzVlSUuJ0Op3On/3sZ8558+Y1Hv/BBx84O3To4Fy0aJFzz549zgULFjg7duzo3LlzZ+MxjzzyiLNbt27OdevWOT/77DPnpEmTnHFxcc6amprz6slutzslOe12u7sfBwAAeIg7P7/dXodl6tSpKisr0/z581VSUqLExETl5OQ0DpotKipSQMC3d5rGjBmjl156Sb/97W/1m9/8RhdffLHWrl3buAaLJD344IOqrq7WnXfeqePHj+uKK65QTk6Ox9dgAQAA3sHtdVi8UVutwwIAANpOm63DAgAA4AkEFgAA4PUILAAAwOsRWAAAgNcjsAAAAK9HYAEAAF6PwAIAALwegQUAAHg9AgsAAPB6bi/N742+WazX4XB4uBMAAHC+vvm5fT6L7vtFYKmsrJQkxcTEeLgTAADgrsrKSlmt1maP8YtnCTU0NOjw4cPq2rWrLBaLp9vxOIfDoZiYGBUXF/NspTbE92wOvmfz8F2bg+/5W06nU5WVlYqOjm7y4GRX/OIKS0BAgPr06ePpNrxOWFhYu//DYAa+Z3PwPZuH79ocfM9nnOvKyjcYdAsAALwegQUAAHg9AosfCg4O1oIFCxQcHOzpVvwa37M5+J7Nw3dtDr7nlvGLQbcAAMC/cYUFAAB4PQILAADwegQWAADg9QgsAADA6xFY2ona2lolJibKYrFo+/btnm7Hr+zfv1+333674uLi1KlTJ8XHx2vBggWqq6vzdGt+ITs7W7GxsQoJCVFycrK2bNni6Zb8TlZWlkaNGqWuXbuqV69emjx5sr744gtPt+X3HnnkEVksFt1///2ebsUnEFjaiQcffFDR0dGebsMvFRQUqKGhQcuWLdPu3bv15z//WUuXLtVvfvMbT7fm81avXq2MjAwtWLBA27ZtU0JCgtLS0nTkyBFPt+ZXNm3apNmzZ+ujjz7SO++8o1OnTumaa65RdXW1p1vzW5988omWLVumYcOGeboVn8G05nbgrbfeUkZGhtasWaPBgwfr008/VWJioqfb8muPP/64nn32WX311VeebsWnJScna9SoUVqyZImkM88Ni4mJ0b333qt58+Z5uDv/VVZWpl69emnTpk0aO3asp9vxO1VVVRoxYoSeeeYZ/eEPf1BiYqIWL17s6ba8HldY/FxpaalmzZqlv/3tbwoNDfV0O+2G3W5Xjx49PN2GT6urq1N+fr5SU1Mb9wUEBCg1NVV5eXke7Mz/2e12SeK/4TYye/ZsXXfddU3+28a5+cXDD+Ga0+nUjBkz9Itf/EIjR47U/v37Pd1Su7B37149/fTTWrRokadb8Wnl5eWqr69XREREk/0REREqKCjwUFf+r6GhQffff78uv/xyDRkyxNPt+J1XXnlF27Zt0yeffOLpVnwOV1h80Lx582SxWJrdCgoK9PTTT6uyslKZmZmebtknne/3/F2HDh3ShAkTNGXKFM2aNctDnQMtN3v2bO3atUuvvPKKp1vxO8XFxbrvvvv097//XSEhIZ5ux+cwhsUHlZWV6ejRo80e079/f91yyy365z//KYvF0ri/vr5egYGBuvXWW7Vq1aq2btWnne/3HBQUJEk6fPiwrr76al122WVauXKlAgL4/4ELUVdXp9DQUL3++uuaPHly4/709HQdP35c69at81xzfuqee+7RunXrtHnzZsXFxXm6Hb+zdu1a/eQnP1FgYGDjvvr6elksFgUEBKi2trZJDU0RWPxYUVGRHA5H4+vDhw8rLS1Nr7/+upKTk9WnTx8PdudfDh06pHHjxikpKUkvvvgif+m0kuTkZI0ePVpPP/20pDO3K/r27at77rmHQbetyOl06t5779U//vEPbdy4URdffLGnW/JLlZWVOnDgQJN9M2fO1KBBgzR37lxuwZ0DY1j8WN++fZu87tKliyQpPj6esNKKDh06pKuvvlr9+vXTokWLVFZW1liLjIz0YGe+LyMjQ+np6Ro5cqRGjx6txYsXq7q6WjNnzvR0a35l9uzZeumll7Ru3Tp17dpVJSUlkiSr1apOnTp5uDv/0bVr17NCSefOnXXRRRcRVs4DgQW4QO+884727t2rvXv3nhUEuYB5YaZOnaqysjLNnz9fJSUlSkxMVE5OzlkDcXFhnn32WUnS1Vdf3WT/888/rxkzZpjfEOACt4QAAIDXY1QgAADwegQWAADg9QgsAADA6xFYAACA1yOwAAAAr0dgAQAAXo/AAgAAvB6BBQAAeD0CCwAA8HoEFgAA4PUILAAAwOsRWAAAgNf7/8vWzYaQqeELAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize model\n",
        "\n",
        "# Simple linear model\n",
        "class LinearRegressionNN(nn.Module):\n",
        "    def __init__(self, hidden_nodes):\n",
        "        super(LinearRegressionNN, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(1, hidden_nodes, bias=False)  # One input, one output\n",
        "        self.output = nn.Linear(hidden_nodes, 1, bias=False)  # One input, one output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        return  self.output(x)\n",
        "\n",
        "\n",
        "base_model = LinearRegressionNN(hidden_nodes=1).requires_grad_(False)\n",
        "# Set parameters in model to random numbers\n",
        "for param in base_model.parameters():\n",
        "    param.data = torch.randn_like(param)\n",
        "\n",
        "\n",
        "base_model.eval()\n",
        "X_tensor = torch.tensor(-5+10*np.random.rand(1000, 1)).float()#.unsqueeze(dim=1)\n",
        "y_tensor = base_model(X_tensor)\n",
        "plt.plot(X_tensor.numpy().flatten(), y_tensor.detach().numpy().flatten(), '.')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "skHwMlmTR1vI",
        "outputId": "9cb670e7-506e-4f29-f518-27d55e952f88"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'LinearRegressionNN' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-bdcda5705187>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegressionNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize weights using Xavier initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LinearRegressionNN' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize model\n",
        "model = LinearRegressionNN(hidden_nodes=1)\n",
        "\n",
        "# Initialize weights using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        try:\n",
        "          torch.nn.init.xavier_uniform_(m.weight)\n",
        "          torch.nn.init.zeros_(m.bias)  # Initialize bias to zero\n",
        "        except:\n",
        "          pass\n",
        "model.apply(init_weights)\n",
        "\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)  # Lower learning rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000  # Increase number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #if (epoch+1) % 100 == 0:\n",
        "    #    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "    #    print([i.data for i in model.parameters()])\n",
        "# Test model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor([[0.5]])\n",
        "    test_prediction = model(test_X)\n",
        "    print(f'Prediction for input 0.5: {test_prediction.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "JNa_Scx8vpbx",
        "outputId": "54ff7220-eb25-433e-866a-5dfa29e2667d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/5000], Loss: 0.0000\n",
            "[tensor([[0.6933],\n",
            "        [1.2056]]), tensor([[-0.0766,  0.3486]])]\n",
            "Epoch [200/5000], Loss: 0.0000\n",
            "[tensor([[0.6933],\n",
            "        [1.2056]]), tensor([[-0.0766,  0.3486]])]\n",
            "Epoch [300/5000], Loss: 0.0000\n",
            "[tensor([[0.6933],\n",
            "        [1.2056]]), tensor([[-0.0766,  0.3486]])]\n",
            "Epoch [400/5000], Loss: 0.0000\n",
            "[tensor([[0.6933],\n",
            "        [1.2056]]), tensor([[-0.0766,  0.3486]])]\n",
            "Epoch [500/5000], Loss: 0.0000\n",
            "[tensor([[0.6933],\n",
            "        [1.2056]]), tensor([[-0.0766,  0.3486]])]\n",
            "Epoch [600/5000], Loss: 0.0000\n",
            "[tensor([[0.6933],\n",
            "        [1.2056]]), tensor([[-0.0766,  0.3486]])]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-61ce88e59ef4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m  \u001b[0;31m# Increase number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m                 )\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallthrough_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize model\n",
        "model = LinearRegressionNN(hidden_nodes=2)\n",
        "\n",
        "# Initialize weights using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        #torch.nn.init.zeros_(m.bias)  # Initialize bias to zero\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)  # Lower learning rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000  # Increase number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        print([i.data for i in model.parameters()])\n",
        "# Test model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor([[0.5]])\n",
        "    test_prediction = model(test_X)\n",
        "    print(f'Prediction for input 0.5: {test_prediction.item()}')\n",
        "\n",
        "\n",
        "print([i.data for i in base_model.parameters()])\n",
        "\n",
        "pred_y = model(X_tensor).detach().numpy().flatten()\n",
        "true_y = base_model(X_tensor).detach().numpy().flatten()\n",
        "plt.plot(X_tensor, true_y, 'rX')\n",
        "plt.plot(X_tensor, pred_y, 'b.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKD4HKeptiIa",
        "outputId": "7af0a28c-2f86-4d8e-8a21-b92d30b8f3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[0.6013]]), tensor([[0.6107]])]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "W7tcPhdJh4w7",
        "outputId": "b8352793-d763-4b92-8295-be1be2130971"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7962b65e8760>]"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred_y = model(X_tensor).detach().numpy().flatten()\n",
        "true_y = base_model(X_tensor).detach().numpy().flatten()\n",
        "plt.plot(X_tensor, true_y, 'rX')\n",
        "plt.plot(X_tensor, pred_y, 'b.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lecNRjWbqE2o",
        "outputId": "195c7c9b-d3c9-44c4-c99b-9f7c02ee457e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.6692],\n",
            "        [ 1.0516]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1967,  1.2103], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-9.4155e-08,  6.5090e-01]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.8643], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[1.0464]])\n",
            "Parameter containing:\n",
            "tensor([1.2043])\n",
            "Parameter containing:\n",
            "tensor([[0.6541]])\n",
            "Parameter containing:\n",
            "tensor([0.8643])\n"
          ]
        }
      ],
      "source": [
        "for i in model.parameters(): print(i)\n",
        "for i in base_model.parameters(): print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ui7UXmrc6jZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GGfOzqG9Pa9C",
        "LxERVD-g8Y9C",
        "DGTF2lp-813s",
        "sHPJ8o_G7mZt",
        "CAtCqkSQbSaA",
        "0IudwqWOdYQR"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0182948587f94b9c8ee166a4857cbc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c788b73048d84d2091ba6fdb4beb032d",
            "placeholder": "​",
            "style": "IPY_MODEL_1f558314e5644b149fd81eaf5f56794d",
            "value": "Map (num_proc=10): 100%"
          }
        },
        "03c318e6b07b47d9afb7aa239aa518ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05d2418c87b44fae8bf00c426ae9eb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "091594635b134ce99c71cf1db16526c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b0250fe6ad14548bba09ed2cf78e9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1efc0432d24e75a9c2625cf0c87016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106412f5c9d8483b94fd992519a05070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0182948587f94b9c8ee166a4857cbc64",
              "IPY_MODEL_a6868646ca214b39a29a231c41a2c5da",
              "IPY_MODEL_37db266040a54bbab4ae0c78d1402c8d"
            ],
            "layout": "IPY_MODEL_da0ebc4694cf422ea7bc65f282f55c2e"
          }
        },
        "1239413aee944685ab10d95a646bb212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131e1bfb869a4c4089ec793e50a09f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f960316d5a4d10ac9363d52bcc6e71",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aac3d0eb5b334915bc9820b793f734bf",
            "value": 25000
          }
        },
        "1a7d0d238df34772b9044ff5be118e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bea7df13e8c47bcba4294bb41161f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f558314e5644b149fd81eaf5f56794d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23f7036a798c47bc9eae5e015dbaf76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28815477856d441080df64f856fce57d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290e0afedc774ea4aff3262a77ae6232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df0dcddfe864d7786dcf9a2af957689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_587a9b50f4534847bcc26bd274f12e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_70b6ba0b163141368c2c1407a3b2a408",
            "value": " 50000/50000 [00:00&lt;00:00, 168109.00 examples/s]"
          }
        },
        "2e1c71c7c3c643c08f2c68703fcfe832": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314a36ea6e5c40c9b14b798715881a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3289b9e724ac4c938678d55e86f7330a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ee488c271ca485b8304df0f30c72d52",
              "IPY_MODEL_f5d6567526c64a9f9286e480ee57e28e",
              "IPY_MODEL_2df0dcddfe864d7786dcf9a2af957689"
            ],
            "layout": "IPY_MODEL_fedda8ef165842d69469450c103ea30d"
          }
        },
        "3578dc7a53cf4708b9153f6e286ab24f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3638f52fad834bfdb0ad1fce6c3a5691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cee7aad85024e8fa0cbb7ecfaaa1cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_bc66a456264f4cfa9eb85c184e3ec27c",
            "value": " 20.5M/20.5M [00:00&lt;00:00, 327MB/s]"
          }
        },
        "37db266040a54bbab4ae0c78d1402c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28815477856d441080df64f856fce57d",
            "placeholder": "​",
            "style": "IPY_MODEL_b8f4a8958030410193aaeb95aab076a9",
            "value": " 250/250 [00:10&lt;00:00, 25.27 examples/s]"
          }
        },
        "3a8290ee014742febefa7e23d048a18a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8a8bad11664c9aaec3a3f17f013477": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1854751fa141a4891196d07f135221": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c7a40a4ce78442895b726019f672ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc0df343506435b8c108cd69d51d89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e1b1a7da8024732af0c9cdbe924b9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4136ceea9ba8471a8010de0e565a709c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420bde1bd00f498593fd8a93dc58902f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ce75556a0c4ccf9feb97e667e5b119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48ce4642b55c48e6bd23a0aba373d364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3acd3526fa4a2993e2274bdf4eae35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "541443183a6d40a38eca72898fb917d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "581fca5501054424b787bb3936fd0c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808a2d87c63249cea15492a02b34e6e6",
            "placeholder": "​",
            "style": "IPY_MODEL_1bea7df13e8c47bcba4294bb41161f4d",
            "value": "Generating train split: 100%"
          }
        },
        "587a9b50f4534847bcc26bd274f12e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c93c4db6e704cf0acb09229cf29472d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ef5b6f865b4583a46f11c6b64baf51",
            "max": 20470363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d2418c87b44fae8bf00c426ae9eb78",
            "value": 20470363
          }
        },
        "6658013b6e424c9882651c71046a97d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70a9c225586b422c89776cb4404cea3b",
            "placeholder": "​",
            "style": "IPY_MODEL_7153f5d451a344689aca4a23fd267a13",
            "value": " 21.0M/21.0M [00:00&lt;00:00, 231MB/s]"
          }
        },
        "680c99e0ead8422eade91e99cdf13ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684c3a767e3b403898a5899cf183e1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d088c6fba45c4fe2be0c3b7b31510f76",
              "IPY_MODEL_754c55b478054e6998e2790ec607f0d7",
              "IPY_MODEL_a522b6e0641e4591842153e5c42ccabd"
            ],
            "layout": "IPY_MODEL_2e1c71c7c3c643c08f2c68703fcfe832"
          }
        },
        "6abafbd7c3f64450b5af4009d52211bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_581fca5501054424b787bb3936fd0c51",
              "IPY_MODEL_131e1bfb869a4c4089ec793e50a09f01",
              "IPY_MODEL_e0055756742448959961f1653a99a81a"
            ],
            "layout": "IPY_MODEL_420bde1bd00f498593fd8a93dc58902f"
          }
        },
        "70a9c225586b422c89776cb4404cea3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b6ba0b163141368c2c1407a3b2a408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "714276a2de3644a682940268ae9ebfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7153f5d451a344689aca4a23fd267a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73e901aa3ef441da81138d2e02a0fcc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8a8bad11664c9aaec3a3f17f013477",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_714276a2de3644a682940268ae9ebfc2",
            "value": 25000
          }
        },
        "754c55b478054e6998e2790ec607f0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1239413aee944685ab10d95a646bb212",
            "max": 41996509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45ce75556a0c4ccf9feb97e667e5b119",
            "value": 41996509
          }
        },
        "78fa322680d14395a54b375cbbbb3418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe52df55cc4b4072ab0a0c1617c71175",
            "max": 7809,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_541443183a6d40a38eca72898fb917d3",
            "value": 7809
          }
        },
        "808a2d87c63249cea15492a02b34e6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86594350ac2e46eea084f223dfda1aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "877754d374964795bde8a66af5cd17b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1753904d2a14ca1adcd17b6f007f193",
            "placeholder": "​",
            "style": "IPY_MODEL_091594635b134ce99c71cf1db16526c6",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "8d71a3168e404f758443cc646dc9b55d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee488c271ca485b8304df0f30c72d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0250fe6ad14548bba09ed2cf78e9a7",
            "placeholder": "​",
            "style": "IPY_MODEL_03c318e6b07b47d9afb7aa239aa518ef",
            "value": "Generating unsupervised split: 100%"
          }
        },
        "931d0757865c42a3b6ee2cd4c56ef562": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cee7aad85024e8fa0cbb7ecfaaa1cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a522b6e0641e4591842153e5c42ccabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a7d0d238df34772b9044ff5be118e8c",
            "placeholder": "​",
            "style": "IPY_MODEL_e1520123c50f4c559bb32fa225d00e8f",
            "value": " 42.0M/42.0M [00:00&lt;00:00, 342MB/s]"
          }
        },
        "a6868646ca214b39a29a231c41a2c5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8290ee014742febefa7e23d048a18a",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb5375e0c5694c5f95483108b83a05cc",
            "value": 250
          }
        },
        "a83affc5e5a946788a7d1f1f4ab252cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d95f1d285e58481882e8b511a33580a5",
            "placeholder": "​",
            "style": "IPY_MODEL_314a36ea6e5c40c9b14b798715881a22",
            "value": "README.md: 100%"
          }
        },
        "a8ad203d8bf043ce95bbeff43691cf44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_290e0afedc774ea4aff3262a77ae6232",
            "placeholder": "​",
            "style": "IPY_MODEL_86594350ac2e46eea084f223dfda1aae",
            "value": "Generating test split: 100%"
          }
        },
        "aac3d0eb5b334915bc9820b793f734bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae7325313ac14a8ca14d5fd2fee89e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877754d374964795bde8a66af5cd17b8",
              "IPY_MODEL_5c93c4db6e704cf0acb09229cf29472d",
              "IPY_MODEL_3638f52fad834bfdb0ad1fce6c3a5691"
            ],
            "layout": "IPY_MODEL_0f1efc0432d24e75a9c2625cf0c87016"
          }
        },
        "ae9a57dbdd444595b3142ce89822c71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ce4642b55c48e6bd23a0aba373d364",
            "placeholder": "​",
            "style": "IPY_MODEL_23f7036a798c47bc9eae5e015dbaf76e",
            "value": " 25000/25000 [00:00&lt;00:00, 142066.48 examples/s]"
          }
        },
        "b8f4a8958030410193aaeb95aab076a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc66a456264f4cfa9eb85c184e3ec27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c07cc541262744ea89f027fe5d859210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a83affc5e5a946788a7d1f1f4ab252cc",
              "IPY_MODEL_78fa322680d14395a54b375cbbbb3418",
              "IPY_MODEL_e67f44172d2247729433f3889847df5b"
            ],
            "layout": "IPY_MODEL_d3323d7c1d694236a948699005f9341c"
          }
        },
        "c0e9174592b0410a8e6a9fb2c0e7278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1753904d2a14ca1adcd17b6f007f193": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46cde1438d54fa89bdbe7c093089125": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8ad203d8bf043ce95bbeff43691cf44",
              "IPY_MODEL_73e901aa3ef441da81138d2e02a0fcc0",
              "IPY_MODEL_ae9a57dbdd444595b3142ce89822c71f"
            ],
            "layout": "IPY_MODEL_8d71a3168e404f758443cc646dc9b55d"
          }
        },
        "c788b73048d84d2091ba6fdb4beb032d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7e73e9cf5347928557a754f1a46754": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb13fb61baea4fd1923f990acd31f200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680c99e0ead8422eade91e99cdf13ef0",
            "placeholder": "​",
            "style": "IPY_MODEL_3e1b1a7da8024732af0c9cdbe924b9ce",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "d088c6fba45c4fe2be0c3b7b31510f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7a40a4ce78442895b726019f672ca6",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e9174592b0410a8e6a9fb2c0e7278f",
            "value": "unsupervised-00000-of-00001.parquet: 100%"
          }
        },
        "d0ef5b6f865b4583a46f11c6b64baf51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3323d7c1d694236a948699005f9341c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f960316d5a4d10ac9363d52bcc6e71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95f1d285e58481882e8b511a33580a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0ebc4694cf422ea7bc65f282f55c2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de80c2d0e39c4e299106c8cc45ac5d37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0055756742448959961f1653a99a81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3578dc7a53cf4708b9153f6e286ab24f",
            "placeholder": "​",
            "style": "IPY_MODEL_931d0757865c42a3b6ee2cd4c56ef562",
            "value": " 25000/25000 [00:00&lt;00:00, 98860.67 examples/s]"
          }
        },
        "e1520123c50f4c559bb32fa225d00e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e67f44172d2247729433f3889847df5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4136ceea9ba8471a8010de0e565a709c",
            "placeholder": "​",
            "style": "IPY_MODEL_4b3acd3526fa4a2993e2274bdf4eae35",
            "value": " 7.81k/7.81k [00:00&lt;00:00, 636kB/s]"
          }
        },
        "e8b144c9c94c4ab5a535341579c07f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de80c2d0e39c4e299106c8cc45ac5d37",
            "max": 20979968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b1854751fa141a4891196d07f135221",
            "value": 20979968
          }
        },
        "efcde3edec9f4d0cb1288f5a1ea7f088": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d6567526c64a9f9286e480ee57e28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efcde3edec9f4d0cb1288f5a1ea7f088",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cc0df343506435b8c108cd69d51d89f",
            "value": 50000
          }
        },
        "f8cc96106385416d809cdec354f8a6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb13fb61baea4fd1923f990acd31f200",
              "IPY_MODEL_e8b144c9c94c4ab5a535341579c07f1f",
              "IPY_MODEL_6658013b6e424c9882651c71046a97d0"
            ],
            "layout": "IPY_MODEL_ca7e73e9cf5347928557a754f1a46754"
          }
        },
        "fb5375e0c5694c5f95483108b83a05cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe52df55cc4b4072ab0a0c1617c71175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fedda8ef165842d69469450c103ea30d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
